{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compnew.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85HgGjzbPxsM",
        "outputId": "a5a7e56f-6158-444a-a71d-b5bda0779efa"
      },
      "source": [
        "!pip install control"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: control in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from control) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from control) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from control) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->control) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->control) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->control) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->control) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->control) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g3cl9VRnUIvS",
        "outputId": "67f0608f-7ed7-4b8d-cf64-fcfaf0b55e80"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "from os import path\n",
        "import control\n",
        "from  control.matlab import *\n",
        "import tensorflow as tfl\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#from Buffer import Buffer\n",
        "#from OUActionNoise import OUActionNoise\n",
        "\n",
        "Us = [0]\n",
        "ts = np.array([0])\n",
        "yout = []\n",
        "\n",
        "class environment():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "            self.kp = 29.5047797848936\n",
        "            self.ki = 2.41863698260906\n",
        "            self.kd = 0.013145\n",
        "            self.j = 1 \n",
        "            self.max_input = 20\n",
        "            #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.I = 0\n",
        "            \n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "            self.Us = [0]\n",
        "            self.ts = np.array([0])\n",
        "            self.yout = []\n",
        "            #self.e00 = 0\n",
        "           #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.j = 1 \n",
        "            self.I = 0\n",
        "            u1 = 0\n",
        "            \n",
        "            #self.rl = random.randrange(-self.max_input, self.max_input)\n",
        "            return np.array([self.e2,u1])\n",
        "\n",
        "    def step(self ,a) :\n",
        "            \n",
        "\n",
        "            mot0 = tf([0.4602, 1.882, 2.038, 0.2338, 0.007103], [62.85, 383.5, 803.4, 624.1, 99.79, 5.916, 0.1204])\n",
        "            self.rl = np.clip(a, -self.max_input, self.max_input)[0]\n",
        "            \n",
        "            P = 29.5047797848936*self.e2\n",
        "            self.I = self.I + 2.41863698260906*(self.e2)*0.1\n",
        "            D = -0.013145*(self.e2-self.e1)/0.1\n",
        "            self.u = P + self.I + D\n",
        "            \n",
        "            uu = self.u + self.rl\n",
        "            \n",
        "            self.Us = np.append(self.Us,uu)\n",
        "            self.ts = np.append(self.ts,0.1*self.j)\n",
        "            y, T, xoutd = lsim(mot0, U=self.Us, T=self.ts)\n",
        "            self.yout.append(y[-1])\n",
        "            #self.ynow = y[-1]\n",
        "            self.e1 = self.e2\n",
        "            self.e2 = 1-y[-1]\n",
        "            self.j+=1\n",
        "            P1 = 29.5047797848936*self.e2\n",
        "            I1 = self.I\n",
        "            I1 = I1 + 2.41863698260906*(self.e2)*0.1\n",
        "            D1 = -0.013145*(self.e2-self.e1)/0.1\n",
        "            u1 = P1 + I1 + D1\n",
        "            \n",
        "            \n",
        "            #reward = 0.8*np.exp(-0.5*(self.e2)**2) + 0.2*np.exp(-np.absolute(self.rl))\n",
        "            reward = -(self.e2)**2\n",
        "\n",
        "            if self.j >= 700:\n",
        "              done = True\n",
        "            else :\n",
        "              done = False\n",
        "            self.state = np.array([self.e2,u1])\n",
        "            return self.state, reward, done, {}\n",
        "\n",
        "env = environment()\n",
        "num_states = 2\n",
        "num_actions = 1\n",
        "upper_bound = 20\n",
        "lower_bound = -20\n",
        "\n",
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tfl.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tfl.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tfl.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tfl.math.reduce_mean(tfl.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tfl.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tfl.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tfl.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tfl.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tfl.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tfl.cast(reward_batch, dtype=tfl.float32)\n",
        "        next_state_batch = tfl.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tfl.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tfl.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tfl.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tfl.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def policy(state, noise_object):\n",
        "    sampled_actions = tfl.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]\n",
        "\n",
        "std_dev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tfl.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tfl.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 160\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(100000, 64)\n",
        "\n",
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tfl.expand_dims(tfl.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "        #print(j)\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    print(\"Episode * {} * Episodic Reward is ==> {}\".format(ep, episodic_reward))\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    #print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/control/timeresp.py:294: UserWarning: return_x specified for a transfer function system. Internal conversion to state space used; results may meaningless.\n",
            "  \"return_x specified for a transfer function system. Internal \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Episodic Reward is ==> -38.296621345032186\n",
            "Episode * 1 * Episodic Reward is ==> -39.01985328154795\n",
            "Episode * 2 * Episodic Reward is ==> -38.94561085363883\n",
            "Episode * 3 * Episodic Reward is ==> -37.83955100210235\n",
            "Episode * 4 * Episodic Reward is ==> -38.344786564175934\n",
            "Episode * 5 * Episodic Reward is ==> -39.23128614514485\n",
            "Episode * 6 * Episodic Reward is ==> -38.15117863882433\n",
            "Episode * 7 * Episodic Reward is ==> -38.531619957258734\n",
            "Episode * 8 * Episodic Reward is ==> -37.8668411805887\n",
            "Episode * 9 * Episodic Reward is ==> -38.17023205763936\n",
            "Episode * 10 * Episodic Reward is ==> -37.5289505464712\n",
            "Episode * 11 * Episodic Reward is ==> -37.62220330290647\n",
            "Episode * 12 * Episodic Reward is ==> -28.66274711997239\n",
            "Episode * 13 * Episodic Reward is ==> -31.64548464870326\n",
            "Episode * 14 * Episodic Reward is ==> -31.864048951408797\n",
            "Episode * 15 * Episodic Reward is ==> -31.638110830766713\n",
            "Episode * 16 * Episodic Reward is ==> -33.34595599641323\n",
            "Episode * 17 * Episodic Reward is ==> -29.278841728535028\n",
            "Episode * 18 * Episodic Reward is ==> -29.42597958065489\n",
            "Episode * 19 * Episodic Reward is ==> -29.53116653572692\n",
            "Episode * 20 * Episodic Reward is ==> -29.617893005920006\n",
            "Episode * 21 * Episodic Reward is ==> -29.82541934567666\n",
            "Episode * 22 * Episodic Reward is ==> -29.80404079451068\n",
            "Episode * 23 * Episodic Reward is ==> -29.6578549515425\n",
            "Episode * 24 * Episodic Reward is ==> -29.48840743723271\n",
            "Episode * 25 * Episodic Reward is ==> -29.931451225501856\n",
            "Episode * 26 * Episodic Reward is ==> -29.677808621914604\n",
            "Episode * 27 * Episodic Reward is ==> -29.371606448382675\n",
            "Episode * 28 * Episodic Reward is ==> -29.736112811976643\n",
            "Episode * 29 * Episodic Reward is ==> -29.767306546817316\n",
            "Episode * 30 * Episodic Reward is ==> -29.302099283082732\n",
            "Episode * 31 * Episodic Reward is ==> -29.535935878669562\n",
            "Episode * 32 * Episodic Reward is ==> -29.488050593261452\n",
            "Episode * 33 * Episodic Reward is ==> -29.837116308962084\n",
            "Episode * 34 * Episodic Reward is ==> -30.00417566872384\n",
            "Episode * 35 * Episodic Reward is ==> -29.434376926921743\n",
            "Episode * 36 * Episodic Reward is ==> -29.34082178979425\n",
            "Episode * 37 * Episodic Reward is ==> -29.563345019651063\n",
            "Episode * 38 * Episodic Reward is ==> -29.427515311104155\n",
            "Episode * 39 * Episodic Reward is ==> -29.653809313505363\n",
            "Episode * 40 * Episodic Reward is ==> -29.76915901057971\n",
            "Episode * 41 * Episodic Reward is ==> -30.208999627443536\n",
            "Episode * 42 * Episodic Reward is ==> -30.022338945634672\n",
            "Episode * 43 * Episodic Reward is ==> -29.528441981914906\n",
            "Episode * 44 * Episodic Reward is ==> -29.48864536924798\n",
            "Episode * 45 * Episodic Reward is ==> -29.557160262671783\n",
            "Episode * 46 * Episodic Reward is ==> -29.618596318417598\n",
            "Episode * 47 * Episodic Reward is ==> -29.68202193427345\n",
            "Episode * 48 * Episodic Reward is ==> -29.378119270487858\n",
            "Episode * 49 * Episodic Reward is ==> -29.56237606521412\n",
            "Episode * 50 * Episodic Reward is ==> -29.448182935743255\n",
            "Episode * 51 * Episodic Reward is ==> -29.931334815300595\n",
            "Episode * 52 * Episodic Reward is ==> -29.7611396426607\n",
            "Episode * 53 * Episodic Reward is ==> -29.33021204195929\n",
            "Episode * 54 * Episodic Reward is ==> -29.15884469824247\n",
            "Episode * 55 * Episodic Reward is ==> -29.798558987348287\n",
            "Episode * 56 * Episodic Reward is ==> -29.388190514913365\n",
            "Episode * 57 * Episodic Reward is ==> -29.439297586237785\n",
            "Episode * 58 * Episodic Reward is ==> -29.428139631581118\n",
            "Episode * 59 * Episodic Reward is ==> -29.787009662731386\n",
            "Episode * 60 * Episodic Reward is ==> -29.68940516585908\n",
            "Episode * 61 * Episodic Reward is ==> -29.667162411390322\n",
            "Episode * 62 * Episodic Reward is ==> -29.40563439011488\n",
            "Episode * 63 * Episodic Reward is ==> -29.909869667604355\n",
            "Episode * 64 * Episodic Reward is ==> -30.063574341301695\n",
            "Episode * 65 * Episodic Reward is ==> -29.731948157374305\n",
            "Episode * 66 * Episodic Reward is ==> -29.683750205923523\n",
            "Episode * 67 * Episodic Reward is ==> -30.14166630942031\n",
            "Episode * 68 * Episodic Reward is ==> -29.76990433076988\n",
            "Episode * 69 * Episodic Reward is ==> -29.556768076252844\n",
            "Episode * 70 * Episodic Reward is ==> -29.40858260483362\n",
            "Episode * 71 * Episodic Reward is ==> -29.450922045312993\n",
            "Episode * 72 * Episodic Reward is ==> -29.72885259588544\n",
            "Episode * 73 * Episodic Reward is ==> -29.687633387439206\n",
            "Episode * 74 * Episodic Reward is ==> -29.51106459018392\n",
            "Episode * 75 * Episodic Reward is ==> -29.412780054094476\n",
            "Episode * 76 * Episodic Reward is ==> -29.958115593903226\n",
            "Episode * 77 * Episodic Reward is ==> -29.73151043413453\n",
            "Episode * 78 * Episodic Reward is ==> -29.635737529739178\n",
            "Episode * 79 * Episodic Reward is ==> -29.690678225432354\n",
            "Episode * 80 * Episodic Reward is ==> -29.356775549611697\n",
            "Episode * 81 * Episodic Reward is ==> -29.535714808920382\n",
            "Episode * 82 * Episodic Reward is ==> -29.351437119460776\n",
            "Episode * 83 * Episodic Reward is ==> -29.602577579531633\n",
            "Episode * 84 * Episodic Reward is ==> -29.546866970889468\n",
            "Episode * 85 * Episodic Reward is ==> -29.363639057148227\n",
            "Episode * 86 * Episodic Reward is ==> -29.157863031750068\n",
            "Episode * 87 * Episodic Reward is ==> -29.536183150445876\n",
            "Episode * 88 * Episodic Reward is ==> -29.78507752024544\n",
            "Episode * 89 * Episodic Reward is ==> -29.90349763336411\n",
            "Episode * 90 * Episodic Reward is ==> -29.15433970047114\n",
            "Episode * 91 * Episodic Reward is ==> -29.463416608721516\n",
            "Episode * 92 * Episodic Reward is ==> -29.90192580515695\n",
            "Episode * 93 * Episodic Reward is ==> -30.102707218160273\n",
            "Episode * 94 * Episodic Reward is ==> -29.540491414519163\n",
            "Episode * 95 * Episodic Reward is ==> -29.54971102161833\n",
            "Episode * 96 * Episodic Reward is ==> -29.853372054615445\n",
            "Episode * 97 * Episodic Reward is ==> -30.175791448058764\n",
            "Episode * 98 * Episodic Reward is ==> -29.698465939816078\n",
            "Episode * 99 * Episodic Reward is ==> -29.61425160432\n",
            "Episode * 100 * Episodic Reward is ==> -30.10586349803623\n",
            "Episode * 101 * Episodic Reward is ==> -29.98452528951408\n",
            "Episode * 102 * Episodic Reward is ==> -29.558705712658902\n",
            "Episode * 103 * Episodic Reward is ==> -29.521701533004933\n",
            "Episode * 104 * Episodic Reward is ==> -29.362271232755276\n",
            "Episode * 105 * Episodic Reward is ==> -29.46960724006122\n",
            "Episode * 106 * Episodic Reward is ==> -29.736930280669537\n",
            "Episode * 107 * Episodic Reward is ==> -30.398596496868894\n",
            "Episode * 108 * Episodic Reward is ==> -30.504418259940863\n",
            "Episode * 109 * Episodic Reward is ==> -30.052114840375086\n",
            "Episode * 110 * Episodic Reward is ==> -29.248917597934774\n",
            "Episode * 111 * Episodic Reward is ==> -29.393619584774083\n",
            "Episode * 112 * Episodic Reward is ==> -29.632180431302885\n",
            "Episode * 113 * Episodic Reward is ==> -29.598907236772792\n",
            "Episode * 114 * Episodic Reward is ==> -29.332768000226977\n",
            "Episode * 115 * Episodic Reward is ==> -29.402246280038973\n",
            "Episode * 116 * Episodic Reward is ==> -29.820460204776623\n",
            "Episode * 117 * Episodic Reward is ==> -29.654840531095008\n",
            "Episode * 118 * Episodic Reward is ==> -29.412467978923548\n",
            "Episode * 119 * Episodic Reward is ==> -29.83171120305094\n",
            "Episode * 120 * Episodic Reward is ==> -29.477541526408153\n",
            "Episode * 121 * Episodic Reward is ==> -29.306109242059204\n",
            "Episode * 122 * Episodic Reward is ==> -29.51548795764264\n",
            "Episode * 123 * Episodic Reward is ==> -29.564205208614332\n",
            "Episode * 124 * Episodic Reward is ==> -29.338137148943517\n",
            "Episode * 125 * Episodic Reward is ==> -29.530809057729073\n",
            "Episode * 126 * Episodic Reward is ==> -29.598431131748548\n",
            "Episode * 127 * Episodic Reward is ==> -29.349039776870892\n",
            "Episode * 128 * Episodic Reward is ==> -29.495336799810406\n",
            "Episode * 129 * Episodic Reward is ==> -30.093638928795478\n",
            "Episode * 130 * Episodic Reward is ==> -29.90257835358423\n",
            "Episode * 131 * Episodic Reward is ==> -29.599933191359618\n",
            "Episode * 132 * Episodic Reward is ==> -29.44726522804192\n",
            "Episode * 133 * Episodic Reward is ==> -29.33787660949601\n",
            "Episode * 134 * Episodic Reward is ==> -29.689356988161062\n",
            "Episode * 135 * Episodic Reward is ==> -29.797699457783082\n",
            "Episode * 136 * Episodic Reward is ==> -29.36542286327901\n",
            "Episode * 137 * Episodic Reward is ==> -29.451018798936268\n",
            "Episode * 138 * Episodic Reward is ==> -29.43093247994182\n",
            "Episode * 139 * Episodic Reward is ==> -29.38098863366867\n",
            "Episode * 140 * Episodic Reward is ==> -29.38655720706969\n",
            "Episode * 141 * Episodic Reward is ==> -29.82362948126179\n",
            "Episode * 142 * Episodic Reward is ==> -29.343898273796178\n",
            "Episode * 143 * Episodic Reward is ==> -29.39875991190616\n",
            "Episode * 144 * Episodic Reward is ==> -29.43045253691386\n",
            "Episode * 145 * Episodic Reward is ==> -29.169764167832604\n",
            "Episode * 146 * Episodic Reward is ==> -29.469908737523664\n",
            "Episode * 147 * Episodic Reward is ==> -29.622110441796533\n",
            "Episode * 148 * Episodic Reward is ==> -29.177502629988087\n",
            "Episode * 149 * Episodic Reward is ==> -29.65499093945195\n",
            "Episode * 150 * Episodic Reward is ==> -29.941951439148003\n",
            "Episode * 151 * Episodic Reward is ==> -29.953754951741413\n",
            "Episode * 152 * Episodic Reward is ==> -29.764686109153427\n",
            "Episode * 153 * Episodic Reward is ==> -29.4502171936245\n",
            "Episode * 154 * Episodic Reward is ==> -29.5044152079315\n",
            "Episode * 155 * Episodic Reward is ==> -29.426090433770174\n",
            "Episode * 156 * Episodic Reward is ==> -29.389847272597564\n",
            "Episode * 157 * Episodic Reward is ==> -29.63315461361396\n",
            "Episode * 158 * Episodic Reward is ==> -29.38244093810589\n",
            "Episode * 159 * Episodic Reward is ==> -29.438366759924996\n",
            "Episode * 160 * Episodic Reward is ==> -29.72380750063986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/statesp.py\u001b[0m in \u001b[0;36m_convert_to_statespace\u001b[0;34m(sys, **kw)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mslycot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtd04ad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'slycot'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8e61698f7229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_prev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mou_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Recieve state and reward from environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-8e61698f7229>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxoutd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmot0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m#self.ynow = y[-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/matlab/timeresp.py\u001b[0m in \u001b[0;36mlsim\u001b[0;34m(sys, U, T, X0)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;31m# Switch output argument order and transpose outputs (and always return x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforced_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/timeresp.py\u001b[0m in \u001b[0;36mforced_response\u001b[0;34m(sys, T, U, X0, transpose, interpolate, return_x, squeeze)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \"conversion to state space used; results may meaningless.\")\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0msys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_statespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/statesp.py\u001b[0m in \u001b[0;36m_convert_to_statespace\u001b[0;34m(sys, **kw)\u001b[0m\n\u001b[1;32m   1408\u001b[0m                 \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m                     \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2ss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mStateSpace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/statesp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ssmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ssmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ssmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/statesp.py\u001b[0m in \u001b[0;36m_ssmatrix\u001b[0;34m(data, axis)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# If data is passed as a string, use (deprecated?) matrix constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statesp.use_numpy_matrix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(subtype, data, dtype, copy)\u001b[0m\n\u001b[1;32m    119\u001b[0m                       \u001b[0;34m'numpy-for-matlab-users.html). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                       \u001b[0;34m'Please adjust your code to use regular ndarray.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                       PendingDeprecationWarning, stacklevel=2)\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mdtype2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "yjDCtsZDi68-",
        "outputId": "cb51fb33-6d79-44cc-d108-3e9d27b21c78"
      },
      "source": [
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va2zb_9Wt9op"
      },
      "source": [
        "actor_model.save_weights(\"actor.h5\")\n",
        "critic_model.save_weights(\"critic.h5\")\n",
        "\n",
        "target_actor.save_weights(\"target_actor.h5\")\n",
        "target_critic.save_weights(\"target_critic.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "R_I7f7kx0Z07",
        "outputId": "41be82ed-5138-4d78-9819-42daa09a72d7"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "from os import path\n",
        "import control\n",
        "from  control.matlab import *\n",
        "import tensorflow as tfl\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#from Buffer import Buffer\n",
        "#from OUActionNoise import OUActionNoise\n",
        "\n",
        "Us = [0]\n",
        "ts = np.array([0])\n",
        "yout = []\n",
        "\n",
        "class environment():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "            self.kp = 34.5047797848936\n",
        "            self.ki = 2.41863698260906\n",
        "            self.kd = 0.013145\n",
        "            self.j = 1 \n",
        "            self.max_input = 500\n",
        "            #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.I = 0\n",
        "            \n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "            self.Us = [0]\n",
        "            self.ts = np.array([0])\n",
        "            self.yout = []\n",
        "            #self.e00 = 0\n",
        "           #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.j = 1 \n",
        "            self.I = 0\n",
        "            u1 = 0\n",
        "            \n",
        "            #self.rl = random.randrange(-self.max_input, self.max_input)\n",
        "            return np.array([self.e2,u1])\n",
        "\n",
        "    def step(self ,a) :\n",
        "            \n",
        "\n",
        "            mot0 = tf([0.4602, 1.882, 2.038, 0.2338, 0.007103], [62.85, 383.5, 803.4, 624.1, 99.79, 5.916, 0.1204])\n",
        "            self.rl = np.clip(a, -self.max_input, self.max_input)[0]\n",
        "            \n",
        "            P = 34.5047797848936*self.e2\n",
        "            self.I = self.I + 2.41863698260906*(self.e2)*0.1\n",
        "            D = -0.013145*(self.e2-self.e1)/0.1\n",
        "            self.u = P + self.I + D\n",
        "            \n",
        "            uu = self.u + self.rl\n",
        "            \n",
        "            self.Us = np.append(self.Us,uu)\n",
        "            self.ts = np.append(self.ts,0.1*self.j)\n",
        "            y, T, xoutd = lsim(3.6*mot0, U=self.Us, T=self.ts)\n",
        "            self.yout.append(y[-1])\n",
        "            #self.ynow = y[-1]\n",
        "            self.e1 = self.e2\n",
        "            self.e2 = 30-y[-1]\n",
        "            self.j+=1\n",
        "            P1 = 34.5047797848936*self.e2\n",
        "            I1 = self.I\n",
        "            I1 = I1 + 2.41863698260906*(self.e2)*0.1\n",
        "            D1 = -0.013145*(self.e2-self.e1)/0.1\n",
        "            u1 = P1 + I1 + D1\n",
        "            \n",
        "            \n",
        "            #reward = 0.8*np.exp(-0.5*(self.e2)**2) + 0.2*np.exp(-np.absolute(self.rl))\n",
        "            reward = -(self.e2)**2\n",
        "\n",
        "            if self.j >= 700:\n",
        "              done = True\n",
        "            else :\n",
        "              done = False\n",
        "            self.state = np.array([self.e2,u1])\n",
        "            return self.state, reward, done, {}\n",
        "\n",
        "env = environment()\n",
        "num_states = 2\n",
        "num_actions = 1\n",
        "upper_bound = 500\n",
        "lower_bound = -500\n",
        "\n",
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tfl.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tfl.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tfl.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tfl.math.reduce_mean(tfl.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tfl.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tfl.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tfl.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tfl.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tfl.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tfl.cast(reward_batch, dtype=tfl.float32)\n",
        "        next_state_batch = tfl.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tfl.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tfl.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tfl.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tfl.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def policy(state, noise_object):\n",
        "    sampled_actions = tfl.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]\n",
        "\n",
        "std_dev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tfl.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tfl.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 10\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(100000, 64)\n",
        "\n",
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tfl.expand_dims(tfl.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "        #print(j)\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    print(\"Episode * {} * Episodic Reward is ==> {}\".format(ep, episodic_reward))\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    #print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/control/timeresp.py:294: UserWarning: return_x specified for a transfer function system. Internal conversion to state space used; results may meaningless.\n",
            "  \"return_x specified for a transfer function system. Internal \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Episodic Reward is ==> -31317.92979701686\n",
            "Episode * 1 * Episodic Reward is ==> -20403.090725787137\n",
            "Episode * 2 * Episodic Reward is ==> -31189.40630671965\n",
            "Episode * 3 * Episodic Reward is ==> -31295.60767318684\n",
            "Episode * 4 * Episodic Reward is ==> -30911.135367942075\n",
            "Episode * 5 * Episodic Reward is ==> -30926.201452947516\n",
            "Episode * 6 * Episodic Reward is ==> -30907.22946629247\n",
            "Episode * 7 * Episodic Reward is ==> -30906.466460681102\n",
            "Episode * 8 * Episodic Reward is ==> -30946.965018951214\n",
            "Episode * 9 * Episodic Reward is ==> -30949.36195179762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEICAYAAABmqDIrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hkV3nn++9burdUrb5Ldl9oXyQ5tjEXd4gzkEnAhpgZwCQ4iSEZG8zgeICEmXNmwMTnTJwD5zlckpCAOU4cLjHBwRAPYJMY7MYwQCaYYGPTxqZVal/a7rZVfe8uSa1Lqd75Y6/dXS1L6pJUu7ZU+n2epx7tvfbeVa9kt16ttd+9lrk7IiIiScmkHYCIiNQ3JRoREUmUEo2IiCRKiUZERBKlRCMiIolSohERkUSlkmjM7ONmttPMdpjZ18xsVdmxD5rZLjPrN7NfL2u/PLTtMrMbytrPMrMfhfYvm1lzaG8J+7vC8a21/B5FRCRiaTxHY2avA77j7kUz+yiAu3/AzM4HvgS8AjgT+DbQGy7LAa8F9gA/Bt7q7o+b2VeAr7r7HWb2V8BP3f0WM3s3cJG7X29mVwG/4e6/c7rY1q1b51u3bq3uNywiUuceeuihA+6+frpjjbUOBsDd7yvbfQC4MmxfAdzh7mPAU2a2iyjpAOxy9ycBzOwO4Aoz+znwGuBt4ZzbgJuAW8J73RTa7wRuNjPz02TWrVu38uCDDy7guxMRWX7MbPdMxxbDPZprgW+G7Y3As2XH9oS2mdrXAkfcvTil/ZT3CsePhvNFRKSGEuvRmNm3ge5pDt3o7neFc24EisDtScVRCTO7DrgOYMuWLWmGIiJSdxJLNO5+2WzHzeztwBuAS8uGs/YCm8tO2xTamKH9ILDKzBpDr6X8/Pi99phZI9AZzp8u1luBWwG2bdumyd9ERKooraqzy4H3A29y95GyQ3cDV4WKsbOAHuBfiW7+94QKs2bgKuDukKC+y8l7PNcAd5W91zVh+0qi4gMlERGRGkulGAC4GWgBtpsZwAPufr27PxaqyB4nGlJ7j7tPApjZe4F7gQbgc+7+WHivDwB3mNmHgYeBz4b2zwJ/FwoKDhElJxERqbFUypsXs23btrmqzkRE5sbMHnL3bdMdWwxVZyIiUseUaOrMQL7Av+w6kHYYIiInKNHUmY9+q58/+NLDaEhURBYLJZo6058/xsHhcQ4MjacdiogIoERTV4bHijx76DgAuXwh5WhERCJKNHVkYN/Qie2dg0o0IrI4KNHUkVxILg0ZO7EtIpK2tB7YlATk8gVaGjO8ZPMq+jV0JiKLhHo0daQ/X6Cnq4Pzz1hJLl+gVFLlmYikT4mmjuTyBXq7svR1ZxkZn2TvkeNphyQiokRTL46MjJM/NkZvV5beriwA/bpPIyKLgBJNncjlo4qzvq4svV0dALpPIyKLghJNnYifm+ntzpJtbWLjqjb1aERkUVCiqRO5fIGOlkbO7GwFoK87q4c2RWRRUKKpE/2DBXq7Ogjr+9DXneWJ/UNMTJZSjkxEljslmjrg7icqzmJ9XVkmJp2nDgynGJmIiBJNXdg/NMbhkYlTEo0qz0RksVCiqQMDccVZ98lEc86G9mgqGt2nEZGUKdHUgbjXUt6jaWls4Kx17ZpcU0RSp0RTB3L5Amvam1nX0XxKe1+XKs9EJH1KNHWgP1+gZ8PJirNYX3eWZw6NMDJeTCkyERElmiXP3RnID51yfybW25XF/eQ9HBGRNCjRLHHPHR1laKx4yv2ZWJx8NBWNiKRJiWaJixc4m65Hs2XNClqbMloETURSpUSzxMW9ld4NL0w0DRmjZ0NWPRoRSZUSzRKXGyzQtbKFzhVN0x7v7crqoU0RSZUSzRLXP2Xqman6ujvYVxjj8PB4DaMSETkplURjZh83s51mtsPMvmZmq0L7a83sITN7NHx9Tdk1F4f2XWb2SQu1vGa2xsy2m9lA+Lo6tFs4b1f4nJen8b0mabLk7No3RN+siWYloIIAEUlPWj2a7cCF7n4RkAM+GNoPAG909xcD1wB/V3bNLcC7gJ7wujy03wDc7+49wP1hH+D1ZedeF66vK88cGmGsWKJ3mkKAWJyE9OCmiKQllUTj7ve5e/wU4QPAptD+sLs/F9ofA9rMrMXMzgBWuvsD7u7AF4A3h/OuAG4L27dNaf+CRx4AVoX3qRvxvZfZejRdK1tY2dqo+zQikprFcI/mWuCb07S/BfiJu48BG4E9Zcf2hDaALnd/PmwPAl1heyPw7AzX1IW4l3Luho4ZzzEzzuteqR6NiKSmMak3NrNvA93THLrR3e8K59wIFIHbp1x7AfBR4HVz+Ux3dzPzecR6HdHwGlu2bJnr5anpzxfYvKaN9pbZ/zP2dndw1yPP4e4vmKZGRCRpiSUad79stuNm9nbgDcClYTgsbt8EfA242t2fCM17CcNrwabQBpA3szPc/fkwNLav7JrNM1wzNdZbgVsBtm3bNudElZaBfGHWYbNYX1eWwmiRwWOjnNHZVoPIREROSqvq7HLg/cCb3H2krH0V8E/ADe7+v+L2MDR2zMwuCdVmVwN3hcN3ExUOEL6Wt18dqs8uAY6WDbEteePFEk/uH561tDl2ovJM92lEJAVp3aO5GcgC283sETP7q9D+XuBc4L+H9kfMbEM49m7gM8Au4AlO3tf5CPBaMxsALgv7APcAT4bz/yZcXzeeOjBMseTTTj0zVW9XdA9HiUZE0pDY0Nls3P3cGdo/DHx4hmMPAhdO034QuHSadgfes7BIF6/4uZieaaaemWrVima6VrboWRoRScViqDqTecgNFmjIGGevb6/o/D5VnolISpRolqhcvsDWtStobWqo6Py+rg4G8kNMlpZMrYOI1AklmiUqly9UdH8m1tuVZaxYYvfB4QSjEhF5ISWaJej4+CS7D41UVHEWOy9Unmn4TERqTYlmCdq1bwh35pRozt3QgRnsVOWZiNSYEs0SFPdK5pJo2pobeNGaFerRiEjNKdEsQbl8geaGDFvXrpjTdX3dWgRNRGpPiWYJ6s8XOGdDB40Nc/vP19eV5emDI4xOTCYUmYjICynRLEG5wQJ9XTPP2DyT3u4skyXnif1DCUQlIjI9JZol5tjoBM8dHaVnDvdnYloETUTSoESzxAzko95IJbM2T7V1XTvNDRlVnolITSnRLDFxb2QuD2vGmhoynL2+nZwSjYjUkBLNEtM/WGBFcwMbV81vXZm+7iy5vO7RiEjtKNEsMbl8gZ6uLJnM/FbK7OvOsvfIcY6NTlQ5MhGR6c24TICZfQqYcQZGd//DRCKSWeXyBV7dt+H0J84gvrczkC9w8YvWVCssEZEZzdajeRB4CGgFXg4MhNdLgebkQ5OpDg6NcWBofF73Z2LxbAL9gxo+E5HamLFH4+63AZjZfwJe5e7FsP9XwA9qE56Ui++tzGXqmak2rW6jvblBJc4iUjOV3KNZDaws2+8IbVJjC6k4i5kZvd1Zdg4eq1ZYIiKzqmQp548AD5vZdwED/i1wU5JByfT68wU625rYkG1Z0Pv0dWW597FB3B2z+RUViIhUatYejZllgH7gl4CvAV8FfjkeVpPayg0W6O3qWHBy6OvOcnhkggND41WKTERkZrMmGncvAZ9290F3vyu8BmsUm5Rxd3L5woLuz8T6ThQE6D6NiCSvkns095vZW0xjLKnKHxvj2GhxQfdnYr3hPfpVECAiNVBJovl94B+AMTM7ZmYFM9Od5Brrn8diZzNZ19HCuo5mTUUjIjVx2mIAd1/4bzZZsDgpVCPRxO+zUz0aEamBSqrOMLPVQA/Rw5sAuPv3kwpKXqg/X2BdRwtr2qvzrGxvV5avPPgspZLPezobEZFKnDbRmNl/BN4HbAIeAS4Bfgi8JtnQpNxAvkBf99wXO5vJed1ZRsYn2XvkOJvXzG1JaBGRuajkHs37gF8Edrv7q4GXAUcSjUpOUSo5ufxQ1YbN4GRBgNamEZGkVZJoRt19FMDMWtx9J9CXbFhSbs/h4xyfmJzXYmcz6dVqmyJSI5Ukmj1mtgr4OrDdzO4Cdi/kQ83s42a208x2mNnXwvuXH99iZkNm9l/L2i43s34z22VmN5S1n2VmPwrtXzaz5tDeEvZ3heNbFxJzmk5UnFWhtDnW0dLIptVtepZGRBJ32kTj7r/h7kfc/Sbg/wY+C7x5gZ+7HbjQ3S8CcsAHpxz/c+Cb8Y6ZNQCfBl4PnA+81czOD4c/CnzC3c8FDgPvDO3vBA6H9k+E85akuNfRs6F692ggenBTiUZEknbaRGNmHzKz15pZu7t/z93vdvcFzV3i7vfFs0EDDxAVGsSf92bgKeCxskteAexy9yfDZ98BXBEeIn0NcGc47zZOJsErwj7h+KVL9aHTXL7AxlVtZFubqvq+vd1Zntg/xHixVNX3FREpV8nQ2ZPAW4EHzexfzezPzOyKKsZwLaH3YmYdwAeAP5lyzkbg2bL9PaFtLXCkLGnF7adcE44fDee/gJldZ2YPmtmD+/fvX/A3VG39YY6zauvrylIsOU8fHK76e4uIxCoZOvu8u18LvBr4IvBb4euszOzbZvazaV5XlJ1zI1AEbg9NNxENg9V0VS53v9Xdt7n7tvXr19fyo09rYrLEk/uHq3p/JtanyjMRqYFKnqP5DNF9kTzRgmdXAj853XXuftlp3vftwBuAS909XjL6l4ArzexjwCqgZGajRCt9bi67fBOwFzgIrDKzxtBridsJXzcTFTM0Ap3h/CVl98FhxidLVa04i529vp2GjEWzDryk6m8vIgJUNjPAWqCB6NmZQ8CBsqGqeTGzy4H3A7/q7iNxu7v/Stk5NwFD7n5zSBQ9ZnYWUQK5Cnibu3tYJ+dKovs21wB3hbe4O+z/MBz/TllCWzLiJZer+QxNrKWxgbPWtWtyTRFJVCVznf0GgJn9AvDrwHfNrMHdN81+5axuBlqIyqUBHnD362eJoWhm7wXuJUp6n3P3uFjgA8AdZvZh4GGiqjjC178zs11ECfKqBcSbmly+gBmcW+WKs1hfd5af7T2ayHuLiEBlQ2dvAH6FaGXNVcB3iIbQ5i2UHJ/unJum7N8D3DPNeU8SVaVNbR8lup+0pOXyBbaubae1qSGR9+/rynLPo88zMl5kRXNFU9+JiMxJJb9ZLidKLH/p7s8lHI9M0Z9PpuIs1tuVxR0G8kO8ZPOq018gIjJHlVSdvZfoWZfzAcyszcy0dEANjE5M8vSB4UTuz8TO0yJoIpKwSh7YfBfRA49/HZo2EU1HIwl7Yv8QJU+mECC2ec0KWpsymiFARBJTyQOb7wFeCRwDcPcBYEOSQUlkIB9VnFVj+eaZNGSMng1ZTa4pIompJNGMlU85E0qNl1yZ8FLUny/Q1GBsXdue6Of0dWvOMxFJTiWJ5ntm9kdAm5m9FvgH4BvJhiUQLd989roOmhsr+c80f31dWfYVxjg8vKAp7EREplXJb7AbgP3Ao8DvA/e4+42JRiVA1KPpSbDiLNarggARSVAlVWcld/8bd/8td78S2G1m22sQ27I2PFZkz+HjiUw9M1Vceab7NCKShBkTjZm9xsxyYQGyL5rZi83sQeD/A26pXYjL08C+MPVMgoUAsQ3ZFjrbmjS5pogkYrYezZ8B1xHNdXYn0Zxhf+vuF7v7V2sR3HKWC7/0a9GjMTP6urInPlNEpJpmSzTu7v/T3cfc/evAXne/uVaBLXf9+QKtTRk2r1lRk8/r687Sny+wBOcdFZFFbrYpaFaZ2W+Wn1u+r15NsnL5Audu6KAhU5tFQXu7sxRGizx/dJQzV7XV5DNFZHmYLdF8D3hj2f73y/YdUKJJUP9ggVf1rKvZ58VDdP35ghKNiFTVjInG3d9Ry0DkpCMj4+wrjNXk/kws/qzcYIFX92niBxGpnmSfBJR5yeVrV3EW61zRRPfKVs0QICJVp0SzCMUPTtayRwNRYtNDmyJSbUo0i1BusEC2pZEzOltr+rl9XR0M7BtisqTKMxGpnkqWCXiPma0q219tZu9ONqzlLZ56JixzXTN93SsZL5bYfXC4pp8rIvWtkh7Nu9z9SLzj7oeBdyUX0vLm7gzkC4kuDTCTE5Vnuk8jIlVUSaJpsLI/rc2sAWhOLqTlbf/QGIdHJhJd7Gwm527owEyTa4pIdc32HE3sW8CXzSxeYfP3Q5skIDcYFjtLIdG0NTewdW27JtcUkaqqJNF8gCi5/Kewvx34TGIRLXNxb6InhUQD0NvVock1RaSqTpto3L1ENFuzZmyugdxggTXtzazrSGd0sq8ry/bH84xOTNLa1JBKDCJSX2ZMNGb2FXf/bTN7lGmWbnb3ixKNbJnK7SvQm0LFWayveyUlhyf2D3HBmZ2pxCAi9WW2Hs37wtc31CIQiSrOcoMFrrx4U2ox9HVHK3r2DxaUaESkKmab6+z58HV37cJZ3vYeOc7w+GRNp56Z6kVr22luyKjyTESqZrahswLTDJnF3H1lIhEtY3G1VxqlzbGmhgznbOjQImgiUjUzPkfj7tmQTP4SuAHYCGwiqkL7i4V8qJl93Mx2mtkOM/valJkHLjKzH5rZY2b2qJm1hvaLw/4uM/tk/GyPma0xs+1mNhC+rg7tFs7bFT7n5QuJuRb6Q2lz74b0Eg1EU9HooU0RqZZKHth8k7v//+5ecPdj7n4LcMUCP3c7cGEoKMgBHwQws0bgi8D17n4B8GvARLjmFqIZCXrC6/LQfgNwv7v3APeHfYDXl517HUugam4gX6B7ZSudK5pSjaO3O8tzR0c5Njpx+pNFRE6jkkQzbGa/a2YNZpYxs98FFjQZlrvf5+7FsPsAUU8J4HXADnf/aTjvoLtPmtkZwEp3f8CjtYa/ALw5XHMFcFvYvm1K+xc88gDRiqFnLCTupPXnC6nen4mdF2IY0H0aEamCShLN24DfBvLAPuC3Qlu1XAt8M2z3Am5m95rZT8zs/aF9I7Cn7Jo9oQ2gKy5cAAaBrrJrnp3hmkVnsuQM7Buir6sj7VBO3CPSg5siUg2VPLD5NPMYKjOzbwPd0xy60d3vCufcCBSB28vieRXwi8AIcL+ZPQQcreQz3d3NbM5z3JvZdUTDa2zZsmWul1fF7oPDjBdLqc0IUG7jqjY6WhpVECAiVXHaRGNmm4BPAa8MTT8A3ufue2a+Ctz9stO879uJntG5NAyHQdTr+L67Hwjn3AO8nOi+TfnDJZuAvWE7b2ZnuPvzYWhsX2jfC2ye4Zqpsd4K3Aqwbdu2VBZjyaW02Nl0zIzerg6VOItIVVQydPZ54G7gzPD6RmibNzO7HHg/UaHBSNmhe4EXm9mKUBjwq8DjYWjsmJldEqrNrgbuCtfcDVwTtq+Z0n51qD67BDhaNsS26MTLN/csgqEzgL7uLP2DBU7+DSAiMj+VJJr17v55dy+G198C6xf4uTcDWWC7mT1iZn8FJ9a6+XPgx8AjwE/c/Z/CNe8mmsxzF/AEJ+/rfAR4rZkNAJeFfYB7gCfD+X8Trl+0+vMFtqxZwYrmSuY5TV5vV5bDIxPsHxpLOxQRWeIq+a120Mx+D/hS2H8rcHAhH+ru585y7ItEQ2VT2x8ELpym/SBw6TTtDrxnIXHWUm6wkOqDmlPFC6/lBofYkK3tktIiUl8q6dFcS1R1Ngg8D1wJvCPJoJabseIkTx0YpneRDJtB2Wqbuk8jIgtUSdXZbuBNNYhl2XrqwDDFkqeyfPNM1na0sK6jmf7BY2mHIiJL3Gxznb3f3T9mZp9i+mUC/jDRyJaRuBBgMQ2dQRRPf4hNRGS+ZuvR/Dx8fbAWgSxnucECDRnj7PXtaYdyir7uLF/+8bOUSk4mk876OCKy9M22TMA3wtd4ehfMLAN0uLvGU6qoP1/grHXttDQurhUt+7qyjIxPsufwcbasXZF2OCKyRJ22GMDM/t7MVppZO/Az4HEz+2/Jh7Z85PKFRVUIEIvnXVNBgIgsRCVVZ+eHHsybiZ5dOQv4D4lGtYwcH5/kmUMji+7+DJy8Z5RTohGRBagk0TSZWRNRornb3SeYZUE0mZtd+4ZwXxxTz0zV0dLIptVtmlxTRBakkkTz18DTQDvwfTN7EaB7NFUSD0sthuUBptPXldXkmiKyIKdNNO7+SXff6O7/Lqztsht4dQ1iWxZy+QLNjRletGZx3mzv687yxP4hxoultEMRkSWqkmKAtWFJ5J+Y2UNm9pdAZw1iWxb6Bwucs76DxoZKOpe119edpVhynjqwoLXuRGQZq+S32x3AfuAtRNPP7Ae+nGRQy0kuX1gUi53NpFdT0YjIAlWSaM5w9w+5+1Ph9WFOrmIpC3BsdILnj44u2vszQNTbypju04jIvFWSaO4zs6vMLBNev020bows0MAiWuxsJs2NGc5a167KMxGZt0oSzbuAvwfGwusO4PfNrGBmqj5bgP7BxTnH2VS93Vk9SyMi81ZJ1VnW3TPu3hRemdCWdfeVtQiyXuXyBVY0N7BxVVvaoczqvK4szxwaYWS8mHYoIrIEzZhowmJn8fYrpxx7b5JBLRf9gwV6urKLfsLK+B5STjM5i8g8zNaj+T/Ktj815di1CcSy7AzsW9wVZ7H4HpIKAkRkPmZLNDbD9nT7MkcHhsY4MDS+6O/PAGxZs4LWpoxKnEVkXmZLND7D9nT7MkfxzfXFtKrmTDIZixZBU49GROZhtoXPzjOzHUS9l3PCNmH/7MQjq3PxMNRS6NFAFOf3cvvTDkNElqDZEs0v1CyKZag/P0RnWxMbsi1ph1KR87qz3PnQHg4Nj7OmvTntcERkCZlthc3dtQxkuRnIF+jrymK2NG53la9Nc8nZa1OORkSWksU5k2Odc3f68wV6uxd/xVksvpek+zQiMldKNCkYPDZKYbS4qKeemWpDtoXOtiZVnonInCnRpCDuFfQsoURjZvR1axE0EZm7eSUaM7upynEsK3Fp81KpOIv1dWXpzxdwV3W7iFRuvj2ahxbyoWb2cTPbaWY7zOxrZrYqtDeZ2W1m9qiZ/dzMPlh2zeVm1m9mu8zshrL2s8zsR6H9y2bWHNpbwv6ucHzrQmKuplx+iPXZliVXvdXbnaUwWuT5o6NphyIiS8i8Eo27f2OBn7sduNDdLwJyQJxQfgtocfcXAxcTzRK91cwagE8DrwfOB95qZueHaz4KfMLdzwUOA+8M7e8EDof2T4TzFoVcqDhbas7r1iJoIjJ3lSzl/MlpXh8ysyvm+6Hufp+7x1MBPwBsig8B7WbWCLQB48Ax4BXALnd/0t3HiZYquMKi2uDXAHeG628D3hy2rwj7hOOX2iKoJS6VnFy+sOSGzQB6N6jyTETmrpIeTSvwUmAgvC4iSgzvNLO/qEIM1wLfDNt3AsPA88AzwJ+6+yFgI/Bs2TV7Qtta4EhZ0orbKb8mHD8azk/Vs4dHGJ0o0bsEJtOcqnNFE90rW1UQICJzMtvMALGLgFe6+ySAmd0C/AB4FfDoTBeZ2beB7mkO3ejud4VzbgSKwO3h2CuASeBMYDXwg/A+iTKz64DrALZs2ZLoZ8W9gcW8fPNs+rqzGjoTkTmpJNGsBjqIegQA7cAad580s7GZLnL3y2Z7UzN7O/AG4FI/Wcb0NuBb7j4B7DOz/wVsI+qZbC67fBOwFzgIrDKzxtBridsJXzcDe8JQXGc4f7pYbwVuBdi2bVuiJVUD+6I1XXo2LL0eDUSJ5of/cpDiZInGBlXHi8jpVfKb4mPAI2b2eTP7W+Bh4ONm1g7Mq7dhZpcD7wfe5O4jZYeeIbrnQnj/S4CdwI+BnlBh1gxcBdwdEtR3gSvD9dcAd4Xtu8M+4fh3fBHU5fYPFti4qo1sa1PaocxLb1eW8WKJ3YdGTn+yiAgV9Gjc/bNmdg/RsBbAH7n7c2H7v83zc28GWoDt4f78A+5+PVFl2efN7DGiWaI/7+474MSqnvcCDcDn3P2x8F4fAO4wsw8TJcHPhvbPAn9nZruAQ0TJKXW5fGFJLA0wk7jyLDdY4Jz1S7NXJiK1ddpEY2bfAP6eqAcxXI0PDSXH07UPEZU4T3fsHuCeadqf5GQSLG8fnem90jIxWeKJ/UP8at/6tEOZt3M3dGAGOwcLvP7FZ6QdjogsAZUMnf0p8CvA42Z2p5ldaWatCcdVl54+MMzEpC/JZ2hirU0NbF3bfmJ2AxGR06lk6Ox7wPfCQ5OvAd4FfA5YmXBsdSeXjwoBluIzNOXiqWhERCpRUdmQmbUBbwGuB36Rkw9Cyhz05wtkLBp+Wsp6u7M8fWCY0YnJtEMRkSWgkpkBvgL8nKg3czNwjrv/QdKB1aPcYIGta9tpbWpIO5QF6evKUnLYFUq1RURmU0mP5rNEyeV6d/8u8G/M7NMJx1WXcvkCPUtwRoCp4qo53acRkUqcNtG4+73ARWb2MTN7GvgQ0bMtMgejE5M8fXB4SRcCxLauXUFzQ0b3aUSkIjMWA5hZL/DW8DoAfBkwd391jWKrK0/sH6LkS3fqmXKNDRnO2dChyTVFpCKz9Wh2Et2XeYO7v8rdP0U0D5nMQzzMVA89Goge3NTkmiJSidkSzW8SzaL8XTP7GzO7lOhpfZmH/sEhmhqMreva0w6lKnq7sjx3dJRjoxNphyIii9yMicbdv+7uVwHnEc0n9p+BDWZ2i5m9rlYB1otcvsDZ6zpoqpOJKPu6o6IG9WpE5HQqKQYYdve/d/c3Es2O/DDR/GIyB7l8oS7uz8Tih05VECAipzOnP6/d/bC73+rulyYVUD0aGiuy5/Bx+uqgtDm2cVUbHS2N6tGIyGnVxzjOIjcQ/upf6lPPlDMzers62KlEIyKnoURTAycqzupo6Ayi7yeXL7AIlvkRkUVMiaYG+geHaG3KsHn1irRDqaq+riyHRybYPzTjQqsiIko0tTCwr0DPhiyZTH1Vh8fFDXpwU0Rmo0RTA/2Dhbq6PxOLHz5VohGR2SjRJOzw8Dj7CmMnnjupJ2s7WljX0aLJNUVkVko0CcvVYcVZub5uzXkmIrNToklYvSea3q4sufwQpZIqz0Rkeko0Ccvlh8i2NHJGZ2vaoSTivO4sxycm2XP4eNqhiMgipUSTsP4w9YxZfVWcxeKe2s7BYylHIiKLlRJNgtw9muOsTofNAHq6tNqmiDNxpgoAAA16SURBVMxOiSZB+wtjHBmZqKs5zqbqaGlk85o2+vNDaYciIouUEk2C+uu8ECDW16VF0ERkZko0CcqFv/LraXmA6fR2ZXli/xDjxVLaoYjIIqREk6DcYIG17c2s62hJO5RE9XVnKZacpw4Mpx2KiCxCqSUaM/uQme0ws0fM7D4zOzO0m5l90sx2heMvL7vmGjMbCK9rytovNrNHwzWftFDiZWZrzGx7OH+7ma2u5ffYX+eFALF4VmotgiYi00mzR/Nxd7/I3V8K/CPw30P764Ge8LoOuAWipAH8MfBLwCuAPy5LHLcA7yq77vLQfgNwv7v3APeH/ZoolZyBfKHulgaYztnrOmjMGP0qcRaRaaSWaNy9/LdSOxA/Wn4F8AWPPACsMrMzgF8Htrv7IXc/DGwHLg/HVrr7Ax4tjPIF4M1l73Vb2L6trD1xe48cZ3h8kp46rjiLNTdmOHt9O/2DqjwTkRdqTPPDzez/Ba4GjgKvDs0bgWfLTtsT2mZr3zNNO0CXuz8ftgeBrmrGP5uBfWGxs2UwdAZRQcCOPUfTDkNEFqFEezRm9m0z+9k0rysA3P1Gd98M3A68N8lYQm9n2gm5zOw6M3vQzB7cv39/VT4v/uu+Z5kkmr6uLM8cGmF4rJh2KCKyyCSaaNz9Mne/cJrXXVNOvR14S9jeC2wuO7YptM3WvmmadoB8GFojfN03Q5y3uvs2d9+2fv36uX+j08jlC5zR2UpnW1NV3m+xi0u4B/Zp+ExETpVm1VlP2e4VwM6wfTdwdag+uwQ4Goa/7gVeZ2arQxHA64B7w7FjZnZJqDa7Grir7L3i6rRrytoTV6+Lnc3kvJBo9OCmiEyV5j2aj5hZH1ACdgPXh/Z7gH8H7AJGgHcAuPshM/sQ8ONw3v/j7ofC9ruBvwXagG+GF8BHgK+Y2TvDZ/x2kt9QbLLk7No/xCvPXVuLj1sUNq9eQWtThp1KNCIyRWqJxt3fMkO7A++Z4djngM9N0/4gcOE07QeBSxcW6dztPjjMeLG0rHo0mYyFtWmUaETkVJoZIAHxL9vl8AxNub6urB7aFJEXUKJJQP/gEGZw7ob6f4amXF93lv2FMQ4Nj6cdiogsIko0CcjlC2xZs4IVzak+plRz8VBhv+7TiEgZJZoE9OcL9GxYXsNmUFZ5puEzESmjRFNlY8VJnj4wTF/38ho2A1ifbWHViiZVnonIKZRoquypA8MUS76sKs5iZqo8E5EXUqKpsvj+xHKrOIud1x2tthlVqYuIKNFUXS5foDFjnL1u+Q2dQVQQUBgr8tzR0bRDEZFFQommyvoHh9i6rp3mxuX5o+3TVDQiMsXy/G2YoIF9hWWzNMB0TpQ46z6NiATL60GPhI2MF3nm0Ai/+bJNpz+5TnW2NXFGZ6t6NGXGipNRkcikYwYZMzJmNGSiAopoP2ovP56x+PjJNstw2vPny92ZLDmT7rhHc/aV3CmVYNLjbafkYT8cnwxtJ7eja0ruZedF7+dx2xxu4c3lft9c7wxmzGjKGE2NGZoaMjRmjObG6GtTQya8wvFMtN2QsQX9nJcjJZoq2rVvCHeWZWlzud6u7LItcS5Olti1f4gdzx5lx94j7NhzlJ3PFxifLNUshhOJKTMlSRkYvDAphCQglWtuyNDYYCcTUUhKjQ126rFMhqZGozETHW8u246va2wwGsoS19QcVp7UTjlk5ZvTX19+/gved5prLv2FLl66eVWFP4XKKdFUUVxxtlwWO5vJed1ZfvjkQYqTJRob6nd0tlRynj44zKN7j/LTZ4+yY88RHnvuGMcnJgHItjRy4cZO3vGqrVxwZidtTQ2UPPqrvvyXvYftuK38eMmjv+hLpfLjFZzvzmTp5Hb5tXFvKk5GDXEPKRP3tE72kOK/3hvKt8sS2IlzQ1vDie2TPbYGi3sB5cdhyq/NWc2lAzGXvkbJnYlJZ2KyRHHSGQ9fJyZL4eUUSyXGi2E7bi85E8USxVJ0Tfl2MVwXv8fYRImhySLj5dfHnxnepxR6bVPTfXlnzsuOntrOtDsznT/1mvIeY3dnqxLNYpfLF2huzPCiNSvSDiVVvV1Zxosldh8a4Zz19dG7c3eeOzrKjmePsGNvlFR27DlKYTRaUbS1KcMFZ3Zy1Ss2c9GmTi7atIqz1raTyWiIRUSJpopy+SHOXd9R13/FVyKuPOsfLCzZRLO/MMaje4+c6Kk8uvcoB4aiyUKbGozzulfyxpecyUtCUunZoP/uIjNRoqmiXL7AJWcvn8XOZnLuhg4yBj/ZfZhfPnstHa2NNC3iX8JHj0/w6J6j/HTPER7dEyWW+DkgM+jZ0MGv9W040VM5rztLa1NDylGLLB1KNFVy9PgEzx8dXZZTz0zV2tTAOes7+Mw/P8Vn/vmp0JYh29pEtqWRbGsjHa2NZFuaoq+tjaG9bL+1iY6WRlbG57Y2saKpYcFDUSPjRR577hg/fTYa+np071GeOjB84vjWtSu4eOsarg1J5YIzV9Leon8mIguhf0FVMhCeG+ntWppDRdV2y+9dzCPPHmFodILCaJGhsSLHRosURicYGitSGC1yoDBMIT4+XnzBDcupzKCj5WRSypYloRNJ6UQii46vaG7g6YMj0b2VPUcZ2Fc4UVp7RmcrF23q5MqLN0W9lY2r6FzRlPwPR2SZUaKpkv4TiUY9GoiGz+ay8Fup5AyPF08koUJISnGSKoxOMDQaJat4vzBa5NDwOLsPjpw4f6w4fRnxmvZmLtrUya9f2M1LNnXy4k2dbMi2VuvbFZFZKNFUyfqOFl57fhcbV7WlHcqSlMlY6KU0cUbn/N9nvFg6JRENjRXZtLqNjava9JCdSEqUaKrkdRd087oLutMOY9lrbsywprGZNe3NaYciIsHiLQUSEZG6oEQjIiKJUqIREZFEKdGIiEiilGhERCRRSjQiIpIoJRoREUmUEo2IiCTK5rJM6nJgZvuB3fO8fB1woIrhLHX6eZxKP4+T9LM4VT38PF7k7uunO6BEU0Vm9qC7b0s7jsVCP49T6edxkn4Wp6r3n4eGzkREJFFKNCIikiglmuq6Ne0AFhn9PE6ln8dJ+lmcqq5/HrpHIyIiiVKPRkREEqVEUyVmdrmZ9ZvZLjO7Ie140mJmm83su2b2uJk9ZmbvSzumxcDMGszsYTP7x7RjSZuZrTKzO81sp5n93Mx+Oe2Y0mJm/yX8O/mZmX3JzOpy2Vclmiowswbg08DrgfOBt5rZ+elGlZoi8H+6+/nAJcB7lvHPotz7gJ+nHcQi8ZfAt9z9POAlLNOfi5ltBP4Q2ObuFwINwFXpRpUMJZrqeAWwy92fdPdx4A7gipRjSoW7P+/uPwnbBaJfIhvTjSpdZrYJ+PfAZ9KOJW1m1gn8W+CzAO4+7u5H0o0qVY1Am5k1AiuA51KOJxFKNNWxEXi2bH8Py/yXK4CZbQVeBvwo3UhS9xfA+4FS2oEsAmcB+4HPh6HEz5hZe9pBpcHd9wJ/CjwDPA8cdff70o0qGUo0kggz6wD+B/Cf3f1Y2vGkxczeAOxz94fSjmWRaAReDtzi7i8DhoFleU/TzFYTjXycBZwJtJvZ76UbVTKUaKpjL7C5bH9TaFuWzKyJKMnc7u5fTTuelL0SeJOZPU00pPoaM/tiuiGlag+wx93jXu6dRIlnOboMeMrd97v7BPBV4N+kHFMilGiq48dAj5mdZWbNRDf07k45plSYmRGNv//c3f887XjS5u4fdPdN7r6V6P+L77h7Xf7VWgl3HwSeNbO+0HQp8HiKIaXpGeASM1sR/t1cSp0WRjSmHUA9cPeimb0XuJeocuRz7v5YymGl5ZXAfwAeNbNHQtsfufs9KcYki8sfALeHP8qeBN6RcjypcPcfmdmdwE+IqjUfpk5nCNDMACIikigNnYmISKKUaEREJFFKNCIikiglGhERSZQSjYiIJEqJRiRhZjZpZo+UvWZ9Et7Mrjezq6vwuU+b2bqFvo/IQqm8WSRhZjbk7h0pfO7TRDMDH6j1Z4uUU49GJCWhx/ExM3vUzP7VzM4N7TeZ2X8N238Y1vbZYWZ3hLY1Zvb10PaAmV0U2tea2X1hfZPPAFb2Wb8XPuMRM/vrsLSFSE0o0Ygkr23K0NnvlB076u4vBm4mmuV5qhuAl7n7RcD1oe1PgIdD2x8BXwjtfwz8s7tfAHwN2AJgZr8A/A7wSnd/KTAJ/G51v0WRmWkKGpHkHQ+/4KfzpbKvn5jm+A6i6Vq+Dnw9tL0KeAuAu38n9GRWEq3z8puh/Z/M7HA4/1LgYuDH0ZRatAH7FvYtiVROiUYkXT7DduzfEyWQNwI3mtmL5/EZBtzm7h+cx7UiC6ahM5F0/U7Z1x+WHzCzDLDZ3b8LfADoBDqAHxCGvszs14ADYc2f7wNvC+2vB1aHt7ofuNLMNoRja8zsRQl+TyKnUI9GJHltZTNZA3zL3eMS59VmtgMYA9465boG4Ith+WMDPunuR8zsJuBz4boR4Jpw/p8AXzKzx4B/IZqGHnd/3Mz+L+C+kLwmgPcAu6v9jYpMR+XNIilR+bEsFxo6ExGRRKlHIyIiiVKPRkREEqVEIyIiiVKiERGRRCnRiIhIopRoREQkUUo0IiKSqP8N6t2JAOuPIXoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULMUGlmtKEhn"
      },
      "source": [
        "actor_model.save_weights(\"10actor.h5\")\n",
        "critic_model.save_weights(\"10critic.h5\")\n",
        "\n",
        "target_actor.save_weights(\"10target_actor.h5\")\n",
        "target_critic.save_weights(\"10target_critic.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "JE12DV4E06Jq",
        "outputId": "56706c18-d6b7-437a-fd6d-fc9c7709f686"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "from os import path\n",
        "import control\n",
        "from  control.matlab import *\n",
        "import tensorflow as tfl\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#from Buffer import Buffer\n",
        "#from OUActionNoise import OUActionNoise\n",
        "\n",
        "Us = [0]\n",
        "ts = np.array([0])\n",
        "yout = []\n",
        "\n",
        "class environment():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "            self.kp = 34.5047797848936\n",
        "            self.ki = 2.41863698260906\n",
        "            self.kd = 0.013145\n",
        "            self.j = 1 \n",
        "            self.max_input = 500\n",
        "            #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.I = 0\n",
        "            \n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "            self.Us = [0]\n",
        "            self.ts = np.array([0])\n",
        "            self.yout = []\n",
        "            #self.e00 = 0\n",
        "           #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.j = 1 \n",
        "            self.I = 0\n",
        "            u1 = 0\n",
        "            \n",
        "            #self.rl = random.randrange(-self.max_input, self.max_input)\n",
        "            return np.array([self.e2,u1])\n",
        "\n",
        "    def step(self ,a) :\n",
        "            \n",
        "\n",
        "            mot0 = tf([0.4602, 1.882, 2.038, 0.2338, 0.007103], [62.85, 383.5, 803.4, 624.1, 99.79, 5.916, 0.1204])\n",
        "            self.rl = np.clip(a, -self.max_input, self.max_input)[0]\n",
        "            \n",
        "            P = 34.5047797848936*self.e2\n",
        "            self.I = self.I + 2.41863698260906*(self.e2)*0.1\n",
        "            D = -0.013145*(self.e2-self.e1)/0.1\n",
        "            self.u = P + self.I + D\n",
        "            \n",
        "            uu = self.u + self.rl\n",
        "            \n",
        "            self.Us = np.append(self.Us,uu)\n",
        "            self.ts = np.append(self.ts,0.1*self.j)\n",
        "            y, T, xoutd = lsim(3.6*mot0, U=self.Us, T=self.ts)\n",
        "            self.yout.append(y[-1])\n",
        "            #self.ynow = y[-1]\n",
        "            self.e1 = self.e2\n",
        "            self.e2 = 30-y[-1]\n",
        "            self.j+=1\n",
        "            P1 = 34.5047797848936*self.e2\n",
        "            I1 = self.I\n",
        "            I1 = I1 + 2.41863698260906*(self.e2)*0.1\n",
        "            D1 = -0.013145*(self.e2-self.e1)/0.1\n",
        "            u1 = P1 + I1 + D1\n",
        "            \n",
        "            \n",
        "            #reward = 0.8*np.exp(-0.5*(self.e2)**2) + 0.2*np.exp(-np.absolute(self.rl))\n",
        "            reward = -(self.e2)**2\n",
        "\n",
        "            if self.j >= 700:\n",
        "              done = True\n",
        "            else :\n",
        "              done = False\n",
        "            self.state = np.array([self.e2,u1])\n",
        "            return self.state, reward, done, {}\n",
        "\n",
        "env = environment()\n",
        "num_states = 2\n",
        "num_actions = 1\n",
        "upper_bound = 500\n",
        "lower_bound = -500\n",
        "\n",
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tfl.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tfl.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tfl.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tfl.math.reduce_mean(tfl.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tfl.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tfl.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tfl.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tfl.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tfl.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tfl.cast(reward_batch, dtype=tfl.float32)\n",
        "        next_state_batch = tfl.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tfl.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tfl.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tfl.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tfl.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def policy(state, noise_object):\n",
        "    sampled_actions = tfl.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]\n",
        "\n",
        "std_dev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tfl.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tfl.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 20\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(100000, 64)\n",
        "\n",
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tfl.expand_dims(tfl.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "        #print(j)\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    print(\"Episode * {} * Episodic Reward is ==> {}\".format(ep, episodic_reward))\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    #print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/control/timeresp.py:294: UserWarning: return_x specified for a transfer function system. Internal conversion to state space used; results may meaningless.\n",
            "  \"return_x specified for a transfer function system. Internal \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Episodic Reward is ==> -30588.19241705189\n",
            "Episode * 1 * Episodic Reward is ==> -31344.196881767024\n",
            "Episode * 2 * Episodic Reward is ==> -31348.195705950155\n",
            "Episode * 3 * Episodic Reward is ==> -31236.226880870403\n",
            "Episode * 4 * Episodic Reward is ==> -31273.607275714534\n",
            "Episode * 5 * Episodic Reward is ==> -31296.7670064625\n",
            "Episode * 6 * Episodic Reward is ==> -31286.479980018496\n",
            "Episode * 7 * Episodic Reward is ==> -31285.117639957545\n",
            "Episode * 8 * Episodic Reward is ==> -31310.33023292563\n",
            "Episode * 9 * Episodic Reward is ==> -31313.03050196858\n",
            "Episode * 10 * Episodic Reward is ==> -31317.55776639879\n",
            "Episode * 11 * Episodic Reward is ==> -31313.263601332263\n",
            "Episode * 12 * Episodic Reward is ==> -31261.658522321126\n",
            "Episode * 13 * Episodic Reward is ==> -31252.529093542056\n",
            "Episode * 14 * Episodic Reward is ==> -31274.52634832171\n",
            "Episode * 15 * Episodic Reward is ==> -31253.705964393008\n",
            "Episode * 16 * Episodic Reward is ==> -31269.431496604084\n",
            "Episode * 17 * Episodic Reward is ==> -31231.026826265355\n",
            "Episode * 18 * Episodic Reward is ==> -31242.22543607221\n",
            "Episode * 19 * Episodic Reward is ==> -31267.798857427686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfnf7ogz1mXO"
      },
      "source": [
        "actor_model.save_weights(\"20actor.h5\")\n",
        "critic_model.save_weights(\"20critic.h5\")\n",
        "\n",
        "target_actor.save_weights(\"20target_actor.h5\")\n",
        "target_critic.save_weights(\"20target_critic.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LM3P_3U1AlP"
      },
      "source": [
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "daWnAkhr1Dbq",
        "outputId": "a71025c4-c26d-49b2-a8c6-fc1f68917591"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "from os import path\n",
        "import control\n",
        "from  control.matlab import *\n",
        "import tensorflow as tfl\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#from Buffer import Buffer\n",
        "#from OUActionNoise import OUActionNoise\n",
        "\n",
        "Us = [0]\n",
        "ts = np.array([0])\n",
        "yout = []\n",
        "\n",
        "class environment():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "            self.kp = 34.5047797848936\n",
        "            self.ki = 2.41863698260906\n",
        "            self.kd = 0.013145\n",
        "            self.j = 1 \n",
        "            self.max_input = 100\n",
        "            #self.e0 = 0\n",
        "            self.e1 = 30\n",
        "            self.e2 = 30\n",
        "            self.I = 0\n",
        "            \n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "            self.Us = [0]\n",
        "            self.ts = np.array([0])\n",
        "            self.yout = []\n",
        "            #self.e00 = 0\n",
        "           #self.e0 = 0\n",
        "            self.e1 = 30\n",
        "            self.e2 = 30\n",
        "            self.j = 1 \n",
        "            self.I = 0\n",
        "            u1 = 0\n",
        "            \n",
        "            #self.rl = random.randrange(-self.max_input, self.max_input)\n",
        "            return np.array([self.e2,u1])\n",
        "\n",
        "    def step(self ,a) :\n",
        "            \n",
        "\n",
        "            mot0 = tf([0.4602, 1.882, 2.038, 0.2338, 0.007103], [62.85, 383.5, 803.4, 624.1, 99.79, 5.916, 0.1204])\n",
        "            self.rl = np.clip(a, -self.max_input, self.max_input)[0]\n",
        "            \n",
        "            P = 34.5047797848936*self.e2\n",
        "            self.I = self.I + 2.41863698260906*(self.e2)*0.1\n",
        "            D = -0.013145*(self.e2-self.e1)/0.1\n",
        "            self.u = P + self.I + D\n",
        "            uu = self.u + self.rl \n",
        " \n",
        "            self.Us = np.append(self.Us,uu)\n",
        "            self.ts = np.append(self.ts,0.1*self.j)\n",
        "            y, T, xoutd = lsim(3.6*mot0, U=self.Us, T=self.ts)\n",
        "            self.yout.append(y[-1])\n",
        "            #self.ynow = y[-1]\n",
        "            self.e1 = self.e2\n",
        "            self.e2 = 30-y[-1]\n",
        "            self.j+=1\n",
        "            P1 = 34.5047797848936*self.e2\n",
        "            I1 = self.I\n",
        "            I1 = I1 + 2.41863698260906*(self.e2)*0.1\n",
        "            D1 = -0.013145*(self.e2-self.e1)/0.1\n",
        "            u1 = P1 + I1 + D1\n",
        "            \n",
        "            \n",
        "            #reward = 0.8*np.exp(-0.5*(self.e2)**2) + 0.2*np.exp(-np.absolute(self.rl))\n",
        "            #reward = -(self.e2)**2\n",
        "            reward = -np.absolute(self.e2)\n",
        "\n",
        "            if self.j >= 500:\n",
        "              done = True\n",
        "            else :\n",
        "              done = False\n",
        "            self.state = np.array([self.e2,u1])\n",
        "            return self.state, reward, done, {}\n",
        "\n",
        "env = environment()\n",
        "num_states = 2\n",
        "num_actions = 1\n",
        "upper_bound = 100\n",
        "lower_bound = -100\n",
        "\n",
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tfl.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tfl.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tfl.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tfl.math.reduce_mean(tfl.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tfl.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tfl.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tfl.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tfl.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tfl.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tfl.cast(reward_batch, dtype=tfl.float32)\n",
        "        next_state_batch = tfl.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tfl.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tfl.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tfl.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tfl.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def policy(state, noise_object):\n",
        "    sampled_actions = tfl.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]\n",
        "\n",
        "std_dev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.001\n",
        "actor_lr = 0.0001\n",
        "\n",
        "critic_optimizer = tfl.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tfl.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 1000\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.0005\n",
        "\n",
        "buffer = Buffer(100000, 128)\n",
        "\n",
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "    episodic_action = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tfl.expand_dims(tfl.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "        episodic_action += action[0]\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "        #print(j)\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    print(\"Episode * {} * Episodic Reward is ==> {}\".format(ep, episodic_reward))\n",
        "    print(\"Episode * {} * Avg Action is ==> {}\".format(ep, episodic_action/500))\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    #print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/control/timeresp.py:294: UserWarning: return_x specified for a transfer function system. Internal conversion to state space used; results may meaningless.\n",
            "  \"return_x specified for a transfer function system. Internal \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Episodic Reward is ==> -669.2923905143066\n",
            "Episode * 0 * Avg Action is ==> 48.787734449332255\n",
            "Episode * 1 * Episodic Reward is ==> -667.9182130652644\n",
            "Episode * 1 * Avg Action is ==> 48.584209807373675\n",
            "Episode * 2 * Episodic Reward is ==> -671.4704379797429\n",
            "Episode * 2 * Avg Action is ==> 49.0260615659248\n",
            "Episode * 3 * Episodic Reward is ==> -669.3265345455233\n",
            "Episode * 3 * Avg Action is ==> 48.46167705966349\n",
            "Episode * 4 * Episodic Reward is ==> -678.1097630920178\n",
            "Episode * 4 * Avg Action is ==> 50.510698470367394\n",
            "Episode * 5 * Episodic Reward is ==> -683.0643979272736\n",
            "Episode * 5 * Avg Action is ==> 50.76714741038688\n",
            "Episode * 6 * Episodic Reward is ==> -679.7579422340341\n",
            "Episode * 6 * Avg Action is ==> 51.72708223519089\n",
            "Episode * 7 * Episodic Reward is ==> -678.3922237969341\n",
            "Episode * 7 * Avg Action is ==> 50.51604347587667\n",
            "Episode * 8 * Episodic Reward is ==> -691.0552196767107\n",
            "Episode * 8 * Avg Action is ==> 52.198891133304144\n",
            "Episode * 9 * Episodic Reward is ==> -680.8721749809679\n",
            "Episode * 9 * Avg Action is ==> 50.78084440785612\n",
            "Episode * 10 * Episodic Reward is ==> -687.1300518946741\n",
            "Episode * 10 * Avg Action is ==> 51.66260181616683\n",
            "Episode * 11 * Episodic Reward is ==> -688.357479364835\n",
            "Episode * 11 * Avg Action is ==> 53.8683867688666\n",
            "Episode * 12 * Episodic Reward is ==> -679.8354850281294\n",
            "Episode * 12 * Avg Action is ==> 51.834379783122486\n",
            "Episode * 13 * Episodic Reward is ==> -683.6793943381446\n",
            "Episode * 13 * Avg Action is ==> 52.263316421514844\n",
            "Episode * 14 * Episodic Reward is ==> -684.9715953883205\n",
            "Episode * 14 * Avg Action is ==> 52.406464434005066\n",
            "Episode * 15 * Episodic Reward is ==> -688.13719539583\n",
            "Episode * 15 * Avg Action is ==> 52.256808341805005\n",
            "Episode * 16 * Episodic Reward is ==> -688.0289189135577\n",
            "Episode * 16 * Avg Action is ==> 53.171900760158366\n",
            "Episode * 17 * Episodic Reward is ==> -700.9681747024242\n",
            "Episode * 17 * Avg Action is ==> 54.970681750482456\n",
            "Episode * 18 * Episodic Reward is ==> -691.2320916960288\n",
            "Episode * 18 * Avg Action is ==> 53.74381975793448\n",
            "Episode * 19 * Episodic Reward is ==> -705.2501459407416\n",
            "Episode * 19 * Avg Action is ==> 55.610873581832706\n",
            "Episode * 20 * Episodic Reward is ==> -696.4659608103246\n",
            "Episode * 20 * Avg Action is ==> 55.87307827482577\n",
            "Episode * 21 * Episodic Reward is ==> -687.9450266413354\n",
            "Episode * 21 * Avg Action is ==> 53.18168852021786\n",
            "Episode * 22 * Episodic Reward is ==> -687.0859715307411\n",
            "Episode * 22 * Avg Action is ==> 53.50871429054562\n",
            "Episode * 23 * Episodic Reward is ==> -700.8436505038511\n",
            "Episode * 23 * Avg Action is ==> 56.68417036931641\n",
            "Episode * 24 * Episodic Reward is ==> -696.7835672912488\n",
            "Episode * 24 * Avg Action is ==> 54.90718863294363\n",
            "Episode * 25 * Episodic Reward is ==> -702.9127714797072\n",
            "Episode * 25 * Avg Action is ==> 56.865545469409895\n",
            "Episode * 26 * Episodic Reward is ==> -716.5548703506163\n",
            "Episode * 26 * Avg Action is ==> 60.001583825444385\n",
            "Episode * 27 * Episodic Reward is ==> -716.6046782321075\n",
            "Episode * 27 * Avg Action is ==> 60.48748081862954\n",
            "Episode * 28 * Episodic Reward is ==> -690.2757136167911\n",
            "Episode * 28 * Avg Action is ==> 55.00945040136\n",
            "Episode * 29 * Episodic Reward is ==> -687.5194731270598\n",
            "Episode * 29 * Avg Action is ==> 54.12423619892242\n",
            "Episode * 30 * Episodic Reward is ==> -721.9133650040267\n",
            "Episode * 30 * Avg Action is ==> 59.75484276417921\n",
            "Episode * 31 * Episodic Reward is ==> -711.0394196000188\n",
            "Episode * 31 * Avg Action is ==> 58.57297523977275\n",
            "Episode * 32 * Episodic Reward is ==> -708.0960496749686\n",
            "Episode * 32 * Avg Action is ==> 57.70053632467718\n",
            "Episode * 33 * Episodic Reward is ==> -684.5838451344015\n",
            "Episode * 33 * Avg Action is ==> 53.33502366211777\n",
            "Episode * 34 * Episodic Reward is ==> -688.2565402815308\n",
            "Episode * 34 * Avg Action is ==> 53.440711772610825\n",
            "Episode * 35 * Episodic Reward is ==> -687.6044652770915\n",
            "Episode * 35 * Avg Action is ==> 54.217105221412666\n",
            "Episode * 36 * Episodic Reward is ==> -684.1216772148965\n",
            "Episode * 36 * Avg Action is ==> 53.17316814596331\n",
            "Episode * 37 * Episodic Reward is ==> -686.4423144845767\n",
            "Episode * 37 * Avg Action is ==> 53.42627524474161\n",
            "Episode * 38 * Episodic Reward is ==> -695.2250886737356\n",
            "Episode * 38 * Avg Action is ==> 53.38119362261564\n",
            "Episode * 39 * Episodic Reward is ==> -686.6575103730858\n",
            "Episode * 39 * Avg Action is ==> 51.39064665891504\n",
            "Episode * 40 * Episodic Reward is ==> -708.2101159278362\n",
            "Episode * 40 * Avg Action is ==> 54.70158199262095\n",
            "Episode * 41 * Episodic Reward is ==> -697.9480033708413\n",
            "Episode * 41 * Avg Action is ==> 53.1704600106594\n",
            "Episode * 42 * Episodic Reward is ==> -691.3875979906703\n",
            "Episode * 42 * Avg Action is ==> 53.49526630799224\n",
            "Episode * 43 * Episodic Reward is ==> -692.1906013356386\n",
            "Episode * 43 * Avg Action is ==> 52.445338232718775\n",
            "Episode * 44 * Episodic Reward is ==> -686.192801255377\n",
            "Episode * 44 * Avg Action is ==> 51.3825790311027\n",
            "Episode * 45 * Episodic Reward is ==> -683.1029921027296\n",
            "Episode * 45 * Avg Action is ==> 50.736569413995845\n",
            "Episode * 46 * Episodic Reward is ==> -719.5235816520236\n",
            "Episode * 46 * Avg Action is ==> 57.61206899627225\n",
            "Episode * 47 * Episodic Reward is ==> -693.4437662583771\n",
            "Episode * 47 * Avg Action is ==> 54.52215291338665\n",
            "Episode * 48 * Episodic Reward is ==> -692.2238143315215\n",
            "Episode * 48 * Avg Action is ==> 50.919438324704\n",
            "Episode * 49 * Episodic Reward is ==> -708.0899786961027\n",
            "Episode * 49 * Avg Action is ==> 53.369723912796196\n",
            "Episode * 50 * Episodic Reward is ==> -722.5450115368222\n",
            "Episode * 50 * Avg Action is ==> 57.87161740262625\n",
            "Episode * 51 * Episodic Reward is ==> -731.4759079502448\n",
            "Episode * 51 * Avg Action is ==> 61.20598487297148\n",
            "Episode * 52 * Episodic Reward is ==> -713.4640254493552\n",
            "Episode * 52 * Avg Action is ==> 56.97398751549564\n",
            "Episode * 53 * Episodic Reward is ==> -683.2911483378037\n",
            "Episode * 53 * Avg Action is ==> 50.27792247252936\n",
            "Episode * 54 * Episodic Reward is ==> -693.9931271806732\n",
            "Episode * 54 * Avg Action is ==> 52.62998721128287\n",
            "Episode * 55 * Episodic Reward is ==> -688.8535316202223\n",
            "Episode * 55 * Avg Action is ==> 51.33857057468097\n",
            "Episode * 56 * Episodic Reward is ==> -693.0735345389611\n",
            "Episode * 56 * Avg Action is ==> 52.965527126356\n",
            "Episode * 57 * Episodic Reward is ==> -702.9272081841269\n",
            "Episode * 57 * Avg Action is ==> 53.528857772832424\n",
            "Episode * 58 * Episodic Reward is ==> -689.1007027057706\n",
            "Episode * 58 * Avg Action is ==> 52.98960074550425\n",
            "Episode * 59 * Episodic Reward is ==> -695.6986012114821\n",
            "Episode * 59 * Avg Action is ==> 53.62896107706445\n",
            "Episode * 60 * Episodic Reward is ==> -708.9748801953986\n",
            "Episode * 60 * Avg Action is ==> 55.81770755292429\n",
            "Episode * 61 * Episodic Reward is ==> -712.6409451681565\n",
            "Episode * 61 * Avg Action is ==> 56.26247134802476\n",
            "Episode * 62 * Episodic Reward is ==> -717.4741176860995\n",
            "Episode * 62 * Avg Action is ==> 58.82712705913217\n",
            "Episode * 63 * Episodic Reward is ==> -710.6510237912439\n",
            "Episode * 63 * Avg Action is ==> 57.472801454030545\n",
            "Episode * 64 * Episodic Reward is ==> -703.248307659051\n",
            "Episode * 64 * Avg Action is ==> 56.090418343720195\n",
            "Episode * 65 * Episodic Reward is ==> -702.4849040157021\n",
            "Episode * 65 * Avg Action is ==> 55.13666185404065\n",
            "Episode * 66 * Episodic Reward is ==> -701.1462859648894\n",
            "Episode * 66 * Avg Action is ==> 55.00115311300948\n",
            "Episode * 67 * Episodic Reward is ==> -684.0412815203769\n",
            "Episode * 67 * Avg Action is ==> 50.999698005371314\n",
            "Episode * 68 * Episodic Reward is ==> -695.3673934467417\n",
            "Episode * 68 * Avg Action is ==> 53.34993483317118\n",
            "Episode * 69 * Episodic Reward is ==> -698.0407513699541\n",
            "Episode * 69 * Avg Action is ==> 53.57307169311097\n",
            "Episode * 70 * Episodic Reward is ==> -704.1091963655089\n",
            "Episode * 70 * Avg Action is ==> 55.54753961413984\n",
            "Episode * 71 * Episodic Reward is ==> -701.2573860145613\n",
            "Episode * 71 * Avg Action is ==> 54.9995664811861\n",
            "Episode * 72 * Episodic Reward is ==> -702.3914776850191\n",
            "Episode * 72 * Avg Action is ==> 54.921610163640345\n",
            "Episode * 73 * Episodic Reward is ==> -699.6234061873791\n",
            "Episode * 73 * Avg Action is ==> 54.59606496327023\n",
            "Episode * 74 * Episodic Reward is ==> -699.2092704853453\n",
            "Episode * 74 * Avg Action is ==> 54.62284942739729\n",
            "Episode * 75 * Episodic Reward is ==> -712.2787088873575\n",
            "Episode * 75 * Avg Action is ==> 55.69079236930668\n",
            "Episode * 76 * Episodic Reward is ==> -734.2411884457935\n",
            "Episode * 76 * Avg Action is ==> 61.87112786089011\n",
            "Episode * 77 * Episodic Reward is ==> -728.7487696487669\n",
            "Episode * 77 * Avg Action is ==> 62.20364876401863\n",
            "Episode * 78 * Episodic Reward is ==> -734.2127230681999\n",
            "Episode * 78 * Avg Action is ==> 63.117974614739204\n",
            "Episode * 79 * Episodic Reward is ==> -733.3594494350779\n",
            "Episode * 79 * Avg Action is ==> 63.27704495005725\n",
            "Episode * 80 * Episodic Reward is ==> -700.45325881111\n",
            "Episode * 80 * Avg Action is ==> 58.37996306147973\n",
            "Episode * 81 * Episodic Reward is ==> -699.3360696021678\n",
            "Episode * 81 * Avg Action is ==> 54.78546896582155\n",
            "Episode * 82 * Episodic Reward is ==> -712.3667594109679\n",
            "Episode * 82 * Avg Action is ==> 56.594208542759446\n",
            "Episode * 83 * Episodic Reward is ==> -677.0881895767509\n",
            "Episode * 83 * Avg Action is ==> 52.01242983828795\n",
            "Episode * 84 * Episodic Reward is ==> -676.536018417304\n",
            "Episode * 84 * Avg Action is ==> 49.84495169118753\n",
            "Episode * 85 * Episodic Reward is ==> -690.3617767725834\n",
            "Episode * 85 * Avg Action is ==> 52.22715117967534\n",
            "Episode * 86 * Episodic Reward is ==> -700.297461704226\n",
            "Episode * 86 * Avg Action is ==> 55.899506659720224\n",
            "Episode * 87 * Episodic Reward is ==> -685.4570040470356\n",
            "Episode * 87 * Avg Action is ==> 52.79766821878875\n",
            "Episode * 88 * Episodic Reward is ==> -688.2949340686102\n",
            "Episode * 88 * Avg Action is ==> 53.713620396572125\n",
            "Episode * 89 * Episodic Reward is ==> -694.8828335964843\n",
            "Episode * 89 * Avg Action is ==> 54.73450567913842\n",
            "Episode * 90 * Episodic Reward is ==> -688.176300391031\n",
            "Episode * 90 * Avg Action is ==> 52.69403315160946\n",
            "Episode * 91 * Episodic Reward is ==> -686.7563783027952\n",
            "Episode * 91 * Avg Action is ==> 52.60913246243046\n",
            "Episode * 92 * Episodic Reward is ==> -688.5654759414804\n",
            "Episode * 92 * Avg Action is ==> 52.508743426057755\n",
            "Episode * 93 * Episodic Reward is ==> -687.0015673129244\n",
            "Episode * 93 * Avg Action is ==> 52.29984796114677\n",
            "Episode * 94 * Episodic Reward is ==> -687.7390940490324\n",
            "Episode * 94 * Avg Action is ==> 52.74127193473114\n",
            "Episode * 95 * Episodic Reward is ==> -690.6318493508868\n",
            "Episode * 95 * Avg Action is ==> 52.48606584088732\n",
            "Episode * 96 * Episodic Reward is ==> -684.1179917066909\n",
            "Episode * 96 * Avg Action is ==> 51.45490106681007\n",
            "Episode * 97 * Episodic Reward is ==> -687.6682501766479\n",
            "Episode * 97 * Avg Action is ==> 52.38148940508091\n",
            "Episode * 98 * Episodic Reward is ==> -687.6305856950286\n",
            "Episode * 98 * Avg Action is ==> 52.26401025802438\n",
            "Episode * 99 * Episodic Reward is ==> -685.1174366127331\n",
            "Episode * 99 * Avg Action is ==> 51.76607370665349\n",
            "Episode * 100 * Episodic Reward is ==> -681.650197528977\n",
            "Episode * 100 * Avg Action is ==> 50.50184439395682\n",
            "Episode * 101 * Episodic Reward is ==> -680.7985980007902\n",
            "Episode * 101 * Avg Action is ==> 49.926841802883985\n",
            "Episode * 102 * Episodic Reward is ==> -682.8862884565581\n",
            "Episode * 102 * Avg Action is ==> 50.262029990988815\n",
            "Episode * 103 * Episodic Reward is ==> -680.9468072403415\n",
            "Episode * 103 * Avg Action is ==> 49.9357482853178\n",
            "Episode * 104 * Episodic Reward is ==> -679.8442087774719\n",
            "Episode * 104 * Avg Action is ==> 49.10760436541004\n",
            "Episode * 105 * Episodic Reward is ==> -682.3651484770677\n",
            "Episode * 105 * Avg Action is ==> 49.61519391514302\n",
            "Episode * 106 * Episodic Reward is ==> -681.9996791806416\n",
            "Episode * 106 * Avg Action is ==> 49.65769130184091\n",
            "Episode * 107 * Episodic Reward is ==> -679.903387375343\n",
            "Episode * 107 * Avg Action is ==> 49.19329581988526\n",
            "Episode * 108 * Episodic Reward is ==> -680.3150579814159\n",
            "Episode * 108 * Avg Action is ==> 49.53698820118178\n",
            "Episode * 109 * Episodic Reward is ==> -682.1207798994301\n",
            "Episode * 109 * Avg Action is ==> 49.454308675953214\n",
            "Episode * 110 * Episodic Reward is ==> -681.8845663702997\n",
            "Episode * 110 * Avg Action is ==> 49.59104838545025\n",
            "Episode * 111 * Episodic Reward is ==> -680.4126394937628\n",
            "Episode * 111 * Avg Action is ==> 49.26908845194232\n",
            "Episode * 112 * Episodic Reward is ==> -679.2935543769271\n",
            "Episode * 112 * Avg Action is ==> 49.11164899676413\n",
            "Episode * 113 * Episodic Reward is ==> -677.1326707929798\n",
            "Episode * 113 * Avg Action is ==> 48.801716665370584\n",
            "Episode * 114 * Episodic Reward is ==> -677.3916631153797\n",
            "Episode * 114 * Avg Action is ==> 48.76966879424251\n",
            "Episode * 115 * Episodic Reward is ==> -668.1779865525763\n",
            "Episode * 115 * Avg Action is ==> 48.75343589209768\n",
            "Episode * 116 * Episodic Reward is ==> -668.3069698914829\n",
            "Episode * 116 * Avg Action is ==> 49.405959031022235\n",
            "Episode * 117 * Episodic Reward is ==> -668.7138455297028\n",
            "Episode * 117 * Avg Action is ==> 49.191031585488425\n",
            "Episode * 118 * Episodic Reward is ==> -665.7110720282994\n",
            "Episode * 118 * Avg Action is ==> 48.65337779605789\n",
            "Episode * 119 * Episodic Reward is ==> -668.903197012669\n",
            "Episode * 119 * Avg Action is ==> 49.3464831477048\n",
            "Episode * 120 * Episodic Reward is ==> -674.891072002957\n",
            "Episode * 120 * Avg Action is ==> 47.894628302981985\n",
            "Episode * 121 * Episodic Reward is ==> -674.7330395781152\n",
            "Episode * 121 * Avg Action is ==> 48.03502802555342\n",
            "Episode * 122 * Episodic Reward is ==> -674.1939275665211\n",
            "Episode * 122 * Avg Action is ==> 47.70653810484721\n",
            "Episode * 123 * Episodic Reward is ==> -676.3323932352828\n",
            "Episode * 123 * Avg Action is ==> 48.248632288028205\n",
            "Episode * 124 * Episodic Reward is ==> -674.2089712567996\n",
            "Episode * 124 * Avg Action is ==> 47.888800213859795\n",
            "Episode * 125 * Episodic Reward is ==> -673.1305451979343\n",
            "Episode * 125 * Avg Action is ==> 47.490886560173955\n",
            "Episode * 126 * Episodic Reward is ==> -673.6073377941966\n",
            "Episode * 126 * Avg Action is ==> 47.74382880497583\n",
            "Episode * 127 * Episodic Reward is ==> -673.6820772700298\n",
            "Episode * 127 * Avg Action is ==> 47.759480358646\n",
            "Episode * 128 * Episodic Reward is ==> -671.3939840085618\n",
            "Episode * 128 * Avg Action is ==> 46.96331510289048\n",
            "Episode * 129 * Episodic Reward is ==> -671.3738669809755\n",
            "Episode * 129 * Avg Action is ==> 46.84438731209499\n",
            "Episode * 130 * Episodic Reward is ==> -670.9168531738273\n",
            "Episode * 130 * Avg Action is ==> 46.687975656049915\n",
            "Episode * 131 * Episodic Reward is ==> -671.9381636052152\n",
            "Episode * 131 * Avg Action is ==> 46.543297095352564\n",
            "Episode * 132 * Episodic Reward is ==> -674.1952522061648\n",
            "Episode * 132 * Avg Action is ==> 47.33220319298301\n",
            "Episode * 133 * Episodic Reward is ==> -674.2084628270685\n",
            "Episode * 133 * Avg Action is ==> 47.30345915629282\n",
            "Episode * 134 * Episodic Reward is ==> -673.669367933018\n",
            "Episode * 134 * Avg Action is ==> 47.32342915249635\n",
            "Episode * 135 * Episodic Reward is ==> -673.1588033488553\n",
            "Episode * 135 * Avg Action is ==> 47.19086784330748\n",
            "Episode * 136 * Episodic Reward is ==> -671.3843825750355\n",
            "Episode * 136 * Avg Action is ==> 46.56581516800495\n",
            "Episode * 137 * Episodic Reward is ==> -669.2283142362763\n",
            "Episode * 137 * Avg Action is ==> 46.27744439148967\n",
            "Episode * 138 * Episodic Reward is ==> -669.3621882291701\n",
            "Episode * 138 * Avg Action is ==> 46.50019143974346\n",
            "Episode * 139 * Episodic Reward is ==> -680.9513567947632\n",
            "Episode * 139 * Avg Action is ==> 47.64254751215878\n",
            "Episode * 140 * Episodic Reward is ==> -671.2296829814611\n",
            "Episode * 140 * Avg Action is ==> 46.770800996362844\n",
            "Episode * 141 * Episodic Reward is ==> -670.3099509373446\n",
            "Episode * 141 * Avg Action is ==> 46.559896911933855\n",
            "Episode * 142 * Episodic Reward is ==> -670.7826595024097\n",
            "Episode * 142 * Avg Action is ==> 46.51222974615251\n",
            "Episode * 143 * Episodic Reward is ==> -668.440227006313\n",
            "Episode * 143 * Avg Action is ==> 46.20829317582208\n",
            "Episode * 144 * Episodic Reward is ==> -673.3157341437478\n",
            "Episode * 144 * Avg Action is ==> 46.568103919006404\n",
            "Episode * 145 * Episodic Reward is ==> -668.6891658666044\n",
            "Episode * 145 * Avg Action is ==> 46.05638098543303\n",
            "Episode * 146 * Episodic Reward is ==> -669.4214014951713\n",
            "Episode * 146 * Avg Action is ==> 46.25814364653835\n",
            "Episode * 147 * Episodic Reward is ==> -667.5506609792109\n",
            "Episode * 147 * Avg Action is ==> 45.95122041768075\n",
            "Episode * 148 * Episodic Reward is ==> -668.735633636565\n",
            "Episode * 148 * Avg Action is ==> 46.25266523617188\n",
            "Episode * 149 * Episodic Reward is ==> -669.7396471346173\n",
            "Episode * 149 * Avg Action is ==> 46.42476202979579\n",
            "Episode * 150 * Episodic Reward is ==> -656.5841776952209\n",
            "Episode * 150 * Avg Action is ==> 44.88241601099738\n",
            "Episode * 151 * Episodic Reward is ==> -653.9983413963918\n",
            "Episode * 151 * Avg Action is ==> 43.36649727137053\n",
            "Episode * 152 * Episodic Reward is ==> -655.6503253288323\n",
            "Episode * 152 * Avg Action is ==> 43.39343981433268\n",
            "Episode * 153 * Episodic Reward is ==> -678.8637540181004\n",
            "Episode * 153 * Avg Action is ==> 47.14154001167118\n",
            "Episode * 154 * Episodic Reward is ==> -660.0624693602527\n",
            "Episode * 154 * Avg Action is ==> 45.25837925364793\n",
            "Episode * 155 * Episodic Reward is ==> -656.6605117888726\n",
            "Episode * 155 * Avg Action is ==> 43.3949688353297\n",
            "Episode * 156 * Episodic Reward is ==> -688.6433221567398\n",
            "Episode * 156 * Avg Action is ==> 46.64313852481053\n",
            "Episode * 157 * Episodic Reward is ==> -656.7276090965405\n",
            "Episode * 157 * Avg Action is ==> 43.860280539352985\n",
            "Episode * 158 * Episodic Reward is ==> -654.8494277213161\n",
            "Episode * 158 * Avg Action is ==> 43.250935679071816\n",
            "Episode * 159 * Episodic Reward is ==> -652.757601219279\n",
            "Episode * 159 * Avg Action is ==> 42.57029814050068\n",
            "Episode * 160 * Episodic Reward is ==> -652.3741527564123\n",
            "Episode * 160 * Avg Action is ==> 42.526362253802084\n",
            "Episode * 161 * Episodic Reward is ==> -650.4069141330078\n",
            "Episode * 161 * Avg Action is ==> 41.897424441651445\n",
            "Episode * 162 * Episodic Reward is ==> -650.8324716473028\n",
            "Episode * 162 * Avg Action is ==> 41.98010118875807\n",
            "Episode * 163 * Episodic Reward is ==> -648.5909330949082\n",
            "Episode * 163 * Avg Action is ==> 41.86305721318172\n",
            "Episode * 164 * Episodic Reward is ==> -646.6783864188412\n",
            "Episode * 164 * Avg Action is ==> 41.72746158587343\n",
            "Episode * 165 * Episodic Reward is ==> -645.8251806549638\n",
            "Episode * 165 * Avg Action is ==> 41.184305769303045\n",
            "Episode * 166 * Episodic Reward is ==> -642.1627523302197\n",
            "Episode * 166 * Avg Action is ==> 40.36634090892452\n",
            "Episode * 167 * Episodic Reward is ==> -641.0400427596179\n",
            "Episode * 167 * Avg Action is ==> 40.28433951725194\n",
            "Episode * 168 * Episodic Reward is ==> -645.1718755596128\n",
            "Episode * 168 * Avg Action is ==> 40.376763884654444\n",
            "Episode * 169 * Episodic Reward is ==> -638.8138260152404\n",
            "Episode * 169 * Avg Action is ==> 39.74979788753554\n",
            "Episode * 170 * Episodic Reward is ==> -627.7165395342208\n",
            "Episode * 170 * Avg Action is ==> 37.08808271418989\n",
            "Episode * 171 * Episodic Reward is ==> -624.4073369468217\n",
            "Episode * 171 * Avg Action is ==> 36.32420595333727\n",
            "Episode * 172 * Episodic Reward is ==> -624.7345891110464\n",
            "Episode * 172 * Avg Action is ==> 36.32141792056195\n",
            "Episode * 173 * Episodic Reward is ==> -626.4948315953504\n",
            "Episode * 173 * Avg Action is ==> 36.726677347967794\n",
            "Episode * 174 * Episodic Reward is ==> -626.1815099213484\n",
            "Episode * 174 * Avg Action is ==> 36.730090379124086\n",
            "Episode * 175 * Episodic Reward is ==> -627.518319401441\n",
            "Episode * 175 * Avg Action is ==> 37.03456357803633\n",
            "Episode * 176 * Episodic Reward is ==> -626.2357591206389\n",
            "Episode * 176 * Avg Action is ==> 36.622228568593\n",
            "Episode * 177 * Episodic Reward is ==> -625.7298968992675\n",
            "Episode * 177 * Avg Action is ==> 36.55829150148031\n",
            "Episode * 178 * Episodic Reward is ==> -624.1570249432057\n",
            "Episode * 178 * Avg Action is ==> 36.4257851978031\n",
            "Episode * 179 * Episodic Reward is ==> -624.0604356220956\n",
            "Episode * 179 * Avg Action is ==> 36.43638321965207\n",
            "Episode * 180 * Episodic Reward is ==> -621.9904410402354\n",
            "Episode * 180 * Avg Action is ==> 36.09128105327517\n",
            "Episode * 181 * Episodic Reward is ==> -621.7028410206457\n",
            "Episode * 181 * Avg Action is ==> 36.014078021849485\n",
            "Episode * 182 * Episodic Reward is ==> -619.7123267039879\n",
            "Episode * 182 * Avg Action is ==> 35.41689720857349\n",
            "Episode * 183 * Episodic Reward is ==> -620.3203321828723\n",
            "Episode * 183 * Avg Action is ==> 35.669613103789196\n",
            "Episode * 184 * Episodic Reward is ==> -617.5824860672419\n",
            "Episode * 184 * Avg Action is ==> 35.01696415709213\n",
            "Episode * 185 * Episodic Reward is ==> -617.567417980331\n",
            "Episode * 185 * Avg Action is ==> 34.505919206458884\n",
            "Episode * 186 * Episodic Reward is ==> -616.6783565185126\n",
            "Episode * 186 * Avg Action is ==> 34.499741760219436\n",
            "Episode * 187 * Episodic Reward is ==> -618.2730467708056\n",
            "Episode * 187 * Avg Action is ==> 35.04444766452324\n",
            "Episode * 188 * Episodic Reward is ==> -617.6017972923696\n",
            "Episode * 188 * Avg Action is ==> 34.74820032390298\n",
            "Episode * 189 * Episodic Reward is ==> -617.5384083831705\n",
            "Episode * 189 * Avg Action is ==> 34.859741233170475\n",
            "Episode * 190 * Episodic Reward is ==> -617.293143171692\n",
            "Episode * 190 * Avg Action is ==> 34.72025729668657\n",
            "Episode * 191 * Episodic Reward is ==> -615.8663053545735\n",
            "Episode * 191 * Avg Action is ==> 34.488348003043214\n",
            "Episode * 192 * Episodic Reward is ==> -616.2231499739988\n",
            "Episode * 192 * Avg Action is ==> 34.488708790370126\n",
            "Episode * 193 * Episodic Reward is ==> -614.5485184202652\n",
            "Episode * 193 * Avg Action is ==> 34.22454586529871\n",
            "Episode * 194 * Episodic Reward is ==> -615.0364315536664\n",
            "Episode * 194 * Avg Action is ==> 34.36166379400823\n",
            "Episode * 195 * Episodic Reward is ==> -614.5660073652008\n",
            "Episode * 195 * Avg Action is ==> 34.60751213041172\n",
            "Episode * 196 * Episodic Reward is ==> -613.1957523596064\n",
            "Episode * 196 * Avg Action is ==> 34.17609307384043\n",
            "Episode * 197 * Episodic Reward is ==> -611.8439827111375\n",
            "Episode * 197 * Avg Action is ==> 33.78247831779492\n",
            "Episode * 198 * Episodic Reward is ==> -613.6014404611843\n",
            "Episode * 198 * Avg Action is ==> 34.11742667127708\n",
            "Episode * 199 * Episodic Reward is ==> -612.8927039863796\n",
            "Episode * 199 * Avg Action is ==> 34.20159717298771\n",
            "Episode * 200 * Episodic Reward is ==> -612.435114185644\n",
            "Episode * 200 * Avg Action is ==> 33.86914213042682\n",
            "Episode * 201 * Episodic Reward is ==> -612.1057629103806\n",
            "Episode * 201 * Avg Action is ==> 33.80593088988554\n",
            "Episode * 202 * Episodic Reward is ==> -608.8359779757249\n",
            "Episode * 202 * Avg Action is ==> 33.47446759729941\n",
            "Episode * 203 * Episodic Reward is ==> -607.5605791805688\n",
            "Episode * 203 * Avg Action is ==> 32.47440948107434\n",
            "Episode * 204 * Episodic Reward is ==> -608.8956057985004\n",
            "Episode * 204 * Avg Action is ==> 32.89921681610251\n",
            "Episode * 205 * Episodic Reward is ==> -609.9014682112341\n",
            "Episode * 205 * Avg Action is ==> 33.096095641516044\n",
            "Episode * 206 * Episodic Reward is ==> -606.7888481949919\n",
            "Episode * 206 * Avg Action is ==> 32.806516179098594\n",
            "Episode * 207 * Episodic Reward is ==> -604.3662568241112\n",
            "Episode * 207 * Avg Action is ==> 32.14080626949715\n",
            "Episode * 208 * Episodic Reward is ==> -605.620104010337\n",
            "Episode * 208 * Avg Action is ==> 32.163751848081404\n",
            "Episode * 209 * Episodic Reward is ==> -605.5184610830712\n",
            "Episode * 209 * Avg Action is ==> 32.13317260607477\n",
            "Episode * 210 * Episodic Reward is ==> -604.1359839333053\n",
            "Episode * 210 * Avg Action is ==> 31.742966038035302\n",
            "Episode * 211 * Episodic Reward is ==> -603.2939922374638\n",
            "Episode * 211 * Avg Action is ==> 31.472178071031287\n",
            "Episode * 212 * Episodic Reward is ==> -602.3610190277547\n",
            "Episode * 212 * Avg Action is ==> 31.508891195889042\n",
            "Episode * 213 * Episodic Reward is ==> -603.808014150196\n",
            "Episode * 213 * Avg Action is ==> 31.667754638779368\n",
            "Episode * 214 * Episodic Reward is ==> -600.7837561460251\n",
            "Episode * 214 * Avg Action is ==> 31.26455361539367\n",
            "Episode * 215 * Episodic Reward is ==> -602.2382054763586\n",
            "Episode * 215 * Avg Action is ==> 31.38595778620608\n",
            "Episode * 216 * Episodic Reward is ==> -600.6012715139391\n",
            "Episode * 216 * Avg Action is ==> 31.372039407636773\n",
            "Episode * 217 * Episodic Reward is ==> -600.0022166353319\n",
            "Episode * 217 * Avg Action is ==> 31.203445864727158\n",
            "Episode * 218 * Episodic Reward is ==> -598.9131738130668\n",
            "Episode * 218 * Avg Action is ==> 30.81184120074736\n",
            "Episode * 219 * Episodic Reward is ==> -598.1787118200928\n",
            "Episode * 219 * Avg Action is ==> 30.807055727158428\n",
            "Episode * 220 * Episodic Reward is ==> -598.3473371096295\n",
            "Episode * 220 * Avg Action is ==> 30.62401646977622\n",
            "Episode * 221 * Episodic Reward is ==> -598.6382219892438\n",
            "Episode * 221 * Avg Action is ==> 30.900488066369743\n",
            "Episode * 222 * Episodic Reward is ==> -598.4199229852445\n",
            "Episode * 222 * Avg Action is ==> 30.804227089373633\n",
            "Episode * 223 * Episodic Reward is ==> -597.1474019572324\n",
            "Episode * 223 * Avg Action is ==> 30.782966994570742\n",
            "Episode * 224 * Episodic Reward is ==> -615.2544010876206\n",
            "Episode * 224 * Avg Action is ==> 35.342919158882346\n",
            "Episode * 225 * Episodic Reward is ==> -613.0993537888812\n",
            "Episode * 225 * Avg Action is ==> 34.36537994399313\n",
            "Episode * 226 * Episodic Reward is ==> -615.5245403656149\n",
            "Episode * 226 * Avg Action is ==> 34.497400272881166\n",
            "Episode * 227 * Episodic Reward is ==> -616.6861121984348\n",
            "Episode * 227 * Avg Action is ==> 34.307181204511664\n",
            "Episode * 228 * Episodic Reward is ==> -613.2717639517767\n",
            "Episode * 228 * Avg Action is ==> 34.190546739357444\n",
            "Episode * 229 * Episodic Reward is ==> -611.560748275386\n",
            "Episode * 229 * Avg Action is ==> 33.903321894650105\n",
            "Episode * 230 * Episodic Reward is ==> -609.6180835090925\n",
            "Episode * 230 * Avg Action is ==> 33.466717098975785\n",
            "Episode * 231 * Episodic Reward is ==> -607.6813924653612\n",
            "Episode * 231 * Avg Action is ==> 33.233151050526295\n",
            "Episode * 232 * Episodic Reward is ==> -606.6653324373505\n",
            "Episode * 232 * Avg Action is ==> 33.02067477896018\n",
            "Episode * 233 * Episodic Reward is ==> -605.4678045928771\n",
            "Episode * 233 * Avg Action is ==> 32.73118302076506\n",
            "Episode * 234 * Episodic Reward is ==> -604.9822686840853\n",
            "Episode * 234 * Avg Action is ==> 32.592890126163894\n",
            "Episode * 235 * Episodic Reward is ==> -603.715449175719\n",
            "Episode * 235 * Avg Action is ==> 32.14008926859165\n",
            "Episode * 236 * Episodic Reward is ==> -606.6969659952775\n",
            "Episode * 236 * Avg Action is ==> 32.62683907702448\n",
            "Episode * 237 * Episodic Reward is ==> -604.139289700409\n",
            "Episode * 237 * Avg Action is ==> 32.321605198864276\n",
            "Episode * 238 * Episodic Reward is ==> -604.9874468830253\n",
            "Episode * 238 * Avg Action is ==> 32.41816084191543\n",
            "Episode * 239 * Episodic Reward is ==> -601.2706857359769\n",
            "Episode * 239 * Avg Action is ==> 31.841259583991167\n",
            "Episode * 240 * Episodic Reward is ==> -600.4045128491983\n",
            "Episode * 240 * Avg Action is ==> 31.54691633614902\n",
            "Episode * 241 * Episodic Reward is ==> -602.9958022035008\n",
            "Episode * 241 * Avg Action is ==> 31.62484957241698\n",
            "Episode * 242 * Episodic Reward is ==> -607.1087576374988\n",
            "Episode * 242 * Avg Action is ==> 32.7530138059815\n",
            "Episode * 243 * Episodic Reward is ==> -599.9362393117929\n",
            "Episode * 243 * Avg Action is ==> 31.447086557859333\n",
            "Episode * 244 * Episodic Reward is ==> -597.151339696575\n",
            "Episode * 244 * Avg Action is ==> 30.684395982875493\n",
            "Episode * 245 * Episodic Reward is ==> -598.3954138077239\n",
            "Episode * 245 * Avg Action is ==> 30.795776121866883\n",
            "Episode * 246 * Episodic Reward is ==> -599.3805411543385\n",
            "Episode * 246 * Avg Action is ==> 30.977139189746328\n",
            "Episode * 247 * Episodic Reward is ==> -599.3686128384628\n",
            "Episode * 247 * Avg Action is ==> 31.214327997448265\n",
            "Episode * 248 * Episodic Reward is ==> -598.3791217507369\n",
            "Episode * 248 * Avg Action is ==> 30.964155141526465\n",
            "Episode * 249 * Episodic Reward is ==> -598.3433281659175\n",
            "Episode * 249 * Avg Action is ==> 30.97300037679336\n",
            "Episode * 250 * Episodic Reward is ==> -600.0070712573709\n",
            "Episode * 250 * Avg Action is ==> 31.22594645655934\n",
            "Episode * 251 * Episodic Reward is ==> -599.5691034729332\n",
            "Episode * 251 * Avg Action is ==> 31.29064677346288\n",
            "Episode * 252 * Episodic Reward is ==> -597.5943281693089\n",
            "Episode * 252 * Avg Action is ==> 30.838620011893735\n",
            "Episode * 253 * Episodic Reward is ==> -596.9575966808239\n",
            "Episode * 253 * Avg Action is ==> 30.710130293031124\n",
            "Episode * 254 * Episodic Reward is ==> -597.0785002381365\n",
            "Episode * 254 * Avg Action is ==> 30.48373403775868\n",
            "Episode * 255 * Episodic Reward is ==> -597.4536890065496\n",
            "Episode * 255 * Avg Action is ==> 30.694196170331637\n",
            "Episode * 256 * Episodic Reward is ==> -596.3389414759257\n",
            "Episode * 256 * Avg Action is ==> 30.449074378968426\n",
            "Episode * 257 * Episodic Reward is ==> -596.1364478260299\n",
            "Episode * 257 * Avg Action is ==> 30.388211712114597\n",
            "Episode * 258 * Episodic Reward is ==> -594.6435960857077\n",
            "Episode * 258 * Avg Action is ==> 30.06719373025606\n",
            "Episode * 259 * Episodic Reward is ==> -594.5883245071998\n",
            "Episode * 259 * Avg Action is ==> 29.69505156429797\n",
            "Episode * 260 * Episodic Reward is ==> -593.724881704522\n",
            "Episode * 260 * Avg Action is ==> 29.463360964749523\n",
            "Episode * 261 * Episodic Reward is ==> -595.9196257149102\n",
            "Episode * 261 * Avg Action is ==> 29.785699730957013\n",
            "Episode * 262 * Episodic Reward is ==> -595.2522438659474\n",
            "Episode * 262 * Avg Action is ==> 30.07207730559777\n",
            "Episode * 263 * Episodic Reward is ==> -595.6821627401565\n",
            "Episode * 263 * Avg Action is ==> 30.221841549723052\n",
            "Episode * 264 * Episodic Reward is ==> -594.3605765690907\n",
            "Episode * 264 * Avg Action is ==> 29.857228269686406\n",
            "Episode * 265 * Episodic Reward is ==> -592.2936867366412\n",
            "Episode * 265 * Avg Action is ==> 29.866478785470242\n",
            "Episode * 266 * Episodic Reward is ==> -591.5543455239651\n",
            "Episode * 266 * Avg Action is ==> 29.748204471866458\n",
            "Episode * 267 * Episodic Reward is ==> -583.9107100728182\n",
            "Episode * 267 * Avg Action is ==> 29.77089108607453\n",
            "Episode * 268 * Episodic Reward is ==> -585.9410967166067\n",
            "Episode * 268 * Avg Action is ==> 30.129912703841068\n",
            "Episode * 269 * Episodic Reward is ==> -585.0308195587693\n",
            "Episode * 269 * Avg Action is ==> 30.15817361073247\n",
            "Episode * 270 * Episodic Reward is ==> -584.9321498921315\n",
            "Episode * 270 * Avg Action is ==> 29.94518893744409\n",
            "Episode * 271 * Episodic Reward is ==> -584.0729522786017\n",
            "Episode * 271 * Avg Action is ==> 29.90985487154917\n",
            "Episode * 272 * Episodic Reward is ==> -585.689594789084\n",
            "Episode * 272 * Avg Action is ==> 29.909263881080573\n",
            "Episode * 273 * Episodic Reward is ==> -585.8845304115958\n",
            "Episode * 273 * Avg Action is ==> 30.375321612526434\n",
            "Episode * 274 * Episodic Reward is ==> -585.472938208552\n",
            "Episode * 274 * Avg Action is ==> 30.260227498992244\n",
            "Episode * 275 * Episodic Reward is ==> -585.1294186045177\n",
            "Episode * 275 * Avg Action is ==> 30.22211573721603\n",
            "Episode * 276 * Episodic Reward is ==> -584.6835623949838\n",
            "Episode * 276 * Avg Action is ==> 29.953128643559758\n",
            "Episode * 277 * Episodic Reward is ==> -584.5245140255954\n",
            "Episode * 277 * Avg Action is ==> 29.796800995678403\n",
            "Episode * 278 * Episodic Reward is ==> -585.1910024938095\n",
            "Episode * 278 * Avg Action is ==> 29.937470806583953\n",
            "Episode * 279 * Episodic Reward is ==> -586.7932345787291\n",
            "Episode * 279 * Avg Action is ==> 30.199576405919746\n",
            "Episode * 280 * Episodic Reward is ==> -585.3898427117731\n",
            "Episode * 280 * Avg Action is ==> 30.13719688901027\n",
            "Episode * 281 * Episodic Reward is ==> -583.7151759902468\n",
            "Episode * 281 * Avg Action is ==> 29.714118041266634\n",
            "Episode * 282 * Episodic Reward is ==> -583.523569183991\n",
            "Episode * 282 * Avg Action is ==> 29.423307699183486\n",
            "Episode * 283 * Episodic Reward is ==> -584.1360846507984\n",
            "Episode * 283 * Avg Action is ==> 29.572244306217332\n",
            "Episode * 284 * Episodic Reward is ==> -584.5956469369374\n",
            "Episode * 284 * Avg Action is ==> 29.732199408864645\n",
            "Episode * 285 * Episodic Reward is ==> -585.7352699070539\n",
            "Episode * 285 * Avg Action is ==> 30.095956420677727\n",
            "Episode * 286 * Episodic Reward is ==> -585.4717477896202\n",
            "Episode * 286 * Avg Action is ==> 30.050409643959707\n",
            "Episode * 287 * Episodic Reward is ==> -585.7434344538534\n",
            "Episode * 287 * Avg Action is ==> 30.191141493658808\n",
            "Episode * 288 * Episodic Reward is ==> -584.2410603111964\n",
            "Episode * 288 * Avg Action is ==> 29.85044268300177\n",
            "Episode * 289 * Episodic Reward is ==> -584.7373598702089\n",
            "Episode * 289 * Avg Action is ==> 29.845867960450892\n",
            "Episode * 290 * Episodic Reward is ==> -585.1665646170508\n",
            "Episode * 290 * Avg Action is ==> 29.958095050784205\n",
            "Episode * 291 * Episodic Reward is ==> -584.3156099325844\n",
            "Episode * 291 * Avg Action is ==> 29.901994020146805\n",
            "Episode * 292 * Episodic Reward is ==> -584.5962902121975\n",
            "Episode * 292 * Avg Action is ==> 29.885487051132205\n",
            "Episode * 293 * Episodic Reward is ==> -584.3257657644571\n",
            "Episode * 293 * Avg Action is ==> 29.820398016088834\n",
            "Episode * 294 * Episodic Reward is ==> -584.2635236971922\n",
            "Episode * 294 * Avg Action is ==> 29.832855452413188\n",
            "Episode * 295 * Episodic Reward is ==> -592.5364353636761\n",
            "Episode * 295 * Avg Action is ==> 29.29280559696545\n",
            "Episode * 296 * Episodic Reward is ==> -592.665444650061\n",
            "Episode * 296 * Avg Action is ==> 29.360445548365497\n",
            "Episode * 297 * Episodic Reward is ==> -592.1109605079414\n",
            "Episode * 297 * Avg Action is ==> 29.31839491520437\n",
            "Episode * 298 * Episodic Reward is ==> -600.9659208571907\n",
            "Episode * 298 * Avg Action is ==> 30.695455987893986\n",
            "Episode * 299 * Episodic Reward is ==> -604.2247255730953\n",
            "Episode * 299 * Avg Action is ==> 32.16135914071421\n",
            "Episode * 300 * Episodic Reward is ==> -601.2071080052892\n",
            "Episode * 300 * Avg Action is ==> 31.566218794358036\n",
            "Episode * 301 * Episodic Reward is ==> -603.0453523726818\n",
            "Episode * 301 * Avg Action is ==> 31.62414462831213\n",
            "Episode * 302 * Episodic Reward is ==> -593.1920518841188\n",
            "Episode * 302 * Avg Action is ==> 30.139976405290938\n",
            "Episode * 303 * Episodic Reward is ==> -591.7283292964672\n",
            "Episode * 303 * Avg Action is ==> 29.330606080715594\n",
            "Episode * 304 * Episodic Reward is ==> -591.5695747298923\n",
            "Episode * 304 * Avg Action is ==> 29.06683502107302\n",
            "Episode * 305 * Episodic Reward is ==> -591.6318712482904\n",
            "Episode * 305 * Avg Action is ==> 29.137796496973504\n",
            "Episode * 306 * Episodic Reward is ==> -597.1383601423537\n",
            "Episode * 306 * Avg Action is ==> 29.785210620232135\n",
            "Episode * 307 * Episodic Reward is ==> -600.7718210402209\n",
            "Episode * 307 * Avg Action is ==> 31.396016854877473\n",
            "Episode * 308 * Episodic Reward is ==> -598.8879557096708\n",
            "Episode * 308 * Avg Action is ==> 31.0789969973215\n",
            "Episode * 309 * Episodic Reward is ==> -590.8492547924311\n",
            "Episode * 309 * Avg Action is ==> 29.71491861561515\n",
            "Episode * 310 * Episodic Reward is ==> -590.4675732449964\n",
            "Episode * 310 * Avg Action is ==> 28.945635703770957\n",
            "Episode * 311 * Episodic Reward is ==> -591.2039791074075\n",
            "Episode * 311 * Avg Action is ==> 29.036700003710322\n",
            "Episode * 312 * Episodic Reward is ==> -598.4999269466545\n",
            "Episode * 312 * Avg Action is ==> 30.952068641471097\n",
            "Episode * 313 * Episodic Reward is ==> -599.946241187318\n",
            "Episode * 313 * Avg Action is ==> 31.114557085880854\n",
            "Episode * 314 * Episodic Reward is ==> -598.6716320407515\n",
            "Episode * 314 * Avg Action is ==> 30.857169761256987\n",
            "Episode * 315 * Episodic Reward is ==> -599.4158798006071\n",
            "Episode * 315 * Avg Action is ==> 30.864178938636087\n",
            "Episode * 316 * Episodic Reward is ==> -599.3316064892447\n",
            "Episode * 316 * Avg Action is ==> 30.702597094055765\n",
            "Episode * 317 * Episodic Reward is ==> -600.5717412816583\n",
            "Episode * 317 * Avg Action is ==> 31.0975423815187\n",
            "Episode * 318 * Episodic Reward is ==> -599.3493533684715\n",
            "Episode * 318 * Avg Action is ==> 30.991885888577595\n",
            "Episode * 319 * Episodic Reward is ==> -598.8504592308165\n",
            "Episode * 319 * Avg Action is ==> 30.653942632082273\n",
            "Episode * 320 * Episodic Reward is ==> -598.9434385360261\n",
            "Episode * 320 * Avg Action is ==> 30.9474722021897\n",
            "Episode * 321 * Episodic Reward is ==> -598.1239087214321\n",
            "Episode * 321 * Avg Action is ==> 30.800471499067\n",
            "Episode * 322 * Episodic Reward is ==> -599.5825449175593\n",
            "Episode * 322 * Avg Action is ==> 30.93521209831634\n",
            "Episode * 323 * Episodic Reward is ==> -592.226906427949\n",
            "Episode * 323 * Avg Action is ==> 29.88812335767513\n",
            "Episode * 324 * Episodic Reward is ==> -596.4665267159585\n",
            "Episode * 324 * Avg Action is ==> 29.679195561788507\n",
            "Episode * 325 * Episodic Reward is ==> -598.9590109471434\n",
            "Episode * 325 * Avg Action is ==> 30.827784303585585\n",
            "Episode * 326 * Episodic Reward is ==> -597.795104337171\n",
            "Episode * 326 * Avg Action is ==> 30.630887947548036\n",
            "Episode * 327 * Episodic Reward is ==> -599.2537903006878\n",
            "Episode * 327 * Avg Action is ==> 30.939647736155127\n",
            "Episode * 328 * Episodic Reward is ==> -596.5878135807515\n",
            "Episode * 328 * Avg Action is ==> 30.167043543826402\n",
            "Episode * 329 * Episodic Reward is ==> -597.8197590756324\n",
            "Episode * 329 * Avg Action is ==> 30.625608116872407\n",
            "Episode * 330 * Episodic Reward is ==> -598.1787395904938\n",
            "Episode * 330 * Avg Action is ==> 30.854821292811327\n",
            "Episode * 331 * Episodic Reward is ==> -590.7148152674516\n",
            "Episode * 331 * Avg Action is ==> 29.276232095821644\n",
            "Episode * 332 * Episodic Reward is ==> -590.5842811250033\n",
            "Episode * 332 * Avg Action is ==> 29.024395053919243\n",
            "Episode * 333 * Episodic Reward is ==> -591.0268206796101\n",
            "Episode * 333 * Avg Action is ==> 28.996474245509457\n",
            "Episode * 334 * Episodic Reward is ==> -589.058522699744\n",
            "Episode * 334 * Avg Action is ==> 28.869215034488533\n",
            "Episode * 335 * Episodic Reward is ==> -595.7755402249323\n",
            "Episode * 335 * Avg Action is ==> 30.049968749376795\n",
            "Episode * 336 * Episodic Reward is ==> -597.532606190808\n",
            "Episode * 336 * Avg Action is ==> 30.593762127261073\n",
            "Episode * 337 * Episodic Reward is ==> -587.6334978592304\n",
            "Episode * 337 * Avg Action is ==> 29.42345854221639\n",
            "Episode * 338 * Episodic Reward is ==> -589.6955106976848\n",
            "Episode * 338 * Avg Action is ==> 31.14281567992654\n",
            "Episode * 339 * Episodic Reward is ==> -589.7249856082004\n",
            "Episode * 339 * Avg Action is ==> 31.24798148287491\n",
            "Episode * 340 * Episodic Reward is ==> -589.0394003695183\n",
            "Episode * 340 * Avg Action is ==> 31.222611269754097\n",
            "Episode * 341 * Episodic Reward is ==> -586.2313443552539\n",
            "Episode * 341 * Avg Action is ==> 30.561819651321816\n",
            "Episode * 342 * Episodic Reward is ==> -587.4129884016193\n",
            "Episode * 342 * Avg Action is ==> 30.33610722521031\n",
            "Episode * 343 * Episodic Reward is ==> -580.1705503272482\n",
            "Episode * 343 * Avg Action is ==> 28.924195098466377\n",
            "Episode * 344 * Episodic Reward is ==> -586.5299153395154\n",
            "Episode * 344 * Avg Action is ==> 30.05465279682113\n",
            "Episode * 345 * Episodic Reward is ==> -580.7593414837157\n",
            "Episode * 345 * Avg Action is ==> 29.413811106613185\n",
            "Episode * 346 * Episodic Reward is ==> -586.2748664332864\n",
            "Episode * 346 * Avg Action is ==> 29.475184757406186\n",
            "Episode * 347 * Episodic Reward is ==> -587.4059760919233\n",
            "Episode * 347 * Avg Action is ==> 30.604837057178617\n",
            "Episode * 348 * Episodic Reward is ==> -588.7306345154353\n",
            "Episode * 348 * Avg Action is ==> 30.762827896676807\n",
            "Episode * 349 * Episodic Reward is ==> -587.7682088987443\n",
            "Episode * 349 * Avg Action is ==> 30.885447366857065\n",
            "Episode * 350 * Episodic Reward is ==> -588.2115666258929\n",
            "Episode * 350 * Avg Action is ==> 30.802969564612898\n",
            "Episode * 351 * Episodic Reward is ==> -586.7954542819492\n",
            "Episode * 351 * Avg Action is ==> 30.63012553796569\n",
            "Episode * 352 * Episodic Reward is ==> -586.3830654834236\n",
            "Episode * 352 * Avg Action is ==> 30.130755446542246\n",
            "Episode * 353 * Episodic Reward is ==> -586.6440067798723\n",
            "Episode * 353 * Avg Action is ==> 29.928177368068372\n",
            "Episode * 354 * Episodic Reward is ==> -586.3696331548317\n",
            "Episode * 354 * Avg Action is ==> 30.29059147667846\n",
            "Episode * 355 * Episodic Reward is ==> -587.8111637820881\n",
            "Episode * 355 * Avg Action is ==> 30.571636898690155\n",
            "Episode * 356 * Episodic Reward is ==> -586.5580952651442\n",
            "Episode * 356 * Avg Action is ==> 30.477478558425027\n",
            "Episode * 357 * Episodic Reward is ==> -586.9780627797873\n",
            "Episode * 357 * Avg Action is ==> 30.600122929477823\n",
            "Episode * 358 * Episodic Reward is ==> -594.9903888115873\n",
            "Episode * 358 * Avg Action is ==> 30.670995081319465\n",
            "Episode * 359 * Episodic Reward is ==> -585.7114619765714\n",
            "Episode * 359 * Avg Action is ==> 30.123617965791993\n",
            "Episode * 360 * Episodic Reward is ==> -585.7035919313698\n",
            "Episode * 360 * Avg Action is ==> 30.363344061664673\n",
            "Episode * 361 * Episodic Reward is ==> -583.1156435564824\n",
            "Episode * 361 * Avg Action is ==> 29.785282945168778\n",
            "Episode * 362 * Episodic Reward is ==> -584.7621770979632\n",
            "Episode * 362 * Avg Action is ==> 29.48019154412255\n",
            "Episode * 363 * Episodic Reward is ==> -584.9238417781977\n",
            "Episode * 363 * Avg Action is ==> 29.97709688721152\n",
            "Episode * 364 * Episodic Reward is ==> -585.9356440316292\n",
            "Episode * 364 * Avg Action is ==> 30.13215253040968\n",
            "Episode * 365 * Episodic Reward is ==> -588.4936952632487\n",
            "Episode * 365 * Avg Action is ==> 29.758562169504234\n",
            "Episode * 366 * Episodic Reward is ==> -578.5263085897078\n",
            "Episode * 366 * Avg Action is ==> 28.35532525747053\n",
            "Episode * 367 * Episodic Reward is ==> -579.8504523084735\n",
            "Episode * 367 * Avg Action is ==> 28.738847327848422\n",
            "Episode * 368 * Episodic Reward is ==> -579.7369944821013\n",
            "Episode * 368 * Avg Action is ==> 28.654839461009832\n",
            "Episode * 369 * Episodic Reward is ==> -579.9205695249447\n",
            "Episode * 369 * Avg Action is ==> 28.88829458721431\n",
            "Episode * 370 * Episodic Reward is ==> -579.9079907951077\n",
            "Episode * 370 * Avg Action is ==> 28.8618931212024\n",
            "Episode * 371 * Episodic Reward is ==> -582.4069267748449\n",
            "Episode * 371 * Avg Action is ==> 28.979867025131714\n",
            "Episode * 372 * Episodic Reward is ==> -585.4009987655616\n",
            "Episode * 372 * Avg Action is ==> 29.989794334211762\n",
            "Episode * 373 * Episodic Reward is ==> -586.281294210666\n",
            "Episode * 373 * Avg Action is ==> 30.295320166054836\n",
            "Episode * 374 * Episodic Reward is ==> -586.9408810684745\n",
            "Episode * 374 * Avg Action is ==> 30.599756858987266\n",
            "Episode * 375 * Episodic Reward is ==> -585.2210239322982\n",
            "Episode * 375 * Avg Action is ==> 30.22030512576199\n",
            "Episode * 376 * Episodic Reward is ==> -583.6812641129743\n",
            "Episode * 376 * Avg Action is ==> 29.841244691921204\n",
            "Episode * 377 * Episodic Reward is ==> -585.5556004318527\n",
            "Episode * 377 * Avg Action is ==> 29.760228926052704\n",
            "Episode * 378 * Episodic Reward is ==> -585.513479001793\n",
            "Episode * 378 * Avg Action is ==> 29.945814282861203\n",
            "Episode * 379 * Episodic Reward is ==> -580.3421807784006\n",
            "Episode * 379 * Avg Action is ==> 28.861356298318547\n",
            "Episode * 380 * Episodic Reward is ==> -579.7364055197796\n",
            "Episode * 380 * Avg Action is ==> 28.79120027544014\n",
            "Episode * 381 * Episodic Reward is ==> -579.4681483000908\n",
            "Episode * 381 * Avg Action is ==> 28.685964392599004\n",
            "Episode * 382 * Episodic Reward is ==> -578.9083963853968\n",
            "Episode * 382 * Avg Action is ==> 28.413588550104922\n",
            "Episode * 383 * Episodic Reward is ==> -579.4822564228785\n",
            "Episode * 383 * Avg Action is ==> 28.66550360827637\n",
            "Episode * 384 * Episodic Reward is ==> -578.234586037615\n",
            "Episode * 384 * Avg Action is ==> 28.40339770446939\n",
            "Episode * 385 * Episodic Reward is ==> -577.1979053587868\n",
            "Episode * 385 * Avg Action is ==> 28.116555324016325\n",
            "Episode * 386 * Episodic Reward is ==> -577.0569003255331\n",
            "Episode * 386 * Avg Action is ==> 28.127301544596236\n",
            "Episode * 387 * Episodic Reward is ==> -576.6164588503163\n",
            "Episode * 387 * Avg Action is ==> 27.75944495155342\n",
            "Episode * 388 * Episodic Reward is ==> -576.344917710231\n",
            "Episode * 388 * Avg Action is ==> 27.56728281044617\n",
            "Episode * 389 * Episodic Reward is ==> -577.2624490649797\n",
            "Episode * 389 * Avg Action is ==> 27.763297186252647\n",
            "Episode * 390 * Episodic Reward is ==> -576.582446813344\n",
            "Episode * 390 * Avg Action is ==> 27.84836142697879\n",
            "Episode * 391 * Episodic Reward is ==> -577.1041713300888\n",
            "Episode * 391 * Avg Action is ==> 27.78570377077405\n",
            "Episode * 392 * Episodic Reward is ==> -577.2445352319833\n",
            "Episode * 392 * Avg Action is ==> 27.94703224387562\n",
            "Episode * 393 * Episodic Reward is ==> -577.8293672613167\n",
            "Episode * 393 * Avg Action is ==> 28.169546614348754\n",
            "Episode * 394 * Episodic Reward is ==> -576.8007315657396\n",
            "Episode * 394 * Avg Action is ==> 27.784034550733068\n",
            "Episode * 395 * Episodic Reward is ==> -579.6664233137071\n",
            "Episode * 395 * Avg Action is ==> 28.374045757694486\n",
            "Episode * 396 * Episodic Reward is ==> -578.5497526314089\n",
            "Episode * 396 * Avg Action is ==> 28.554255208171995\n",
            "Episode * 397 * Episodic Reward is ==> -575.4357944413438\n",
            "Episode * 397 * Avg Action is ==> 27.678683479921524\n",
            "Episode * 398 * Episodic Reward is ==> -577.0477098736309\n",
            "Episode * 398 * Avg Action is ==> 27.79831416105114\n",
            "Episode * 399 * Episodic Reward is ==> -577.2139874322448\n",
            "Episode * 399 * Avg Action is ==> 27.703699956015686\n",
            "Episode * 400 * Episodic Reward is ==> -587.4877882734954\n",
            "Episode * 400 * Avg Action is ==> 30.06702488252073\n",
            "Episode * 401 * Episodic Reward is ==> -586.5788245018813\n",
            "Episode * 401 * Avg Action is ==> 30.154973961760817\n",
            "Episode * 402 * Episodic Reward is ==> -587.2615606330527\n",
            "Episode * 402 * Avg Action is ==> 30.08636546855653\n",
            "Episode * 403 * Episodic Reward is ==> -584.5636428229858\n",
            "Episode * 403 * Avg Action is ==> 29.895427324719588\n",
            "Episode * 404 * Episodic Reward is ==> -585.4867789644164\n",
            "Episode * 404 * Avg Action is ==> 30.06322391463648\n",
            "Episode * 405 * Episodic Reward is ==> -586.8940718213462\n",
            "Episode * 405 * Avg Action is ==> 30.13722500111135\n",
            "Episode * 406 * Episodic Reward is ==> -587.1076161571216\n",
            "Episode * 406 * Avg Action is ==> 30.370874519203664\n",
            "Episode * 407 * Episodic Reward is ==> -585.0036815728904\n",
            "Episode * 407 * Avg Action is ==> 29.784211780657014\n",
            "Episode * 408 * Episodic Reward is ==> -585.4592221131375\n",
            "Episode * 408 * Avg Action is ==> 29.68999035270962\n",
            "Episode * 409 * Episodic Reward is ==> -585.3616041244126\n",
            "Episode * 409 * Avg Action is ==> 29.677743000741224\n",
            "Episode * 410 * Episodic Reward is ==> -585.0727780730479\n",
            "Episode * 410 * Avg Action is ==> 29.797205576053866\n",
            "Episode * 411 * Episodic Reward is ==> -583.8434914752395\n",
            "Episode * 411 * Avg Action is ==> 29.24466428629216\n",
            "Episode * 412 * Episodic Reward is ==> -582.8532256653261\n",
            "Episode * 412 * Avg Action is ==> 29.24407544477352\n",
            "Episode * 413 * Episodic Reward is ==> -583.8023939143109\n",
            "Episode * 413 * Avg Action is ==> 28.76356052068293\n",
            "Episode * 414 * Episodic Reward is ==> -582.381791345472\n",
            "Episode * 414 * Avg Action is ==> 28.72213774706615\n",
            "Episode * 415 * Episodic Reward is ==> -578.4341830043383\n",
            "Episode * 415 * Avg Action is ==> 28.24168944219725\n",
            "Episode * 416 * Episodic Reward is ==> -581.3147838151468\n",
            "Episode * 416 * Avg Action is ==> 28.833993961499242\n",
            "Episode * 417 * Episodic Reward is ==> -582.0453007925119\n",
            "Episode * 417 * Avg Action is ==> 28.754896544692684\n",
            "Episode * 418 * Episodic Reward is ==> -584.7175800918346\n",
            "Episode * 418 * Avg Action is ==> 29.68473796742312\n",
            "Episode * 419 * Episodic Reward is ==> -581.5777286471817\n",
            "Episode * 419 * Avg Action is ==> 29.00441357381782\n",
            "Episode * 420 * Episodic Reward is ==> -583.5057335591339\n",
            "Episode * 420 * Avg Action is ==> 29.312752131231413\n",
            "Episode * 421 * Episodic Reward is ==> -583.0550872498021\n",
            "Episode * 421 * Avg Action is ==> 29.32922958957594\n",
            "Episode * 422 * Episodic Reward is ==> -581.921088860435\n",
            "Episode * 422 * Avg Action is ==> 29.095413185095875\n",
            "Episode * 423 * Episodic Reward is ==> -582.4071338728403\n",
            "Episode * 423 * Avg Action is ==> 29.110461662753252\n",
            "Episode * 424 * Episodic Reward is ==> -580.9118765107313\n",
            "Episode * 424 * Avg Action is ==> 28.720861448370822\n",
            "Episode * 425 * Episodic Reward is ==> -577.6502811494674\n",
            "Episode * 425 * Avg Action is ==> 28.02470084673828\n",
            "Episode * 426 * Episodic Reward is ==> -581.2957318752821\n",
            "Episode * 426 * Avg Action is ==> 28.825075736836084\n",
            "Episode * 427 * Episodic Reward is ==> -581.7412411723658\n",
            "Episode * 427 * Avg Action is ==> 28.80015621822854\n",
            "Episode * 428 * Episodic Reward is ==> -579.7754627258792\n",
            "Episode * 428 * Avg Action is ==> 28.61621833226025\n",
            "Episode * 429 * Episodic Reward is ==> -579.5719478587235\n",
            "Episode * 429 * Avg Action is ==> 28.387768804284995\n",
            "Episode * 430 * Episodic Reward is ==> -580.4951189418852\n",
            "Episode * 430 * Avg Action is ==> 28.322135111214184\n",
            "Episode * 431 * Episodic Reward is ==> -581.0911818789059\n",
            "Episode * 431 * Avg Action is ==> 28.84176236229051\n",
            "Episode * 432 * Episodic Reward is ==> -581.6392669267558\n",
            "Episode * 432 * Avg Action is ==> 29.185036108130557\n",
            "Episode * 433 * Episodic Reward is ==> -582.2558495308564\n",
            "Episode * 433 * Avg Action is ==> 29.017353373861354\n",
            "Episode * 434 * Episodic Reward is ==> -580.3347834830785\n",
            "Episode * 434 * Avg Action is ==> 28.747649380116876\n",
            "Episode * 435 * Episodic Reward is ==> -580.1403413576318\n",
            "Episode * 435 * Avg Action is ==> 28.552553124824755\n",
            "Episode * 436 * Episodic Reward is ==> -580.5180285308126\n",
            "Episode * 436 * Avg Action is ==> 28.785522152020746\n",
            "Episode * 437 * Episodic Reward is ==> -576.8053591266882\n",
            "Episode * 437 * Avg Action is ==> 28.14644541258162\n",
            "Episode * 438 * Episodic Reward is ==> -584.0746938688351\n",
            "Episode * 438 * Avg Action is ==> 28.96584768808313\n",
            "Episode * 439 * Episodic Reward is ==> -578.6508615178219\n",
            "Episode * 439 * Avg Action is ==> 28.3952320286401\n",
            "Episode * 440 * Episodic Reward is ==> -578.2394865484748\n",
            "Episode * 440 * Avg Action is ==> 28.29302280715535\n",
            "Episode * 441 * Episodic Reward is ==> -575.9960639694093\n",
            "Episode * 441 * Avg Action is ==> 27.838184473424935\n",
            "Episode * 442 * Episodic Reward is ==> -580.2465945970238\n",
            "Episode * 442 * Avg Action is ==> 28.652656761827767\n",
            "Episode * 443 * Episodic Reward is ==> -579.7110435306469\n",
            "Episode * 443 * Avg Action is ==> 28.714951674397085\n",
            "Episode * 444 * Episodic Reward is ==> -579.3788445391385\n",
            "Episode * 444 * Avg Action is ==> 28.585486688638056\n",
            "Episode * 445 * Episodic Reward is ==> -579.7364643769845\n",
            "Episode * 445 * Avg Action is ==> 28.541344888513173\n",
            "Episode * 446 * Episodic Reward is ==> -579.6078661200958\n",
            "Episode * 446 * Avg Action is ==> 28.622310534689056\n",
            "Episode * 447 * Episodic Reward is ==> -581.129954696742\n",
            "Episode * 447 * Avg Action is ==> 28.9083559904064\n",
            "Episode * 448 * Episodic Reward is ==> -579.5374928664975\n",
            "Episode * 448 * Avg Action is ==> 28.274514094574222\n",
            "Episode * 449 * Episodic Reward is ==> -579.6258297674592\n",
            "Episode * 449 * Avg Action is ==> 28.653847400951015\n",
            "Episode * 450 * Episodic Reward is ==> -580.1585747533252\n",
            "Episode * 450 * Avg Action is ==> 28.469473478804456\n",
            "Episode * 451 * Episodic Reward is ==> -579.2457261051875\n",
            "Episode * 451 * Avg Action is ==> 28.470556232444153\n",
            "Episode * 452 * Episodic Reward is ==> -579.0125319018307\n",
            "Episode * 452 * Avg Action is ==> 28.30595367197136\n",
            "Episode * 453 * Episodic Reward is ==> -578.9577836838127\n",
            "Episode * 453 * Avg Action is ==> 28.385197477184775\n",
            "Episode * 454 * Episodic Reward is ==> -580.5319941465197\n",
            "Episode * 454 * Avg Action is ==> 28.71758207945247\n",
            "Episode * 455 * Episodic Reward is ==> -577.0700724782326\n",
            "Episode * 455 * Avg Action is ==> 27.967906440986333\n",
            "Episode * 456 * Episodic Reward is ==> -578.5926988089922\n",
            "Episode * 456 * Avg Action is ==> 28.291613733156385\n",
            "Episode * 457 * Episodic Reward is ==> -579.2628366101646\n",
            "Episode * 457 * Avg Action is ==> 28.55105314662738\n",
            "Episode * 458 * Episodic Reward is ==> -578.6584370484269\n",
            "Episode * 458 * Avg Action is ==> 28.32801077891026\n",
            "Episode * 459 * Episodic Reward is ==> -579.1406900133046\n",
            "Episode * 459 * Avg Action is ==> 28.347076190302566\n",
            "Episode * 460 * Episodic Reward is ==> -576.9878354442918\n",
            "Episode * 460 * Avg Action is ==> 27.9428970811527\n",
            "Episode * 461 * Episodic Reward is ==> -577.7102475698916\n",
            "Episode * 461 * Avg Action is ==> 28.050170007654465\n",
            "Episode * 462 * Episodic Reward is ==> -579.1451917845108\n",
            "Episode * 462 * Avg Action is ==> 28.37899599691288\n",
            "Episode * 463 * Episodic Reward is ==> -578.7738068891628\n",
            "Episode * 463 * Avg Action is ==> 28.47301858771577\n",
            "Episode * 464 * Episodic Reward is ==> -577.6467383506958\n",
            "Episode * 464 * Avg Action is ==> 28.211999809653065\n",
            "Episode * 465 * Episodic Reward is ==> -576.6461951637833\n",
            "Episode * 465 * Avg Action is ==> 27.773823460335556\n",
            "Episode * 466 * Episodic Reward is ==> -577.6237774468833\n",
            "Episode * 466 * Avg Action is ==> 28.00059665757498\n",
            "Episode * 467 * Episodic Reward is ==> -574.6305208983179\n",
            "Episode * 467 * Avg Action is ==> 27.606523739941366\n",
            "Episode * 468 * Episodic Reward is ==> -576.4299851735179\n",
            "Episode * 468 * Avg Action is ==> 27.752009478013015\n",
            "Episode * 469 * Episodic Reward is ==> -577.6284603346339\n",
            "Episode * 469 * Avg Action is ==> 27.959428408175384\n",
            "Episode * 470 * Episodic Reward is ==> -579.1372098370681\n",
            "Episode * 470 * Avg Action is ==> 28.47101024187685\n",
            "Episode * 471 * Episodic Reward is ==> -576.5387273163047\n",
            "Episode * 471 * Avg Action is ==> 27.78434094204454\n",
            "Episode * 472 * Episodic Reward is ==> -578.0590346119513\n",
            "Episode * 472 * Avg Action is ==> 28.115566090903602\n",
            "Episode * 473 * Episodic Reward is ==> -576.809430512702\n",
            "Episode * 473 * Avg Action is ==> 28.11031034261538\n",
            "Episode * 474 * Episodic Reward is ==> -576.3315150304238\n",
            "Episode * 474 * Avg Action is ==> 27.767251645403416\n",
            "Episode * 475 * Episodic Reward is ==> -578.8720703071655\n",
            "Episode * 475 * Avg Action is ==> 28.39579137652805\n",
            "Episode * 476 * Episodic Reward is ==> -578.7328227244591\n",
            "Episode * 476 * Avg Action is ==> 28.611050814369953\n",
            "Episode * 477 * Episodic Reward is ==> -579.6849281251751\n",
            "Episode * 477 * Avg Action is ==> 28.581321856807996\n",
            "Episode * 478 * Episodic Reward is ==> -576.9067939463288\n",
            "Episode * 478 * Avg Action is ==> 27.953237797349477\n",
            "Episode * 479 * Episodic Reward is ==> -576.9999285233611\n",
            "Episode * 479 * Avg Action is ==> 27.872024987212985\n",
            "Episode * 480 * Episodic Reward is ==> -578.9188579961847\n",
            "Episode * 480 * Avg Action is ==> 28.42375703512002\n",
            "Episode * 481 * Episodic Reward is ==> -577.3513403250734\n",
            "Episode * 481 * Avg Action is ==> 27.968731176089864\n",
            "Episode * 482 * Episodic Reward is ==> -579.3717486307163\n",
            "Episode * 482 * Avg Action is ==> 28.612521240973752\n",
            "Episode * 483 * Episodic Reward is ==> -579.2573315929409\n",
            "Episode * 483 * Avg Action is ==> 28.47383550183121\n",
            "Episode * 484 * Episodic Reward is ==> -577.7031972997781\n",
            "Episode * 484 * Avg Action is ==> 28.20744610381167\n",
            "Episode * 485 * Episodic Reward is ==> -577.4832127901927\n",
            "Episode * 485 * Avg Action is ==> 28.17663757868914\n",
            "Episode * 486 * Episodic Reward is ==> -576.1664251894174\n",
            "Episode * 486 * Avg Action is ==> 27.713831811535705\n",
            "Episode * 487 * Episodic Reward is ==> -577.7859088545947\n",
            "Episode * 487 * Avg Action is ==> 28.119941651646013\n",
            "Episode * 488 * Episodic Reward is ==> -577.1490553496205\n",
            "Episode * 488 * Avg Action is ==> 28.26577000823532\n",
            "Episode * 489 * Episodic Reward is ==> -579.3748739151773\n",
            "Episode * 489 * Avg Action is ==> 28.36205428612281\n",
            "Episode * 490 * Episodic Reward is ==> -579.046070941383\n",
            "Episode * 490 * Avg Action is ==> 28.435706316240182\n",
            "Episode * 491 * Episodic Reward is ==> -579.3425799641847\n",
            "Episode * 491 * Avg Action is ==> 28.52723362630163\n",
            "Episode * 492 * Episodic Reward is ==> -577.1959868303024\n",
            "Episode * 492 * Avg Action is ==> 28.246961976545517\n",
            "Episode * 493 * Episodic Reward is ==> -577.8959833955042\n",
            "Episode * 493 * Avg Action is ==> 28.157168157451807\n",
            "Episode * 494 * Episodic Reward is ==> -575.3534203037989\n",
            "Episode * 494 * Avg Action is ==> 27.63603112777378\n",
            "Episode * 495 * Episodic Reward is ==> -596.9130711886895\n",
            "Episode * 495 * Avg Action is ==> 30.544328866780276\n",
            "Episode * 496 * Episodic Reward is ==> -638.9905455609835\n",
            "Episode * 496 * Avg Action is ==> 42.52319179774325\n",
            "Episode * 497 * Episodic Reward is ==> -637.0909185645662\n",
            "Episode * 497 * Avg Action is ==> 42.70425517639694\n",
            "Episode * 498 * Episodic Reward is ==> -641.7265668103296\n",
            "Episode * 498 * Avg Action is ==> 43.740955671539794\n",
            "Episode * 499 * Episodic Reward is ==> -640.1016330827317\n",
            "Episode * 499 * Avg Action is ==> 44.34871817729595\n",
            "Episode * 500 * Episodic Reward is ==> -641.5200472266886\n",
            "Episode * 500 * Avg Action is ==> 44.516176911328074\n",
            "Episode * 501 * Episodic Reward is ==> -640.2729526171239\n",
            "Episode * 501 * Avg Action is ==> 44.75108270025786\n",
            "Episode * 502 * Episodic Reward is ==> -642.5658375656427\n",
            "Episode * 502 * Avg Action is ==> 45.0120203836561\n",
            "Episode * 503 * Episodic Reward is ==> -639.81673282021\n",
            "Episode * 503 * Avg Action is ==> 44.87269658653846\n",
            "Episode * 504 * Episodic Reward is ==> -639.8542351798758\n",
            "Episode * 504 * Avg Action is ==> 44.41391495819387\n",
            "Episode * 505 * Episodic Reward is ==> -639.8671118104295\n",
            "Episode * 505 * Avg Action is ==> 44.37165716675983\n",
            "Episode * 506 * Episodic Reward is ==> -642.3389382757439\n",
            "Episode * 506 * Avg Action is ==> 44.88691823109759\n",
            "Episode * 507 * Episodic Reward is ==> -665.7438144181207\n",
            "Episode * 507 * Avg Action is ==> 40.5558949328241\n",
            "Episode * 508 * Episodic Reward is ==> -582.0201846501573\n",
            "Episode * 508 * Avg Action is ==> 29.90237658362377\n",
            "Episode * 509 * Episodic Reward is ==> -589.3156616754702\n",
            "Episode * 509 * Avg Action is ==> 30.281356470386434\n",
            "Episode * 510 * Episodic Reward is ==> -581.3426452536091\n",
            "Episode * 510 * Avg Action is ==> 29.748065726768598\n",
            "Episode * 511 * Episodic Reward is ==> -579.8773474137763\n",
            "Episode * 511 * Avg Action is ==> 29.02681508675106\n",
            "Episode * 512 * Episodic Reward is ==> -579.1028352138185\n",
            "Episode * 512 * Avg Action is ==> 28.81010198242012\n",
            "Episode * 513 * Episodic Reward is ==> -578.1985663098409\n",
            "Episode * 513 * Avg Action is ==> 28.475133784353964\n",
            "Episode * 514 * Episodic Reward is ==> -579.043610325354\n",
            "Episode * 514 * Avg Action is ==> 28.496246231846957\n",
            "Episode * 515 * Episodic Reward is ==> -579.9641840941847\n",
            "Episode * 515 * Avg Action is ==> 28.381029752906635\n",
            "Episode * 516 * Episodic Reward is ==> -575.5664986356114\n",
            "Episode * 516 * Avg Action is ==> 27.72008201057208\n",
            "Episode * 517 * Episodic Reward is ==> -575.5743767083044\n",
            "Episode * 517 * Avg Action is ==> 27.432037443401846\n",
            "Episode * 518 * Episodic Reward is ==> -577.8443834401129\n",
            "Episode * 518 * Avg Action is ==> 27.671232053821466\n",
            "Episode * 519 * Episodic Reward is ==> -576.9789975616974\n",
            "Episode * 519 * Avg Action is ==> 27.943813038742345\n",
            "Episode * 520 * Episodic Reward is ==> -576.8165618457919\n",
            "Episode * 520 * Avg Action is ==> 27.85996688885668\n",
            "Episode * 521 * Episodic Reward is ==> -576.7801878620933\n",
            "Episode * 521 * Avg Action is ==> 27.58189487640638\n",
            "Episode * 522 * Episodic Reward is ==> -575.7354558903905\n",
            "Episode * 522 * Avg Action is ==> 27.362320107007328\n",
            "Episode * 523 * Episodic Reward is ==> -577.4176092570992\n",
            "Episode * 523 * Avg Action is ==> 27.68420225528206\n",
            "Episode * 524 * Episodic Reward is ==> -577.8518423991352\n",
            "Episode * 524 * Avg Action is ==> 28.060223235467948\n",
            "Episode * 525 * Episodic Reward is ==> -576.4969716457917\n",
            "Episode * 525 * Avg Action is ==> 27.279096217991697\n",
            "Episode * 526 * Episodic Reward is ==> -576.9447021837005\n",
            "Episode * 526 * Avg Action is ==> 27.64405217996688\n",
            "Episode * 527 * Episodic Reward is ==> -576.6654322347197\n",
            "Episode * 527 * Avg Action is ==> 27.697064376433868\n",
            "Episode * 528 * Episodic Reward is ==> -578.0182772305395\n",
            "Episode * 528 * Avg Action is ==> 27.983522081821658\n",
            "Episode * 529 * Episodic Reward is ==> -576.871054736349\n",
            "Episode * 529 * Avg Action is ==> 27.67895547460957\n",
            "Episode * 530 * Episodic Reward is ==> -576.4651347969058\n",
            "Episode * 530 * Avg Action is ==> 27.60460401640042\n",
            "Episode * 531 * Episodic Reward is ==> -575.2950158161452\n",
            "Episode * 531 * Avg Action is ==> 27.40261373018972\n",
            "Episode * 532 * Episodic Reward is ==> -574.9541645144798\n",
            "Episode * 532 * Avg Action is ==> 27.139698601647012\n",
            "Episode * 533 * Episodic Reward is ==> -574.8398147988268\n",
            "Episode * 533 * Avg Action is ==> 27.45675851715593\n",
            "Episode * 534 * Episodic Reward is ==> -576.1680519163308\n",
            "Episode * 534 * Avg Action is ==> 27.427921260668153\n",
            "Episode * 535 * Episodic Reward is ==> -574.2261794758177\n",
            "Episode * 535 * Avg Action is ==> 27.353249215204603\n",
            "Episode * 536 * Episodic Reward is ==> -577.2136985215409\n",
            "Episode * 536 * Avg Action is ==> 27.891816456952977\n",
            "Episode * 537 * Episodic Reward is ==> -576.9700451466761\n",
            "Episode * 537 * Avg Action is ==> 28.02284599814255\n",
            "Episode * 538 * Episodic Reward is ==> -577.0559465166485\n",
            "Episode * 538 * Avg Action is ==> 27.941510958900693\n",
            "Episode * 539 * Episodic Reward is ==> -577.3884900337792\n",
            "Episode * 539 * Avg Action is ==> 28.21505776986389\n",
            "Episode * 540 * Episodic Reward is ==> -577.3881016220909\n",
            "Episode * 540 * Avg Action is ==> 28.071926894407902\n",
            "Episode * 541 * Episodic Reward is ==> -576.2352017209424\n",
            "Episode * 541 * Avg Action is ==> 27.831836215494043\n",
            "Episode * 542 * Episodic Reward is ==> -577.1269829092086\n",
            "Episode * 542 * Avg Action is ==> 27.84298918385444\n",
            "Episode * 543 * Episodic Reward is ==> -576.4780633612033\n",
            "Episode * 543 * Avg Action is ==> 27.729972265384887\n",
            "Episode * 544 * Episodic Reward is ==> -576.1070985860157\n",
            "Episode * 544 * Avg Action is ==> 27.92919122999713\n",
            "Episode * 545 * Episodic Reward is ==> -576.7397400695745\n",
            "Episode * 545 * Avg Action is ==> 28.1098227772822\n",
            "Episode * 546 * Episodic Reward is ==> -576.2334313620953\n",
            "Episode * 546 * Avg Action is ==> 27.778508823426016\n",
            "Episode * 547 * Episodic Reward is ==> -577.2994827780093\n",
            "Episode * 547 * Avg Action is ==> 28.0822981052238\n",
            "Episode * 548 * Episodic Reward is ==> -577.3620570196395\n",
            "Episode * 548 * Avg Action is ==> 28.010839199582414\n",
            "Episode * 549 * Episodic Reward is ==> -576.7787747771296\n",
            "Episode * 549 * Avg Action is ==> 28.009163893459668\n",
            "Episode * 550 * Episodic Reward is ==> -578.0640052717367\n",
            "Episode * 550 * Avg Action is ==> 27.98404471743998\n",
            "Episode * 551 * Episodic Reward is ==> -577.2462386146791\n",
            "Episode * 551 * Avg Action is ==> 28.005230317228445\n",
            "Episode * 552 * Episodic Reward is ==> -578.3630687509756\n",
            "Episode * 552 * Avg Action is ==> 28.239428368446994\n",
            "Episode * 553 * Episodic Reward is ==> -575.8856615194929\n",
            "Episode * 553 * Avg Action is ==> 27.921007414209075\n",
            "Episode * 554 * Episodic Reward is ==> -576.3989138181574\n",
            "Episode * 554 * Avg Action is ==> 28.2013969421545\n",
            "Episode * 555 * Episodic Reward is ==> -578.3815747547102\n",
            "Episode * 555 * Avg Action is ==> 28.210943493442066\n",
            "Episode * 556 * Episodic Reward is ==> -577.2404211762413\n",
            "Episode * 556 * Avg Action is ==> 28.276822549485473\n",
            "Episode * 557 * Episodic Reward is ==> -575.1188170567242\n",
            "Episode * 557 * Avg Action is ==> 27.496320566436786\n",
            "Episode * 558 * Episodic Reward is ==> -575.7624270266292\n",
            "Episode * 558 * Avg Action is ==> 27.582880418585898\n",
            "Episode * 559 * Episodic Reward is ==> -576.4650007889917\n",
            "Episode * 559 * Avg Action is ==> 27.593274522124382\n",
            "Episode * 560 * Episodic Reward is ==> -576.5359511230332\n",
            "Episode * 560 * Avg Action is ==> 27.575794551208833\n",
            "Episode * 561 * Episodic Reward is ==> -576.7376700269801\n",
            "Episode * 561 * Avg Action is ==> 28.00815489698501\n",
            "Episode * 562 * Episodic Reward is ==> -576.958252316063\n",
            "Episode * 562 * Avg Action is ==> 27.918681992278355\n",
            "Episode * 563 * Episodic Reward is ==> -576.3625751164178\n",
            "Episode * 563 * Avg Action is ==> 28.011608077013925\n",
            "Episode * 564 * Episodic Reward is ==> -579.357906097422\n",
            "Episode * 564 * Avg Action is ==> 28.195626669619326\n",
            "Episode * 565 * Episodic Reward is ==> -578.2609979642872\n",
            "Episode * 565 * Avg Action is ==> 28.211314209991357\n",
            "Episode * 566 * Episodic Reward is ==> -577.616257216175\n",
            "Episode * 566 * Avg Action is ==> 28.176882692929542\n",
            "Episode * 567 * Episodic Reward is ==> -578.0777979681703\n",
            "Episode * 567 * Avg Action is ==> 28.466937855339875\n",
            "Episode * 568 * Episodic Reward is ==> -577.4725586796657\n",
            "Episode * 568 * Avg Action is ==> 28.37667302595746\n",
            "Episode * 569 * Episodic Reward is ==> -575.8896855626053\n",
            "Episode * 569 * Avg Action is ==> 27.932346466938963\n",
            "Episode * 570 * Episodic Reward is ==> -575.3757702714524\n",
            "Episode * 570 * Avg Action is ==> 27.72691422849864\n",
            "Episode * 571 * Episodic Reward is ==> -576.3079354324258\n",
            "Episode * 571 * Avg Action is ==> 27.833889433478607\n",
            "Episode * 572 * Episodic Reward is ==> -574.2711722143481\n",
            "Episode * 572 * Avg Action is ==> 27.36108632251491\n",
            "Episode * 573 * Episodic Reward is ==> -574.0113637522613\n",
            "Episode * 573 * Avg Action is ==> 27.318040742016656\n",
            "Episode * 574 * Episodic Reward is ==> -575.4708470322029\n",
            "Episode * 574 * Avg Action is ==> 27.551175891113157\n",
            "Episode * 575 * Episodic Reward is ==> -576.0792507634841\n",
            "Episode * 575 * Avg Action is ==> 27.830820004722252\n",
            "Episode * 576 * Episodic Reward is ==> -577.5744818545041\n",
            "Episode * 576 * Avg Action is ==> 26.965125701698614\n",
            "Episode * 577 * Episodic Reward is ==> -566.0019818188298\n",
            "Episode * 577 * Avg Action is ==> 25.08698326763645\n",
            "Episode * 578 * Episodic Reward is ==> -577.4855472477889\n",
            "Episode * 578 * Avg Action is ==> 28.000762695955263\n",
            "Episode * 579 * Episodic Reward is ==> -574.5358574509298\n",
            "Episode * 579 * Avg Action is ==> 27.471506053184136\n",
            "Episode * 580 * Episodic Reward is ==> -575.6393092988891\n",
            "Episode * 580 * Avg Action is ==> 27.681793660705452\n",
            "Episode * 581 * Episodic Reward is ==> -573.4769519161177\n",
            "Episode * 581 * Avg Action is ==> 27.218768371401524\n",
            "Episode * 582 * Episodic Reward is ==> -575.9555208670744\n",
            "Episode * 582 * Avg Action is ==> 27.5231632764976\n",
            "Episode * 583 * Episodic Reward is ==> -575.9254965780844\n",
            "Episode * 583 * Avg Action is ==> 27.658686039879843\n",
            "Episode * 584 * Episodic Reward is ==> -576.6431335704665\n",
            "Episode * 584 * Avg Action is ==> 27.920459581556557\n",
            "Episode * 585 * Episodic Reward is ==> -577.0730263850731\n",
            "Episode * 585 * Avg Action is ==> 27.995726248953247\n",
            "Episode * 586 * Episodic Reward is ==> -576.1583799090736\n",
            "Episode * 586 * Avg Action is ==> 27.885594768019065\n",
            "Episode * 587 * Episodic Reward is ==> -575.1246883945147\n",
            "Episode * 587 * Avg Action is ==> 27.71905911136689\n",
            "Episode * 588 * Episodic Reward is ==> -573.9963770492999\n",
            "Episode * 588 * Avg Action is ==> 27.28900009747589\n",
            "Episode * 589 * Episodic Reward is ==> -574.4586073578281\n",
            "Episode * 589 * Avg Action is ==> 27.271263943100458\n",
            "Episode * 590 * Episodic Reward is ==> -575.3991652986274\n",
            "Episode * 590 * Avg Action is ==> 27.39401909506431\n",
            "Episode * 591 * Episodic Reward is ==> -576.2749911623677\n",
            "Episode * 591 * Avg Action is ==> 27.944027050791934\n",
            "Episode * 592 * Episodic Reward is ==> -576.3727383994749\n",
            "Episode * 592 * Avg Action is ==> 27.637969381772262\n",
            "Episode * 593 * Episodic Reward is ==> -576.212529969174\n",
            "Episode * 593 * Avg Action is ==> 27.75602882209466\n",
            "Episode * 594 * Episodic Reward is ==> -574.5684850179075\n",
            "Episode * 594 * Avg Action is ==> 27.48219732838708\n",
            "Episode * 595 * Episodic Reward is ==> -572.9402179902349\n",
            "Episode * 595 * Avg Action is ==> 26.922983211706683\n",
            "Episode * 596 * Episodic Reward is ==> -574.4830350884206\n",
            "Episode * 596 * Avg Action is ==> 26.96593972553653\n",
            "Episode * 597 * Episodic Reward is ==> -576.3633039656037\n",
            "Episode * 597 * Avg Action is ==> 27.43267773714978\n",
            "Episode * 598 * Episodic Reward is ==> -574.6037784763712\n",
            "Episode * 598 * Avg Action is ==> 27.51147156890835\n",
            "Episode * 599 * Episodic Reward is ==> -574.4990978511177\n",
            "Episode * 599 * Avg Action is ==> 27.30730329750316\n",
            "Episode * 600 * Episodic Reward is ==> -574.0760384379528\n",
            "Episode * 600 * Avg Action is ==> 27.075821916862587\n",
            "Episode * 601 * Episodic Reward is ==> -573.9655087973644\n",
            "Episode * 601 * Avg Action is ==> 27.334972237915142\n",
            "Episode * 602 * Episodic Reward is ==> -574.7888150194449\n",
            "Episode * 602 * Avg Action is ==> 27.31605212105211\n",
            "Episode * 603 * Episodic Reward is ==> -574.7440077289214\n",
            "Episode * 603 * Avg Action is ==> 27.432313347938685\n",
            "Episode * 604 * Episodic Reward is ==> -574.8343215335187\n",
            "Episode * 604 * Avg Action is ==> 27.284110761172222\n",
            "Episode * 605 * Episodic Reward is ==> -575.9716935624708\n",
            "Episode * 605 * Avg Action is ==> 27.565991101839504\n",
            "Episode * 606 * Episodic Reward is ==> -575.4091403904107\n",
            "Episode * 606 * Avg Action is ==> 27.589279857314292\n",
            "Episode * 607 * Episodic Reward is ==> -574.1767155521004\n",
            "Episode * 607 * Avg Action is ==> 27.37359051156143\n",
            "Episode * 608 * Episodic Reward is ==> -574.6100652078484\n",
            "Episode * 608 * Avg Action is ==> 27.244659370974667\n",
            "Episode * 609 * Episodic Reward is ==> -575.9754214720804\n",
            "Episode * 609 * Avg Action is ==> 27.68913393579396\n",
            "Episode * 610 * Episodic Reward is ==> -574.6640393345626\n",
            "Episode * 610 * Avg Action is ==> 27.507866680900076\n",
            "Episode * 611 * Episodic Reward is ==> -574.409121384086\n",
            "Episode * 611 * Avg Action is ==> 27.25738734265961\n",
            "Episode * 612 * Episodic Reward is ==> -573.8144638719276\n",
            "Episode * 612 * Avg Action is ==> 27.219451585584018\n",
            "Episode * 613 * Episodic Reward is ==> -574.412871510098\n",
            "Episode * 613 * Avg Action is ==> 27.148499219121035\n",
            "Episode * 614 * Episodic Reward is ==> -573.5156305199428\n",
            "Episode * 614 * Avg Action is ==> 26.970919275734808\n",
            "Episode * 615 * Episodic Reward is ==> -574.1995336300564\n",
            "Episode * 615 * Avg Action is ==> 27.18493537814417\n",
            "Episode * 616 * Episodic Reward is ==> -575.4850398463576\n",
            "Episode * 616 * Avg Action is ==> 27.576069176124125\n",
            "Episode * 617 * Episodic Reward is ==> -574.9818840421824\n",
            "Episode * 617 * Avg Action is ==> 27.52764937062845\n",
            "Episode * 618 * Episodic Reward is ==> -574.4660814930982\n",
            "Episode * 618 * Avg Action is ==> 27.28827211478113\n",
            "Episode * 619 * Episodic Reward is ==> -574.9257084249301\n",
            "Episode * 619 * Avg Action is ==> 27.325154899742646\n",
            "Episode * 620 * Episodic Reward is ==> -575.4877931115416\n",
            "Episode * 620 * Avg Action is ==> 27.527548816916273\n",
            "Episode * 621 * Episodic Reward is ==> -574.6630658710004\n",
            "Episode * 621 * Avg Action is ==> 27.390676934747553\n",
            "Episode * 622 * Episodic Reward is ==> -574.1566462212576\n",
            "Episode * 622 * Avg Action is ==> 27.190868680148427\n",
            "Episode * 623 * Episodic Reward is ==> -574.8116068871677\n",
            "Episode * 623 * Avg Action is ==> 27.33042598174808\n",
            "Episode * 624 * Episodic Reward is ==> -574.0658661950281\n",
            "Episode * 624 * Avg Action is ==> 27.19397295224897\n",
            "Episode * 625 * Episodic Reward is ==> -574.5956287384024\n",
            "Episode * 625 * Avg Action is ==> 26.9384261784932\n",
            "Episode * 626 * Episodic Reward is ==> -574.408125848371\n",
            "Episode * 626 * Avg Action is ==> 27.233573571918807\n",
            "Episode * 627 * Episodic Reward is ==> -576.227112915204\n",
            "Episode * 627 * Avg Action is ==> 27.624241132731612\n",
            "Episode * 628 * Episodic Reward is ==> -577.1052331771841\n",
            "Episode * 628 * Avg Action is ==> 27.81869885418497\n",
            "Episode * 629 * Episodic Reward is ==> -576.2632184311188\n",
            "Episode * 629 * Avg Action is ==> 27.72613576685332\n",
            "Episode * 630 * Episodic Reward is ==> -574.8794100730696\n",
            "Episode * 630 * Avg Action is ==> 27.319898147493742\n",
            "Episode * 631 * Episodic Reward is ==> -574.5202537680219\n",
            "Episode * 631 * Avg Action is ==> 27.33483960621035\n",
            "Episode * 632 * Episodic Reward is ==> -574.3165289569084\n",
            "Episode * 632 * Avg Action is ==> 27.199283427685018\n",
            "Episode * 633 * Episodic Reward is ==> -574.5468554806423\n",
            "Episode * 633 * Avg Action is ==> 27.182200303314247\n",
            "Episode * 634 * Episodic Reward is ==> -575.0392790521191\n",
            "Episode * 634 * Avg Action is ==> 27.34196517405225\n",
            "Episode * 635 * Episodic Reward is ==> -573.1766266103441\n",
            "Episode * 635 * Avg Action is ==> 27.212077357335055\n",
            "Episode * 636 * Episodic Reward is ==> -574.0541327935847\n",
            "Episode * 636 * Avg Action is ==> 27.01534632517573\n",
            "Episode * 637 * Episodic Reward is ==> -573.5895750828766\n",
            "Episode * 637 * Avg Action is ==> 27.081403154497018\n",
            "Episode * 638 * Episodic Reward is ==> -573.2853277452188\n",
            "Episode * 638 * Avg Action is ==> 26.928083208212442\n",
            "Episode * 639 * Episodic Reward is ==> -574.5134320959071\n",
            "Episode * 639 * Avg Action is ==> 27.15456668178098\n",
            "Episode * 640 * Episodic Reward is ==> -576.1606326167638\n",
            "Episode * 640 * Avg Action is ==> 27.555913698336013\n",
            "Episode * 641 * Episodic Reward is ==> -574.263833722667\n",
            "Episode * 641 * Avg Action is ==> 27.2940923074663\n",
            "Episode * 642 * Episodic Reward is ==> -574.2503022958307\n",
            "Episode * 642 * Avg Action is ==> 27.29387437935438\n",
            "Episode * 643 * Episodic Reward is ==> -573.9620404039122\n",
            "Episode * 643 * Avg Action is ==> 27.158018720120204\n",
            "Episode * 644 * Episodic Reward is ==> -574.3442323576761\n",
            "Episode * 644 * Avg Action is ==> 27.199520961519298\n",
            "Episode * 645 * Episodic Reward is ==> -575.5207098436969\n",
            "Episode * 645 * Avg Action is ==> 27.510906307257425\n",
            "Episode * 646 * Episodic Reward is ==> -574.1526665620834\n",
            "Episode * 646 * Avg Action is ==> 27.225058597662727\n",
            "Episode * 647 * Episodic Reward is ==> -574.7020981014398\n",
            "Episode * 647 * Avg Action is ==> 27.315976049297785\n",
            "Episode * 648 * Episodic Reward is ==> -574.8631677188271\n",
            "Episode * 648 * Avg Action is ==> 27.37359755423333\n",
            "Episode * 649 * Episodic Reward is ==> -574.7640999354192\n",
            "Episode * 649 * Avg Action is ==> 27.38987846546497\n",
            "Episode * 650 * Episodic Reward is ==> -573.9213092602839\n",
            "Episode * 650 * Avg Action is ==> 27.148276097202796\n",
            "Episode * 651 * Episodic Reward is ==> -573.695541830584\n",
            "Episode * 651 * Avg Action is ==> 27.225918035088508\n",
            "Episode * 652 * Episodic Reward is ==> -572.736743792069\n",
            "Episode * 652 * Avg Action is ==> 26.906059116588917\n",
            "Episode * 653 * Episodic Reward is ==> -571.8799542331607\n",
            "Episode * 653 * Avg Action is ==> 26.757246493972012\n",
            "Episode * 654 * Episodic Reward is ==> -573.8183873824663\n",
            "Episode * 654 * Avg Action is ==> 27.121769590945494\n",
            "Episode * 655 * Episodic Reward is ==> -573.5666632055006\n",
            "Episode * 655 * Avg Action is ==> 27.28768240656216\n",
            "Episode * 656 * Episodic Reward is ==> -572.6371414298716\n",
            "Episode * 656 * Avg Action is ==> 26.928866922860383\n",
            "Episode * 657 * Episodic Reward is ==> -571.9812515910934\n",
            "Episode * 657 * Avg Action is ==> 26.962131204875206\n",
            "Episode * 658 * Episodic Reward is ==> -572.6543204408288\n",
            "Episode * 658 * Avg Action is ==> 26.85264887977037\n",
            "Episode * 659 * Episodic Reward is ==> -572.7557892280948\n",
            "Episode * 659 * Avg Action is ==> 27.022296333001314\n",
            "Episode * 660 * Episodic Reward is ==> -571.7510695080969\n",
            "Episode * 660 * Avg Action is ==> 26.750102669597194\n",
            "Episode * 661 * Episodic Reward is ==> -571.2156654538105\n",
            "Episode * 661 * Avg Action is ==> 26.524016821435065\n",
            "Episode * 662 * Episodic Reward is ==> -572.1922964310259\n",
            "Episode * 662 * Avg Action is ==> 26.68691238354071\n",
            "Episode * 663 * Episodic Reward is ==> -571.2552192519868\n",
            "Episode * 663 * Avg Action is ==> 26.797756869201624\n",
            "Episode * 664 * Episodic Reward is ==> -571.498697411246\n",
            "Episode * 664 * Avg Action is ==> 26.417486075873907\n",
            "Episode * 665 * Episodic Reward is ==> -572.328211357827\n",
            "Episode * 665 * Avg Action is ==> 26.58204814664894\n",
            "Episode * 666 * Episodic Reward is ==> -571.3753744468406\n",
            "Episode * 666 * Avg Action is ==> 26.6969248295333\n",
            "Episode * 667 * Episodic Reward is ==> -570.92986018244\n",
            "Episode * 667 * Avg Action is ==> 26.267737164675953\n",
            "Episode * 668 * Episodic Reward is ==> -573.191725099244\n",
            "Episode * 668 * Avg Action is ==> 26.931409759022277\n",
            "Episode * 669 * Episodic Reward is ==> -571.9057445888303\n",
            "Episode * 669 * Avg Action is ==> 26.787868506125754\n",
            "Episode * 670 * Episodic Reward is ==> -572.3448637665521\n",
            "Episode * 670 * Avg Action is ==> 27.005930673002666\n",
            "Episode * 671 * Episodic Reward is ==> -573.4624075652071\n",
            "Episode * 671 * Avg Action is ==> 27.04810835670183\n",
            "Episode * 672 * Episodic Reward is ==> -574.1961305300022\n",
            "Episode * 672 * Avg Action is ==> 27.200233036813653\n",
            "Episode * 673 * Episodic Reward is ==> -574.1973023576148\n",
            "Episode * 673 * Avg Action is ==> 27.21168526920487\n",
            "Episode * 674 * Episodic Reward is ==> -574.6868871796452\n",
            "Episode * 674 * Avg Action is ==> 27.380609161140796\n",
            "Episode * 675 * Episodic Reward is ==> -575.5504497422672\n",
            "Episode * 675 * Avg Action is ==> 27.52252647953372\n",
            "Episode * 676 * Episodic Reward is ==> -575.9299206917154\n",
            "Episode * 676 * Avg Action is ==> 27.63554276448763\n",
            "Episode * 677 * Episodic Reward is ==> -574.5807544390436\n",
            "Episode * 677 * Avg Action is ==> 27.49057319937386\n",
            "Episode * 678 * Episodic Reward is ==> -573.7835097591984\n",
            "Episode * 678 * Avg Action is ==> 27.15935980147085\n",
            "Episode * 679 * Episodic Reward is ==> -573.072058175215\n",
            "Episode * 679 * Avg Action is ==> 26.994119036957727\n",
            "Episode * 680 * Episodic Reward is ==> -572.3967865223506\n",
            "Episode * 680 * Avg Action is ==> 27.08817229503225\n",
            "Episode * 681 * Episodic Reward is ==> -572.4900397188262\n",
            "Episode * 681 * Avg Action is ==> 26.646108667679687\n",
            "Episode * 682 * Episodic Reward is ==> -573.6905713698428\n",
            "Episode * 682 * Avg Action is ==> 27.366938634533938\n",
            "Episode * 683 * Episodic Reward is ==> -572.5665564495392\n",
            "Episode * 683 * Avg Action is ==> 27.150988989233714\n",
            "Episode * 684 * Episodic Reward is ==> -571.78199654247\n",
            "Episode * 684 * Avg Action is ==> 26.807714896467765\n",
            "Episode * 685 * Episodic Reward is ==> -571.4308175824865\n",
            "Episode * 685 * Avg Action is ==> 26.554722689341034\n",
            "Episode * 686 * Episodic Reward is ==> -568.4594655986652\n",
            "Episode * 686 * Avg Action is ==> 25.953064287661793\n",
            "Episode * 687 * Episodic Reward is ==> -570.2629799029257\n",
            "Episode * 687 * Avg Action is ==> 26.587603358024996\n",
            "Episode * 688 * Episodic Reward is ==> -571.9700030476341\n",
            "Episode * 688 * Avg Action is ==> 26.651513476415214\n",
            "Episode * 689 * Episodic Reward is ==> -571.4286955183211\n",
            "Episode * 689 * Avg Action is ==> 26.702799036122915\n",
            "Episode * 690 * Episodic Reward is ==> -573.8563941793027\n",
            "Episode * 690 * Avg Action is ==> 26.955337175449962\n",
            "Episode * 691 * Episodic Reward is ==> -571.7902908260809\n",
            "Episode * 691 * Avg Action is ==> 26.871652571192293\n",
            "Episode * 692 * Episodic Reward is ==> -572.2056485958789\n",
            "Episode * 692 * Avg Action is ==> 27.042420862145566\n",
            "Episode * 693 * Episodic Reward is ==> -572.5869498113594\n",
            "Episode * 693 * Avg Action is ==> 27.03256381647524\n",
            "Episode * 694 * Episodic Reward is ==> -572.5323075822616\n",
            "Episode * 694 * Avg Action is ==> 26.8905032152125\n",
            "Episode * 695 * Episodic Reward is ==> -573.7038524762979\n",
            "Episode * 695 * Avg Action is ==> 27.37577289763597\n",
            "Episode * 696 * Episodic Reward is ==> -571.8975499252615\n",
            "Episode * 696 * Avg Action is ==> 26.930905681813773\n",
            "Episode * 697 * Episodic Reward is ==> -574.2204841070967\n",
            "Episode * 697 * Avg Action is ==> 27.178294791158045\n",
            "Episode * 698 * Episodic Reward is ==> -574.2306021822641\n",
            "Episode * 698 * Avg Action is ==> 27.24078594152923\n",
            "Episode * 699 * Episodic Reward is ==> -568.2732323114012\n",
            "Episode * 699 * Avg Action is ==> 26.21010466865178\n",
            "Episode * 700 * Episodic Reward is ==> -571.21628112586\n",
            "Episode * 700 * Avg Action is ==> 26.50164676872099\n",
            "Episode * 701 * Episodic Reward is ==> -575.4829468493361\n",
            "Episode * 701 * Avg Action is ==> 27.403574353989764\n",
            "Episode * 702 * Episodic Reward is ==> -576.4303626424419\n",
            "Episode * 702 * Avg Action is ==> 28.0027469069586\n",
            "Episode * 703 * Episodic Reward is ==> -572.9945148988875\n",
            "Episode * 703 * Avg Action is ==> 27.2506568491523\n",
            "Episode * 704 * Episodic Reward is ==> -574.6067542604437\n",
            "Episode * 704 * Avg Action is ==> 27.5745097983498\n",
            "Episode * 705 * Episodic Reward is ==> -575.3297792947333\n",
            "Episode * 705 * Avg Action is ==> 27.625121270933647\n",
            "Episode * 706 * Episodic Reward is ==> -575.8489267843544\n",
            "Episode * 706 * Avg Action is ==> 27.893396427491048\n",
            "Episode * 707 * Episodic Reward is ==> -568.9605466073078\n",
            "Episode * 707 * Avg Action is ==> 26.36973545091377\n",
            "Episode * 708 * Episodic Reward is ==> -573.3962750548058\n",
            "Episode * 708 * Avg Action is ==> 27.340617882750227\n",
            "Episode * 709 * Episodic Reward is ==> -570.318906573892\n",
            "Episode * 709 * Avg Action is ==> 26.668720158109963\n",
            "Episode * 710 * Episodic Reward is ==> -567.8211658916252\n",
            "Episode * 710 * Avg Action is ==> 26.013881360989135\n",
            "Episode * 711 * Episodic Reward is ==> -571.4699043702968\n",
            "Episode * 711 * Avg Action is ==> 26.440019877525344\n",
            "Episode * 712 * Episodic Reward is ==> -572.1783002358836\n",
            "Episode * 712 * Avg Action is ==> 26.844606731277125\n",
            "Episode * 713 * Episodic Reward is ==> -572.6685761686272\n",
            "Episode * 713 * Avg Action is ==> 26.611082437561155\n",
            "Episode * 714 * Episodic Reward is ==> -572.7685652041484\n",
            "Episode * 714 * Avg Action is ==> 26.898277842318482\n",
            "Episode * 715 * Episodic Reward is ==> -568.1586164182166\n",
            "Episode * 715 * Avg Action is ==> 26.201879221185656\n",
            "Episode * 716 * Episodic Reward is ==> -572.3895477026626\n",
            "Episode * 716 * Avg Action is ==> 26.96558273168047\n",
            "Episode * 717 * Episodic Reward is ==> -571.9375994060009\n",
            "Episode * 717 * Avg Action is ==> 26.678370612000812\n",
            "Episode * 718 * Episodic Reward is ==> -571.5878974668192\n",
            "Episode * 718 * Avg Action is ==> 26.697959476935495\n",
            "Episode * 719 * Episodic Reward is ==> -570.9368644316902\n",
            "Episode * 719 * Avg Action is ==> 26.66288896221249\n",
            "Episode * 720 * Episodic Reward is ==> -571.3732596238889\n",
            "Episode * 720 * Avg Action is ==> 26.597775034144874\n",
            "Episode * 721 * Episodic Reward is ==> -568.3697241176482\n",
            "Episode * 721 * Avg Action is ==> 26.145470683842873\n",
            "Episode * 722 * Episodic Reward is ==> -572.9307597983645\n",
            "Episode * 722 * Avg Action is ==> 27.19763705535672\n",
            "Episode * 723 * Episodic Reward is ==> -573.0426879524877\n",
            "Episode * 723 * Avg Action is ==> 27.04615415602582\n",
            "Episode * 724 * Episodic Reward is ==> -568.8674227921434\n",
            "Episode * 724 * Avg Action is ==> 26.371113807455608\n",
            "Episode * 725 * Episodic Reward is ==> -573.1263114352851\n",
            "Episode * 725 * Avg Action is ==> 26.990659460204178\n",
            "Episode * 726 * Episodic Reward is ==> -575.0513218131664\n",
            "Episode * 726 * Avg Action is ==> 27.453183516500097\n",
            "Episode * 727 * Episodic Reward is ==> -574.6657001470822\n",
            "Episode * 727 * Avg Action is ==> 27.575778449940387\n",
            "Episode * 728 * Episodic Reward is ==> -573.8313513657591\n",
            "Episode * 728 * Avg Action is ==> 27.291969775355067\n",
            "Episode * 729 * Episodic Reward is ==> -572.113463249977\n",
            "Episode * 729 * Avg Action is ==> 26.967218603225184\n",
            "Episode * 730 * Episodic Reward is ==> -571.5920709747529\n",
            "Episode * 730 * Avg Action is ==> 26.954214471607962\n",
            "Episode * 731 * Episodic Reward is ==> -570.3457245335773\n",
            "Episode * 731 * Avg Action is ==> 26.298053723875604\n",
            "Episode * 732 * Episodic Reward is ==> -569.5604823697332\n",
            "Episode * 732 * Avg Action is ==> 26.26102135832729\n",
            "Episode * 733 * Episodic Reward is ==> -570.7680380448639\n",
            "Episode * 733 * Avg Action is ==> 26.229857372656088\n",
            "Episode * 734 * Episodic Reward is ==> -567.4728204249258\n",
            "Episode * 734 * Avg Action is ==> 25.564884633263855\n",
            "Episode * 735 * Episodic Reward is ==> -573.298159269148\n",
            "Episode * 735 * Avg Action is ==> 27.190150302119147\n",
            "Episode * 736 * Episodic Reward is ==> -575.2221711808828\n",
            "Episode * 736 * Avg Action is ==> 27.707265671714712\n",
            "Episode * 737 * Episodic Reward is ==> -571.9113327593027\n",
            "Episode * 737 * Avg Action is ==> 27.068996082509113\n",
            "Episode * 738 * Episodic Reward is ==> -572.6222440740509\n",
            "Episode * 738 * Avg Action is ==> 26.776186026319674\n",
            "Episode * 739 * Episodic Reward is ==> -572.3504153668133\n",
            "Episode * 739 * Avg Action is ==> 26.78864882752822\n",
            "Episode * 740 * Episodic Reward is ==> -570.6931024236247\n",
            "Episode * 740 * Avg Action is ==> 26.63393546709587\n",
            "Episode * 741 * Episodic Reward is ==> -569.3820257789323\n",
            "Episode * 741 * Avg Action is ==> 25.999753633513404\n",
            "Episode * 742 * Episodic Reward is ==> -568.3810008223604\n",
            "Episode * 742 * Avg Action is ==> 25.59543929998564\n",
            "Episode * 743 * Episodic Reward is ==> -571.8952800952147\n",
            "Episode * 743 * Avg Action is ==> 26.577273101820154\n",
            "Episode * 744 * Episodic Reward is ==> -565.7178542165309\n",
            "Episode * 744 * Avg Action is ==> 25.584504026638943\n",
            "Episode * 745 * Episodic Reward is ==> -571.2142097738903\n",
            "Episode * 745 * Avg Action is ==> 26.353740327191442\n",
            "Episode * 746 * Episodic Reward is ==> -572.4798057659307\n",
            "Episode * 746 * Avg Action is ==> 26.78386336355563\n",
            "Episode * 747 * Episodic Reward is ==> -572.858767288512\n",
            "Episode * 747 * Avg Action is ==> 26.932925789086717\n",
            "Episode * 748 * Episodic Reward is ==> -570.5167890003185\n",
            "Episode * 748 * Avg Action is ==> 26.45725176586008\n",
            "Episode * 749 * Episodic Reward is ==> -566.2879394903017\n",
            "Episode * 749 * Avg Action is ==> 25.47564245375705\n",
            "Episode * 750 * Episodic Reward is ==> -569.5302782887152\n",
            "Episode * 750 * Avg Action is ==> 26.090887578973202\n",
            "Episode * 751 * Episodic Reward is ==> -573.164235383764\n",
            "Episode * 751 * Avg Action is ==> 27.252525897769196\n",
            "Episode * 752 * Episodic Reward is ==> -570.2018943840291\n",
            "Episode * 752 * Avg Action is ==> 26.587859955978463\n",
            "Episode * 753 * Episodic Reward is ==> -567.7093948993878\n",
            "Episode * 753 * Avg Action is ==> 26.053695712512685\n",
            "Episode * 754 * Episodic Reward is ==> -568.3049777126818\n",
            "Episode * 754 * Avg Action is ==> 25.95129104665877\n",
            "Episode * 755 * Episodic Reward is ==> -569.9903637611395\n",
            "Episode * 755 * Avg Action is ==> 26.515279133584976\n",
            "Episode * 756 * Episodic Reward is ==> -569.1637346427507\n",
            "Episode * 756 * Avg Action is ==> 26.34136948021748\n",
            "Episode * 757 * Episodic Reward is ==> -570.8100162567761\n",
            "Episode * 757 * Avg Action is ==> 26.674004218095913\n",
            "Episode * 758 * Episodic Reward is ==> -569.0947245804869\n",
            "Episode * 758 * Avg Action is ==> 26.325034842949375\n",
            "Episode * 759 * Episodic Reward is ==> -569.8601012984138\n",
            "Episode * 759 * Avg Action is ==> 26.539029144291465\n",
            "Episode * 760 * Episodic Reward is ==> -569.8171922460555\n",
            "Episode * 760 * Avg Action is ==> 26.47099704882556\n",
            "Episode * 761 * Episodic Reward is ==> -568.7894885792906\n",
            "Episode * 761 * Avg Action is ==> 26.06110681648814\n",
            "Episode * 762 * Episodic Reward is ==> -569.7085378792909\n",
            "Episode * 762 * Avg Action is ==> 26.198066870274808\n",
            "Episode * 763 * Episodic Reward is ==> -570.5875745302295\n",
            "Episode * 763 * Avg Action is ==> 26.553933621093037\n",
            "Episode * 764 * Episodic Reward is ==> -571.3940563845335\n",
            "Episode * 764 * Avg Action is ==> 26.77997553627269\n",
            "Episode * 765 * Episodic Reward is ==> -570.9219804452993\n",
            "Episode * 765 * Avg Action is ==> 26.779921805893895\n",
            "Episode * 766 * Episodic Reward is ==> -568.7509972037642\n",
            "Episode * 766 * Avg Action is ==> 26.028938160332537\n",
            "Episode * 767 * Episodic Reward is ==> -569.7583274551105\n",
            "Episode * 767 * Avg Action is ==> 26.244755527031018\n",
            "Episode * 768 * Episodic Reward is ==> -567.2465914938454\n",
            "Episode * 768 * Avg Action is ==> 26.031633133184204\n",
            "Episode * 769 * Episodic Reward is ==> -567.5694331996717\n",
            "Episode * 769 * Avg Action is ==> 25.862330450247804\n",
            "Episode * 770 * Episodic Reward is ==> -566.7430600642075\n",
            "Episode * 770 * Avg Action is ==> 25.854624622579852\n",
            "Episode * 771 * Episodic Reward is ==> -566.486797581506\n",
            "Episode * 771 * Avg Action is ==> 25.423403591712948\n",
            "Episode * 772 * Episodic Reward is ==> -564.8133469566391\n",
            "Episode * 772 * Avg Action is ==> 25.093810803559403\n",
            "Episode * 773 * Episodic Reward is ==> -564.4792936855539\n",
            "Episode * 773 * Avg Action is ==> 25.105304242614462\n",
            "Episode * 774 * Episodic Reward is ==> -565.5339081594607\n",
            "Episode * 774 * Avg Action is ==> 25.483335187853065\n",
            "Episode * 775 * Episodic Reward is ==> -564.2335594423654\n",
            "Episode * 775 * Avg Action is ==> 25.244362118138735\n",
            "Episode * 776 * Episodic Reward is ==> -564.1339710328175\n",
            "Episode * 776 * Avg Action is ==> 25.187579466973016\n",
            "Episode * 777 * Episodic Reward is ==> -563.7489803363408\n",
            "Episode * 777 * Avg Action is ==> 24.81095542216687\n",
            "Episode * 778 * Episodic Reward is ==> -563.8030235730112\n",
            "Episode * 778 * Avg Action is ==> 24.883805933982632\n",
            "Episode * 779 * Episodic Reward is ==> -565.568420986207\n",
            "Episode * 779 * Avg Action is ==> 25.28451969993017\n",
            "Episode * 780 * Episodic Reward is ==> -562.9574591816723\n",
            "Episode * 780 * Avg Action is ==> 25.050232648648215\n",
            "Episode * 781 * Episodic Reward is ==> -564.990782370836\n",
            "Episode * 781 * Avg Action is ==> 25.231735552834678\n",
            "Episode * 782 * Episodic Reward is ==> -565.3863800048707\n",
            "Episode * 782 * Avg Action is ==> 25.45173442436321\n",
            "Episode * 783 * Episodic Reward is ==> -564.3714539625556\n",
            "Episode * 783 * Avg Action is ==> 25.279215246791384\n",
            "Episode * 784 * Episodic Reward is ==> -564.425582360905\n",
            "Episode * 784 * Avg Action is ==> 25.41971431798308\n",
            "Episode * 785 * Episodic Reward is ==> -566.5112741474476\n",
            "Episode * 785 * Avg Action is ==> 25.602154870349285\n",
            "Episode * 786 * Episodic Reward is ==> -563.3868253691853\n",
            "Episode * 786 * Avg Action is ==> 25.39783117986607\n",
            "Episode * 787 * Episodic Reward is ==> -563.5006153328651\n",
            "Episode * 787 * Avg Action is ==> 25.06776288785091\n",
            "Episode * 788 * Episodic Reward is ==> -565.1801061690765\n",
            "Episode * 788 * Avg Action is ==> 25.440816631316338\n",
            "Episode * 789 * Episodic Reward is ==> -565.5338401589906\n",
            "Episode * 789 * Avg Action is ==> 25.888559789988193\n",
            "Episode * 790 * Episodic Reward is ==> -563.5629197563773\n",
            "Episode * 790 * Avg Action is ==> 25.296859459369177\n",
            "Episode * 791 * Episodic Reward is ==> -563.5904092200233\n",
            "Episode * 791 * Avg Action is ==> 25.283175769651592\n",
            "Episode * 792 * Episodic Reward is ==> -562.6738078326811\n",
            "Episode * 792 * Avg Action is ==> 25.004927193884516\n",
            "Episode * 793 * Episodic Reward is ==> -562.3515736681285\n",
            "Episode * 793 * Avg Action is ==> 24.71059990731264\n",
            "Episode * 794 * Episodic Reward is ==> -565.0564658735832\n",
            "Episode * 794 * Avg Action is ==> 25.33567516125774\n",
            "Episode * 795 * Episodic Reward is ==> -566.2771669706312\n",
            "Episode * 795 * Avg Action is ==> 25.840256054220532\n",
            "Episode * 796 * Episodic Reward is ==> -565.2034814711297\n",
            "Episode * 796 * Avg Action is ==> 25.48274825582424\n",
            "Episode * 797 * Episodic Reward is ==> -563.7148233193758\n",
            "Episode * 797 * Avg Action is ==> 25.186480597593718\n",
            "Episode * 798 * Episodic Reward is ==> -561.0530408101844\n",
            "Episode * 798 * Avg Action is ==> 24.739438298881527\n",
            "Episode * 799 * Episodic Reward is ==> -561.8713928447492\n",
            "Episode * 799 * Avg Action is ==> 24.76700473962166\n",
            "Episode * 800 * Episodic Reward is ==> -566.1380675178804\n",
            "Episode * 800 * Avg Action is ==> 25.688635901942135\n",
            "Episode * 801 * Episodic Reward is ==> -562.1497226610958\n",
            "Episode * 801 * Avg Action is ==> 24.96896016183686\n",
            "Episode * 802 * Episodic Reward is ==> -566.0153496975245\n",
            "Episode * 802 * Avg Action is ==> 25.736008237522082\n",
            "Episode * 803 * Episodic Reward is ==> -567.4184717384746\n",
            "Episode * 803 * Avg Action is ==> 25.955726809861407\n",
            "Episode * 804 * Episodic Reward is ==> -566.6006673823417\n",
            "Episode * 804 * Avg Action is ==> 25.638297911357466\n",
            "Episode * 805 * Episodic Reward is ==> -568.8068477124646\n",
            "Episode * 805 * Avg Action is ==> 26.324771732354073\n",
            "Episode * 806 * Episodic Reward is ==> -569.983740617319\n",
            "Episode * 806 * Avg Action is ==> 26.71299408989105\n",
            "Episode * 807 * Episodic Reward is ==> -566.8643285589421\n",
            "Episode * 807 * Avg Action is ==> 26.107452506533168\n",
            "Episode * 808 * Episodic Reward is ==> -567.8011940166004\n",
            "Episode * 808 * Avg Action is ==> 26.16536903279076\n",
            "Episode * 809 * Episodic Reward is ==> -566.4717933029107\n",
            "Episode * 809 * Avg Action is ==> 25.8444730540435\n",
            "Episode * 810 * Episodic Reward is ==> -566.4045405486603\n",
            "Episode * 810 * Avg Action is ==> 25.83329711928141\n",
            "Episode * 811 * Episodic Reward is ==> -567.5047219878552\n",
            "Episode * 811 * Avg Action is ==> 26.070467180842883\n",
            "Episode * 812 * Episodic Reward is ==> -567.6286058954536\n",
            "Episode * 812 * Avg Action is ==> 26.15594100125319\n",
            "Episode * 813 * Episodic Reward is ==> -566.096342866215\n",
            "Episode * 813 * Avg Action is ==> 25.85408020232628\n",
            "Episode * 814 * Episodic Reward is ==> -566.3203228027901\n",
            "Episode * 814 * Avg Action is ==> 25.332870264963066\n",
            "Episode * 815 * Episodic Reward is ==> -562.1248076931778\n",
            "Episode * 815 * Avg Action is ==> 24.887570551109164\n",
            "Episode * 816 * Episodic Reward is ==> -562.9641459874645\n",
            "Episode * 816 * Avg Action is ==> 24.769612022042256\n",
            "Episode * 817 * Episodic Reward is ==> -563.5377230903941\n",
            "Episode * 817 * Avg Action is ==> 24.814717430760226\n",
            "Episode * 818 * Episodic Reward is ==> -560.1502591081631\n",
            "Episode * 818 * Avg Action is ==> 24.591785831230485\n",
            "Episode * 819 * Episodic Reward is ==> -561.2992037207921\n",
            "Episode * 819 * Avg Action is ==> 24.776955132050745\n",
            "Episode * 820 * Episodic Reward is ==> -559.6788225125464\n",
            "Episode * 820 * Avg Action is ==> 24.34003183732423\n",
            "Episode * 821 * Episodic Reward is ==> -561.1153635504467\n",
            "Episode * 821 * Avg Action is ==> 24.50480259486085\n",
            "Episode * 822 * Episodic Reward is ==> -560.0186539706227\n",
            "Episode * 822 * Avg Action is ==> 24.416537282760686\n",
            "Episode * 823 * Episodic Reward is ==> -558.2036145372551\n",
            "Episode * 823 * Avg Action is ==> 24.07757597563721\n",
            "Episode * 824 * Episodic Reward is ==> -559.1472433252162\n",
            "Episode * 824 * Avg Action is ==> 23.970573512586697\n",
            "Episode * 825 * Episodic Reward is ==> -558.7770332598723\n",
            "Episode * 825 * Avg Action is ==> 23.977838002612575\n",
            "Episode * 826 * Episodic Reward is ==> -560.443469013387\n",
            "Episode * 826 * Avg Action is ==> 24.447885430570906\n",
            "Episode * 827 * Episodic Reward is ==> -559.4678430028134\n",
            "Episode * 827 * Avg Action is ==> 24.401670569773767\n",
            "Episode * 828 * Episodic Reward is ==> -560.2681969798923\n",
            "Episode * 828 * Avg Action is ==> 24.548266435641484\n",
            "Episode * 829 * Episodic Reward is ==> -561.1948762761749\n",
            "Episode * 829 * Avg Action is ==> 24.81301850895974\n",
            "Episode * 830 * Episodic Reward is ==> -561.394229939499\n",
            "Episode * 830 * Avg Action is ==> 24.67191648227595\n",
            "Episode * 831 * Episodic Reward is ==> -557.7849123042917\n",
            "Episode * 831 * Avg Action is ==> 24.12848042096282\n",
            "Episode * 832 * Episodic Reward is ==> -557.9831749357355\n",
            "Episode * 832 * Avg Action is ==> 23.741142110276872\n",
            "Episode * 833 * Episodic Reward is ==> -574.8047486268796\n",
            "Episode * 833 * Avg Action is ==> 26.092445092392488\n",
            "Episode * 834 * Episodic Reward is ==> -620.9396060450894\n",
            "Episode * 834 * Avg Action is ==> 35.87892957209214\n",
            "Episode * 835 * Episodic Reward is ==> -623.6308536110531\n",
            "Episode * 835 * Avg Action is ==> 36.49670631421067\n",
            "Episode * 836 * Episodic Reward is ==> -628.8697230423536\n",
            "Episode * 836 * Avg Action is ==> 37.77282640158496\n",
            "Episode * 837 * Episodic Reward is ==> -627.4284285577804\n",
            "Episode * 837 * Avg Action is ==> 37.16895327811853\n",
            "Episode * 838 * Episodic Reward is ==> -629.2234688126742\n",
            "Episode * 838 * Avg Action is ==> 37.59338692484693\n",
            "Episode * 839 * Episodic Reward is ==> -634.4693138536638\n",
            "Episode * 839 * Avg Action is ==> 38.62703821443635\n",
            "Episode * 840 * Episodic Reward is ==> -641.1725472218718\n",
            "Episode * 840 * Avg Action is ==> 40.12960473381626\n",
            "Episode * 841 * Episodic Reward is ==> -652.1290115114207\n",
            "Episode * 841 * Avg Action is ==> 42.7978527591758\n",
            "Episode * 842 * Episodic Reward is ==> -661.5672624823619\n",
            "Episode * 842 * Avg Action is ==> 45.02401849204345\n",
            "Episode * 843 * Episodic Reward is ==> -655.1709021179078\n",
            "Episode * 843 * Avg Action is ==> 45.27439375416501\n",
            "Episode * 844 * Episodic Reward is ==> -645.9467745423011\n",
            "Episode * 844 * Avg Action is ==> 43.70878026054566\n",
            "Episode * 845 * Episodic Reward is ==> -647.5510634870965\n",
            "Episode * 845 * Avg Action is ==> 44.0017866259611\n",
            "Episode * 846 * Episodic Reward is ==> -665.5225659422015\n",
            "Episode * 846 * Avg Action is ==> 46.895771836541016\n",
            "Episode * 847 * Episodic Reward is ==> -681.5252907328077\n",
            "Episode * 847 * Avg Action is ==> 50.034729020810424\n",
            "Episode * 848 * Episodic Reward is ==> -648.0328615053152\n",
            "Episode * 848 * Avg Action is ==> 44.022207761832945\n",
            "Episode * 849 * Episodic Reward is ==> -648.6636824425968\n",
            "Episode * 849 * Avg Action is ==> 43.114831348944264\n",
            "Episode * 850 * Episodic Reward is ==> -634.2300975753218\n",
            "Episode * 850 * Avg Action is ==> 41.1903701903177\n",
            "Episode * 851 * Episodic Reward is ==> -636.0945603709397\n",
            "Episode * 851 * Avg Action is ==> 41.32578588989366\n",
            "Episode * 852 * Episodic Reward is ==> -633.5902458150806\n",
            "Episode * 852 * Avg Action is ==> 40.752438596400594\n",
            "Episode * 853 * Episodic Reward is ==> -637.9494714729586\n",
            "Episode * 853 * Avg Action is ==> 41.34359742061935\n",
            "Episode * 854 * Episodic Reward is ==> -608.2013998445819\n",
            "Episode * 854 * Avg Action is ==> 30.697415173605513\n",
            "Episode * 855 * Episodic Reward is ==> -561.7464006653242\n",
            "Episode * 855 * Avg Action is ==> 24.955070844846084\n",
            "Episode * 856 * Episodic Reward is ==> -562.493040788237\n",
            "Episode * 856 * Avg Action is ==> 24.982934858848708\n",
            "Episode * 857 * Episodic Reward is ==> -561.1371410129876\n",
            "Episode * 857 * Avg Action is ==> 24.793644444305112\n",
            "Episode * 858 * Episodic Reward is ==> -561.9278070210386\n",
            "Episode * 858 * Avg Action is ==> 24.836437377584677\n",
            "Episode * 859 * Episodic Reward is ==> -559.7492477467015\n",
            "Episode * 859 * Avg Action is ==> 24.56559423944377\n",
            "Episode * 860 * Episodic Reward is ==> -558.9856568205364\n",
            "Episode * 860 * Avg Action is ==> 24.34613552999995\n",
            "Episode * 861 * Episodic Reward is ==> -561.0044572038038\n",
            "Episode * 861 * Avg Action is ==> 24.45220307915043\n",
            "Episode * 862 * Episodic Reward is ==> -572.8137434013685\n",
            "Episode * 862 * Avg Action is ==> 26.86799006259304\n",
            "Episode * 863 * Episodic Reward is ==> -568.4642540975993\n",
            "Episode * 863 * Avg Action is ==> 26.48640907604657\n",
            "Episode * 864 * Episodic Reward is ==> -569.0978868679645\n",
            "Episode * 864 * Avg Action is ==> 26.547029526276848\n",
            "Episode * 865 * Episodic Reward is ==> -577.3668580960153\n",
            "Episode * 865 * Avg Action is ==> 27.295420623693484\n",
            "Episode * 866 * Episodic Reward is ==> -596.6640656103092\n",
            "Episode * 866 * Avg Action is ==> 30.99721097221338\n",
            "Episode * 867 * Episodic Reward is ==> -598.7620587703176\n",
            "Episode * 867 * Avg Action is ==> 31.447003867537088\n",
            "Episode * 868 * Episodic Reward is ==> -562.9288314877429\n",
            "Episode * 868 * Avg Action is ==> 25.048856609843412\n",
            "Episode * 869 * Episodic Reward is ==> -560.5082885643088\n",
            "Episode * 869 * Avg Action is ==> 24.85817001640087\n",
            "Episode * 870 * Episodic Reward is ==> -561.7558087416163\n",
            "Episode * 870 * Avg Action is ==> 25.106149037632427\n",
            "Episode * 871 * Episodic Reward is ==> -559.9955936764194\n",
            "Episode * 871 * Avg Action is ==> 24.778884235842796\n",
            "Episode * 872 * Episodic Reward is ==> -561.6384258710244\n",
            "Episode * 872 * Avg Action is ==> 24.931973246085164\n",
            "Episode * 873 * Episodic Reward is ==> -562.9309305071424\n",
            "Episode * 873 * Avg Action is ==> 25.36136218341425\n",
            "Episode * 874 * Episodic Reward is ==> -561.8829877282817\n",
            "Episode * 874 * Avg Action is ==> 25.229286094468115\n",
            "Episode * 875 * Episodic Reward is ==> -561.4902549561888\n",
            "Episode * 875 * Avg Action is ==> 24.936527609830602\n",
            "Episode * 876 * Episodic Reward is ==> -560.9739904408427\n",
            "Episode * 876 * Avg Action is ==> 24.975233769829693\n",
            "Episode * 877 * Episodic Reward is ==> -558.7283299436376\n",
            "Episode * 877 * Avg Action is ==> 24.407546405385546\n",
            "Episode * 878 * Episodic Reward is ==> -560.5286008475557\n",
            "Episode * 878 * Avg Action is ==> 24.489219891815214\n",
            "Episode * 879 * Episodic Reward is ==> -559.52207796282\n",
            "Episode * 879 * Avg Action is ==> 24.51191380834158\n",
            "Episode * 880 * Episodic Reward is ==> -559.3626756436595\n",
            "Episode * 880 * Avg Action is ==> 24.471656844595465\n",
            "Episode * 881 * Episodic Reward is ==> -559.8668922496893\n",
            "Episode * 881 * Avg Action is ==> 24.601004405438555\n",
            "Episode * 882 * Episodic Reward is ==> -560.5261397498016\n",
            "Episode * 882 * Avg Action is ==> 24.74473182896272\n",
            "Episode * 883 * Episodic Reward is ==> -559.1818057500683\n",
            "Episode * 883 * Avg Action is ==> 24.50574000657586\n",
            "Episode * 884 * Episodic Reward is ==> -559.9281519541098\n",
            "Episode * 884 * Avg Action is ==> 24.461638154185476\n",
            "Episode * 885 * Episodic Reward is ==> -558.5648637326549\n",
            "Episode * 885 * Avg Action is ==> 24.4144507831048\n",
            "Episode * 886 * Episodic Reward is ==> -558.659458749807\n",
            "Episode * 886 * Avg Action is ==> 24.043110217523427\n",
            "Episode * 887 * Episodic Reward is ==> -568.157656779523\n",
            "Episode * 887 * Avg Action is ==> 26.36582226923952\n",
            "Episode * 888 * Episodic Reward is ==> -557.7415355309722\n",
            "Episode * 888 * Avg Action is ==> 24.04389036185705\n",
            "Episode * 889 * Episodic Reward is ==> -556.5939322587454\n",
            "Episode * 889 * Avg Action is ==> 23.896929293612246\n",
            "Episode * 890 * Episodic Reward is ==> -557.3569388964542\n",
            "Episode * 890 * Avg Action is ==> 24.04255747933588\n",
            "Episode * 891 * Episodic Reward is ==> -557.7448052749633\n",
            "Episode * 891 * Avg Action is ==> 24.297052739312264\n",
            "Episode * 892 * Episodic Reward is ==> -601.9961576857197\n",
            "Episode * 892 * Avg Action is ==> 31.80407415326674\n",
            "Episode * 893 * Episodic Reward is ==> -608.4165520273339\n",
            "Episode * 893 * Avg Action is ==> 33.31828479674138\n",
            "Episode * 894 * Episodic Reward is ==> -612.3438747330375\n",
            "Episode * 894 * Avg Action is ==> 34.24345802185504\n",
            "Episode * 895 * Episodic Reward is ==> -604.4110636199388\n",
            "Episode * 895 * Avg Action is ==> 33.201285459342806\n",
            "Episode * 896 * Episodic Reward is ==> -589.2537195557437\n",
            "Episode * 896 * Avg Action is ==> 29.933122466781445\n",
            "Episode * 897 * Episodic Reward is ==> -587.4831017552303\n",
            "Episode * 897 * Avg Action is ==> 29.5508869521627\n",
            "Episode * 898 * Episodic Reward is ==> -587.4925438304438\n",
            "Episode * 898 * Avg Action is ==> 29.494887824289584\n",
            "Episode * 899 * Episodic Reward is ==> -587.0349889752436\n",
            "Episode * 899 * Avg Action is ==> 29.356369790932536\n",
            "Episode * 900 * Episodic Reward is ==> -587.1676241585112\n",
            "Episode * 900 * Avg Action is ==> 29.286840266532604\n",
            "Episode * 901 * Episodic Reward is ==> -595.6570503730542\n",
            "Episode * 901 * Avg Action is ==> 30.819467649517502\n",
            "Episode * 902 * Episodic Reward is ==> -601.5567578710471\n",
            "Episode * 902 * Avg Action is ==> 31.824275078630997\n",
            "Episode * 903 * Episodic Reward is ==> -602.0586470953995\n",
            "Episode * 903 * Avg Action is ==> 32.181550934436814\n",
            "Episode * 904 * Episodic Reward is ==> -603.1558411272631\n",
            "Episode * 904 * Avg Action is ==> 32.453155861333016\n",
            "Episode * 905 * Episodic Reward is ==> -601.2773627266267\n",
            "Episode * 905 * Avg Action is ==> 32.156639829239644\n",
            "Episode * 906 * Episodic Reward is ==> -600.173407962671\n",
            "Episode * 906 * Avg Action is ==> 31.58172082063685\n",
            "Episode * 907 * Episodic Reward is ==> -599.3263391119856\n",
            "Episode * 907 * Avg Action is ==> 31.717571897726717\n",
            "Episode * 908 * Episodic Reward is ==> -601.6018624864128\n",
            "Episode * 908 * Avg Action is ==> 32.08343741322961\n",
            "Episode * 909 * Episodic Reward is ==> -589.9341998447006\n",
            "Episode * 909 * Avg Action is ==> 29.904368771765633\n",
            "Episode * 910 * Episodic Reward is ==> -590.1962442467368\n",
            "Episode * 910 * Avg Action is ==> 30.120666496791664\n",
            "Episode * 911 * Episodic Reward is ==> -590.3696976589256\n",
            "Episode * 911 * Avg Action is ==> 30.106156592244126\n",
            "Episode * 912 * Episodic Reward is ==> -599.2413152588128\n",
            "Episode * 912 * Avg Action is ==> 31.74602150390795\n",
            "Episode * 913 * Episodic Reward is ==> -597.0526392470813\n",
            "Episode * 913 * Avg Action is ==> 30.87721041477227\n",
            "Episode * 914 * Episodic Reward is ==> -596.2315884696249\n",
            "Episode * 914 * Avg Action is ==> 31.03905591485409\n",
            "Episode * 915 * Episodic Reward is ==> -596.8277388049163\n",
            "Episode * 915 * Avg Action is ==> 31.238740271968727\n",
            "Episode * 916 * Episodic Reward is ==> -595.0180362577448\n",
            "Episode * 916 * Avg Action is ==> 30.901116155702468\n",
            "Episode * 917 * Episodic Reward is ==> -596.1335893021064\n",
            "Episode * 917 * Avg Action is ==> 30.86546764951077\n",
            "Episode * 918 * Episodic Reward is ==> -594.964085641747\n",
            "Episode * 918 * Avg Action is ==> 30.602504119606255\n",
            "Episode * 919 * Episodic Reward is ==> -591.9327861412269\n",
            "Episode * 919 * Avg Action is ==> 30.343073826084627\n",
            "Episode * 920 * Episodic Reward is ==> -595.5493888931602\n",
            "Episode * 920 * Avg Action is ==> 31.05871524056724\n",
            "Episode * 921 * Episodic Reward is ==> -593.5882648647989\n",
            "Episode * 921 * Avg Action is ==> 30.446325971317307\n",
            "Episode * 922 * Episodic Reward is ==> -601.0937927700968\n",
            "Episode * 922 * Avg Action is ==> 31.5339879799803\n",
            "Episode * 923 * Episodic Reward is ==> -591.2734613859644\n",
            "Episode * 923 * Avg Action is ==> 29.803732095309606\n",
            "Episode * 924 * Episodic Reward is ==> -590.9451024646349\n",
            "Episode * 924 * Avg Action is ==> 29.826371184881648\n",
            "Episode * 925 * Episodic Reward is ==> -589.3839992154748\n",
            "Episode * 925 * Avg Action is ==> 29.787422401970638\n",
            "Episode * 926 * Episodic Reward is ==> -589.2496407401889\n",
            "Episode * 926 * Avg Action is ==> 29.817444539584155\n",
            "Episode * 927 * Episodic Reward is ==> -585.7406647181397\n",
            "Episode * 927 * Avg Action is ==> 29.018684316298422\n",
            "Episode * 928 * Episodic Reward is ==> -586.6564821496027\n",
            "Episode * 928 * Avg Action is ==> 29.035868493070737\n",
            "Episode * 929 * Episodic Reward is ==> -585.2557032843348\n",
            "Episode * 929 * Avg Action is ==> 27.310140386371785\n",
            "Episode * 930 * Episodic Reward is ==> -557.9252755515137\n",
            "Episode * 930 * Avg Action is ==> 24.117572468629948\n",
            "Episode * 931 * Episodic Reward is ==> -557.8518366738343\n",
            "Episode * 931 * Avg Action is ==> 24.132885782979923\n",
            "Episode * 932 * Episodic Reward is ==> -566.3841036276183\n",
            "Episode * 932 * Avg Action is ==> 26.141330847748875\n",
            "Episode * 933 * Episodic Reward is ==> -565.7936259476513\n",
            "Episode * 933 * Avg Action is ==> 25.933783637614372\n",
            "Episode * 934 * Episodic Reward is ==> -564.1419339972149\n",
            "Episode * 934 * Avg Action is ==> 25.678304108085975\n",
            "Episode * 935 * Episodic Reward is ==> -565.1729085582381\n",
            "Episode * 935 * Avg Action is ==> 24.978517882459833\n",
            "Episode * 936 * Episodic Reward is ==> -556.4213505608545\n",
            "Episode * 936 * Avg Action is ==> 23.61345258910427\n",
            "Episode * 937 * Episodic Reward is ==> -555.0848214857109\n",
            "Episode * 937 * Avg Action is ==> 23.359844187936467\n",
            "Episode * 938 * Episodic Reward is ==> -555.9942971630046\n",
            "Episode * 938 * Avg Action is ==> 23.514793538025845\n",
            "Episode * 939 * Episodic Reward is ==> -555.7999484223999\n",
            "Episode * 939 * Avg Action is ==> 23.469289066486386\n",
            "Episode * 940 * Episodic Reward is ==> -559.3909617357624\n",
            "Episode * 940 * Avg Action is ==> 24.087087216572137\n",
            "Episode * 941 * Episodic Reward is ==> -561.6352889696666\n",
            "Episode * 941 * Avg Action is ==> 24.332178139399275\n",
            "Episode * 942 * Episodic Reward is ==> -554.7718975171066\n",
            "Episode * 942 * Avg Action is ==> 23.19296860147487\n",
            "Episode * 943 * Episodic Reward is ==> -553.688900872113\n",
            "Episode * 943 * Avg Action is ==> 22.973466835131777\n",
            "Episode * 944 * Episodic Reward is ==> -553.8554140788269\n",
            "Episode * 944 * Avg Action is ==> 22.825124595243746\n",
            "Episode * 945 * Episodic Reward is ==> -554.9377191244794\n",
            "Episode * 945 * Avg Action is ==> 23.012882789060267\n",
            "Episode * 946 * Episodic Reward is ==> -560.40562805377\n",
            "Episode * 946 * Avg Action is ==> 23.893322726437276\n",
            "Episode * 947 * Episodic Reward is ==> -562.7272214679508\n",
            "Episode * 947 * Avg Action is ==> 25.145789387838384\n",
            "Episode * 948 * Episodic Reward is ==> -560.83789250082\n",
            "Episode * 948 * Avg Action is ==> 24.831266278848343\n",
            "Episode * 949 * Episodic Reward is ==> -560.4156575790316\n",
            "Episode * 949 * Avg Action is ==> 24.576806773300845\n",
            "Episode * 950 * Episodic Reward is ==> -560.6991792244964\n",
            "Episode * 950 * Avg Action is ==> 24.567971264367333\n",
            "Episode * 951 * Episodic Reward is ==> -559.9768678697549\n",
            "Episode * 951 * Avg Action is ==> 24.032828762296905\n",
            "Episode * 952 * Episodic Reward is ==> -561.0155459576125\n",
            "Episode * 952 * Avg Action is ==> 24.78461622244166\n",
            "Episode * 953 * Episodic Reward is ==> -560.4220433755685\n",
            "Episode * 953 * Avg Action is ==> 24.707481993736796\n",
            "Episode * 954 * Episodic Reward is ==> -562.1893190068574\n",
            "Episode * 954 * Avg Action is ==> 24.884458143430955\n",
            "Episode * 955 * Episodic Reward is ==> -561.4211077081459\n",
            "Episode * 955 * Avg Action is ==> 24.902559901152618\n",
            "Episode * 956 * Episodic Reward is ==> -560.3424336992815\n",
            "Episode * 956 * Avg Action is ==> 24.6709926711368\n",
            "Episode * 957 * Episodic Reward is ==> -559.5486525602263\n",
            "Episode * 957 * Avg Action is ==> 24.319135762719004\n",
            "Episode * 958 * Episodic Reward is ==> -560.2936777696682\n",
            "Episode * 958 * Avg Action is ==> 24.719064643780754\n",
            "Episode * 959 * Episodic Reward is ==> -561.090616454433\n",
            "Episode * 959 * Avg Action is ==> 24.69039361458457\n",
            "Episode * 960 * Episodic Reward is ==> -556.4855977747052\n",
            "Episode * 960 * Avg Action is ==> 23.3920934979191\n",
            "Episode * 961 * Episodic Reward is ==> -558.8530798726852\n",
            "Episode * 961 * Avg Action is ==> 24.012008193788812\n",
            "Episode * 962 * Episodic Reward is ==> -554.9318063071966\n",
            "Episode * 962 * Avg Action is ==> 23.32830996844787\n",
            "Episode * 963 * Episodic Reward is ==> -560.1971189519832\n",
            "Episode * 963 * Avg Action is ==> 24.24598681118321\n",
            "Episode * 964 * Episodic Reward is ==> -560.1862874254856\n",
            "Episode * 964 * Avg Action is ==> 24.00573341513867\n",
            "Episode * 965 * Episodic Reward is ==> -557.513494703564\n",
            "Episode * 965 * Avg Action is ==> 23.667017928937742\n",
            "Episode * 966 * Episodic Reward is ==> -558.9374031817886\n",
            "Episode * 966 * Avg Action is ==> 23.88344325745374\n",
            "Episode * 967 * Episodic Reward is ==> -557.127669358292\n",
            "Episode * 967 * Avg Action is ==> 23.9035679452854\n",
            "Episode * 968 * Episodic Reward is ==> -553.6545061394365\n",
            "Episode * 968 * Avg Action is ==> 22.773564461481495\n",
            "Episode * 969 * Episodic Reward is ==> -548.3860556332603\n",
            "Episode * 969 * Avg Action is ==> 21.778375239400035\n",
            "Episode * 970 * Episodic Reward is ==> -548.6550961076551\n",
            "Episode * 970 * Avg Action is ==> 21.663301572745823\n",
            "Episode * 971 * Episodic Reward is ==> -553.0050703578103\n",
            "Episode * 971 * Avg Action is ==> 22.55560210372106\n",
            "Episode * 972 * Episodic Reward is ==> -554.1066369470411\n",
            "Episode * 972 * Avg Action is ==> 23.117256246516302\n",
            "Episode * 973 * Episodic Reward is ==> -554.5868827727724\n",
            "Episode * 973 * Avg Action is ==> 23.227026852529185\n",
            "Episode * 974 * Episodic Reward is ==> -559.4149859600002\n",
            "Episode * 974 * Avg Action is ==> 24.400589703007153\n",
            "Episode * 975 * Episodic Reward is ==> -557.2468777002592\n",
            "Episode * 975 * Avg Action is ==> 23.96557122094031\n",
            "Episode * 976 * Episodic Reward is ==> -558.1012457193964\n",
            "Episode * 976 * Avg Action is ==> 23.951498566898866\n",
            "Episode * 977 * Episodic Reward is ==> -557.7187456448796\n",
            "Episode * 977 * Avg Action is ==> 23.76312149927028\n",
            "Episode * 978 * Episodic Reward is ==> -558.455048728046\n",
            "Episode * 978 * Avg Action is ==> 24.01278581057513\n",
            "Episode * 979 * Episodic Reward is ==> -557.90098325946\n",
            "Episode * 979 * Avg Action is ==> 23.90160909185607\n",
            "Episode * 980 * Episodic Reward is ==> -558.6934852355382\n",
            "Episode * 980 * Avg Action is ==> 24.027952954666873\n",
            "Episode * 981 * Episodic Reward is ==> -558.8768462645919\n",
            "Episode * 981 * Avg Action is ==> 24.185464831674064\n",
            "Episode * 982 * Episodic Reward is ==> -557.7637384807254\n",
            "Episode * 982 * Avg Action is ==> 23.88038725861108\n",
            "Episode * 983 * Episodic Reward is ==> -557.5885558197137\n",
            "Episode * 983 * Avg Action is ==> 23.69343183817971\n",
            "Episode * 984 * Episodic Reward is ==> -559.0635002493822\n",
            "Episode * 984 * Avg Action is ==> 24.15663737962495\n",
            "Episode * 985 * Episodic Reward is ==> -559.7331277773163\n",
            "Episode * 985 * Avg Action is ==> 24.49048348680535\n",
            "Episode * 986 * Episodic Reward is ==> -559.9461753960369\n",
            "Episode * 986 * Avg Action is ==> 24.318083899007753\n",
            "Episode * 987 * Episodic Reward is ==> -557.7912966849235\n",
            "Episode * 987 * Avg Action is ==> 23.75471838007796\n",
            "Episode * 988 * Episodic Reward is ==> -554.2737056021776\n",
            "Episode * 988 * Avg Action is ==> 23.09680261611411\n",
            "Episode * 989 * Episodic Reward is ==> -554.6680292184992\n",
            "Episode * 989 * Avg Action is ==> 23.183479294712132\n",
            "Episode * 990 * Episodic Reward is ==> -553.7002760883344\n",
            "Episode * 990 * Avg Action is ==> 23.09926850755232\n",
            "Episode * 991 * Episodic Reward is ==> -553.6278530429142\n",
            "Episode * 991 * Avg Action is ==> 22.807335328433144\n",
            "Episode * 992 * Episodic Reward is ==> -554.050010224467\n",
            "Episode * 992 * Avg Action is ==> 22.93840866568261\n",
            "Episode * 993 * Episodic Reward is ==> -555.329158285518\n",
            "Episode * 993 * Avg Action is ==> 23.245865370936944\n",
            "Episode * 994 * Episodic Reward is ==> -555.1947575298368\n",
            "Episode * 994 * Avg Action is ==> 23.037205572586966\n",
            "Episode * 995 * Episodic Reward is ==> -560.7508478580997\n",
            "Episode * 995 * Avg Action is ==> 23.684713639523967\n",
            "Episode * 996 * Episodic Reward is ==> -548.716579418252\n",
            "Episode * 996 * Avg Action is ==> 21.060390296414557\n",
            "Episode * 997 * Episodic Reward is ==> -548.0353328180273\n",
            "Episode * 997 * Avg Action is ==> 21.53411599098115\n",
            "Episode * 998 * Episodic Reward is ==> -551.1672755920978\n",
            "Episode * 998 * Avg Action is ==> 21.211334973147657\n",
            "Episode * 999 * Episodic Reward is ==> -548.9878216875904\n",
            "Episode * 999 * Avg Action is ==> 21.728087832880185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_ATS7LNcpma"
      },
      "source": [
        "pisode * 445 * Avg Action is ==> 23.622088526907206\n",
        "Episode * 446 * Episodic Reward is ==> -554.8173259769125\n",
        "Episode * 446 * Avg Action is ==> 22.107861473494474\n",
        "Episode * 447 * Episodic Reward is ==> -562.1585233577445\n",
        "Episode * 447 * Avg Action is ==> 22.35101448271628\n",
        "Episode * 448 * Episodic Reward is ==> -555.1460709389372\n",
        "Episode * 448 * Avg Action is ==> 20.41383171602479\n",
        "Episode * 449 * Episodic Reward is ==> -552.7534893044752\n",
        "Episode * 449 * Avg Action is ==> 21.621465369075917"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ebkxXmuYS1mG",
        "outputId": "3d8a3436-4dc0-4c73-e58c-4fc40cd0abbc"
      },
      "source": [
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkI-ri4d2O3w"
      },
      "source": [
        "actor_model.save_weights(\"y5actor.h5\")\n",
        "critic_model.save_weights(\"y5critic.h5\")\n",
        "\n",
        "target_actor.save_weights(\"y5target_actor.h5\")\n",
        "target_critic.save_weights(\"y5target_critic.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPtGNTiWz_BC"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(ep_reward_list)\n",
        "df.to_csv (r'training4.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxnJ_x1sFDog"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "def func(x, a, b, c):\n",
        "    return a * np.exp(-b * x) + c\n",
        "\n",
        "x = np.linspace(0,4,50)\n",
        "y = func(x, 2.5, 1.3, 0.5)\n",
        "yn = y + 0.2*np.random.normal(size=len(x))\n",
        "\n",
        "popt, pcov = curve_fit(func, x, yn)\n",
        "popt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "-EYjFL4524Yc",
        "outputId": "a7342833-6e92-4954-fee3-6c6a4cfb733c"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "from os import path\n",
        "import control\n",
        "from  control.matlab import *\n",
        "import tensorflow as tfl\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#from Buffer import Buffer\n",
        "#from OUActionNoise import OUActionNoise\n",
        "\n",
        "Us = [0]\n",
        "ts = np.array([0])\n",
        "yout = []\n",
        "\n",
        "class environment():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "            self.kp = 29.5047797848936\n",
        "            self.ki = 2.41863698260906\n",
        "            self.kd = 0.013145\n",
        "            self.j = 1 \n",
        "            self.max_input = 100\n",
        "            #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.I = 0\n",
        "            \n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "            self.Us = [0]\n",
        "            self.ts = np.array([0])\n",
        "            self.yout = []\n",
        "            #self.e00 = 0\n",
        "           #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.j = 1 \n",
        "            self.I = 0\n",
        "            u1 = 0\n",
        "            \n",
        "            #self.rl = random.randrange(-self.max_input, self.max_input)\n",
        "            return np.array([self.e2,u1])\n",
        "\n",
        "    def step(self ,a) :\n",
        "            \n",
        "\n",
        "            mot0 = tf([0.4602, 1.882, 2.038, 0.2338, 0.007103], [62.85, 383.5, 803.4, 624.1, 99.79, 5.916, 0.1204])\n",
        "            self.rl = np.clip(a, -self.max_input, self.max_input)[0]\n",
        "            \n",
        "            P = 29.5047797848936*self.e2\n",
        "            self.I = self.I + 2.41863698260906*(self.e2)*0.1\n",
        "            D = -0.013145*(self.e2-self.e1)/0.1\n",
        "            self.u = P + self.I + D\n",
        "            \n",
        "            uu = self.u + self.rl\n",
        "            \n",
        "            self.Us = np.append(self.Us,uu)\n",
        "            self.ts = np.append(self.ts,0.1*self.j)\n",
        "            y, T, xoutd = lsim(3.6*mot0, U=self.Us, T=self.ts)\n",
        "            self.yout.append(y[-1])\n",
        "            #self.ynow = y[-1]\n",
        "            self.e1 = self.e2\n",
        "            self.e2 = 30-y[-1]\n",
        "            self.j+=1\n",
        "            P1 = 29.5047797848936*self.e2\n",
        "            I1 = self.I\n",
        "            I1 = I1 + 2.41863698260906*(self.e2)*0.1\n",
        "            D1 = -0.013145*(self.e2-self.e1)/0.1\n",
        "            u1 = P1 + I1 + D1\n",
        "            \n",
        "            \n",
        "            #reward = 0.8*np.exp(-0.5*(self.e2)**2) + 0.2*np.exp(-np.absolute(self.rl))\n",
        "            reward = -(self.e2)**2\n",
        "\n",
        "            if self.j >= 500:\n",
        "              done = True\n",
        "            else :\n",
        "              done = False\n",
        "            self.state = np.array([self.e2,u1])\n",
        "            return self.state, reward, done, {}\n",
        "\n",
        "env = environment()\n",
        "num_states = 2\n",
        "num_actions = 1\n",
        "upper_bound = 100\n",
        "lower_bound = -100\n",
        "\n",
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tfl.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tfl.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tfl.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tfl.math.reduce_mean(tfl.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tfl.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tfl.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tfl.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tfl.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tfl.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tfl.cast(reward_batch, dtype=tfl.float32)\n",
        "        next_state_batch = tfl.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tfl.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tfl.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tfl.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tfl.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def policy(state, noise_object):\n",
        "    sampled_actions = tfl.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]\n",
        "\n",
        "std_dev = 0.1\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tfl.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tfl.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 200\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(10000, 64)\n",
        "\n",
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tfl.expand_dims(tfl.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "        #print(j)\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    print(\"Episode * {} * Episodic Reward is ==> {}\".format(ep, episodic_reward))\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    #print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/control/timeresp.py:294: UserWarning: return_x specified for a transfer function system. Internal conversion to state space used; results may meaningless.\n",
            "  \"return_x specified for a transfer function system. Internal \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Episodic Reward is ==> -15583.692371196457\n",
            "Episode * 1 * Episodic Reward is ==> -15721.728900749196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cc25a64cb9d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_prev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mou_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Recieve state and reward from environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-cc25a64cb9d9>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxoutd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmot0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m#self.ynow = y[-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/matlab/timeresp.py\u001b[0m in \u001b[0;36mlsim\u001b[0;34m(sys, U, T, X0)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;31m# Switch output argument order and transpose outputs (and always return x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforced_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/timeresp.py\u001b[0m in \u001b[0;36mforced_response\u001b[0;34m(sys, T, U, X0, transpose, interpolate, return_x, squeeze)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 xout[:, i] = (dot(Ad, xout[:, i-1]) + dot(Bd0, U[:, i-1]) +\n\u001b[0m\u001b[1;32m    407\u001b[0m                               dot(Bd1, U[:, i]))\n\u001b[1;32m    408\u001b[0m             \u001b[0myout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxm63JSW5k_e"
      },
      "source": [
        "actor_model.save_weights(\"80actor.h5\")\n",
        "critic_model.save_weights(\"80critic.h5\")\n",
        "\n",
        "target_actor.save_weights(\"80target_actor.h5\")\n",
        "target_critic.save_weights(\"80target_critic.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VQpMdPNB9JJM",
        "outputId": "12658842-007e-4a72-fe1d-240db9691ecb"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "from os import path\n",
        "import control\n",
        "from  control.matlab import *\n",
        "import tensorflow as tfl\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#from Buffer import Buffer\n",
        "#from OUActionNoise import OUActionNoise\n",
        "\n",
        "Us = [0]\n",
        "ts = np.array([0])\n",
        "yout = []\n",
        "\n",
        "class environment():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "            self.kp = 34.5047797848936\n",
        "            self.ki = 2.41863698260906\n",
        "            self.kd = 0.013145\n",
        "            self.j = 1 \n",
        "            self.max_input = 500\n",
        "            #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.I = 0\n",
        "            \n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "            self.Us = [0]\n",
        "            self.ts = np.array([0])\n",
        "            self.yout = []\n",
        "            #self.e00 = 0\n",
        "           #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.j = 1 \n",
        "            self.I = 0\n",
        "            u1 = 0\n",
        "            \n",
        "            #self.rl = random.randrange(-self.max_input, self.max_input)\n",
        "            return np.array([self.e2,u1])\n",
        "\n",
        "    def step(self ,a) :\n",
        "            \n",
        "\n",
        "            mot0 = tf([0.4602, 1.882, 2.038, 0.2338, 0.007103], [62.85, 383.5, 803.4, 624.1, 99.79, 5.916, 0.1204])\n",
        "            self.rl = np.clip(a, -self.max_input, self.max_input)[0]\n",
        "            \n",
        "            P = 34.5047797848936*self.e2\n",
        "            self.I = self.I + 2.41863698260906*(self.e2)*0.1\n",
        "            D = -0.013145*(self.e2-self.e1)/0.1\n",
        "            self.u = P + self.I + D\n",
        "            \n",
        "            uu = self.u + self.rl\n",
        "            \n",
        "            self.Us = np.append(self.Us,uu)\n",
        "            self.ts = np.append(self.ts,0.1*self.j)\n",
        "            y, T, xoutd = lsim(3.6*mot0, U=self.Us, T=self.ts)\n",
        "            self.yout.append(y[-1])\n",
        "            #self.ynow = y[-1]\n",
        "            self.e1 = self.e2\n",
        "            self.e2 = 30-y[-1]\n",
        "            self.j+=1\n",
        "            P1 = 34.5047797848936*self.e2\n",
        "            I1 = self.I\n",
        "            I1 = I1 + 2.41863698260906*(self.e2)*0.1\n",
        "            D1 = -0.013145*(self.e2-self.e1)/0.1\n",
        "            u1 = P1 + I1 + D1\n",
        "            \n",
        "            \n",
        "            #reward = 0.8*np.exp(-0.5*(self.e2)**2) + 0.2*np.exp(-np.absolute(self.rl))\n",
        "            reward = -(self.e2)**2\n",
        "\n",
        "            if self.j >= 700:\n",
        "              done = True\n",
        "            else :\n",
        "              done = False\n",
        "            self.state = np.array([self.e2,u1])\n",
        "            return self.state, reward, done, {}\n",
        "\n",
        "env = environment()\n",
        "num_states = 2\n",
        "num_actions = 1\n",
        "upper_bound = 500\n",
        "lower_bound = -500\n",
        "\n",
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tfl.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tfl.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tfl.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tfl.math.reduce_mean(tfl.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tfl.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tfl.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tfl.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tfl.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tfl.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tfl.cast(reward_batch, dtype=tfl.float32)\n",
        "        next_state_batch = tfl.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tfl.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tfl.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tfl.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tfl.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def policy(state, noise_object):\n",
        "    sampled_actions = tfl.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]\n",
        "\n",
        "std_dev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tfl.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tfl.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 120\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(100000, 256)\n",
        "\n",
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tfl.expand_dims(tfl.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "        #print(j)\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    print(\"Episode * {} * Episodic Reward is ==> {}\".format(ep, episodic_reward))\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    #print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/control/timeresp.py:294: UserWarning: return_x specified for a transfer function system. Internal conversion to state space used; results may meaningless.\n",
            "  \"return_x specified for a transfer function system. Internal \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Episodic Reward is ==> -30608.676069160778\n",
            "Episode * 1 * Episodic Reward is ==> -31339.239198267594\n",
            "Episode * 2 * Episodic Reward is ==> -31345.55314530479\n",
            "Episode * 3 * Episodic Reward is ==> -31348.563042187907\n",
            "Episode * 4 * Episodic Reward is ==> -31349.40910323367\n",
            "Episode * 5 * Episodic Reward is ==> -31370.69205430628\n",
            "Episode * 6 * Episodic Reward is ==> -31332.349013007704\n",
            "Episode * 7 * Episodic Reward is ==> -31347.255992162263\n",
            "Episode * 8 * Episodic Reward is ==> -31354.986335336696\n",
            "Episode * 9 * Episodic Reward is ==> -31305.191184348918\n",
            "Episode * 10 * Episodic Reward is ==> -31360.218749776133\n",
            "Episode * 11 * Episodic Reward is ==> -31355.35929953813\n",
            "Episode * 12 * Episodic Reward is ==> -31356.767766769062\n",
            "Episode * 13 * Episodic Reward is ==> -31375.052263182326\n",
            "Episode * 14 * Episodic Reward is ==> -31339.187450550646\n",
            "Episode * 15 * Episodic Reward is ==> -31365.653575262426\n",
            "Episode * 16 * Episodic Reward is ==> -31352.86148198814\n",
            "Episode * 17 * Episodic Reward is ==> -31355.36636586428\n",
            "Episode * 18 * Episodic Reward is ==> -31378.16336852461\n",
            "Episode * 19 * Episodic Reward is ==> -31359.337803409297\n",
            "Episode * 20 * Episodic Reward is ==> -31371.835666795876\n",
            "Episode * 21 * Episodic Reward is ==> -31338.568729867464\n",
            "Episode * 22 * Episodic Reward is ==> -31341.88006075874\n",
            "Episode * 23 * Episodic Reward is ==> -31339.61824236596\n",
            "Episode * 24 * Episodic Reward is ==> -31323.086011340973\n",
            "Episode * 25 * Episodic Reward is ==> -31301.797121684478\n",
            "Episode * 26 * Episodic Reward is ==> -31333.52772597676\n",
            "Episode * 27 * Episodic Reward is ==> -31343.008224566278\n",
            "Episode * 28 * Episodic Reward is ==> -31312.89257609788\n",
            "Episode * 29 * Episodic Reward is ==> -31294.741814272416\n",
            "Episode * 30 * Episodic Reward is ==> -31336.0168350291\n",
            "Episode * 31 * Episodic Reward is ==> -31331.32142263872\n",
            "Episode * 32 * Episodic Reward is ==> -31351.61045372777\n",
            "Episode * 33 * Episodic Reward is ==> -31355.25044957491\n",
            "Episode * 34 * Episodic Reward is ==> -31355.000165785506\n",
            "Episode * 35 * Episodic Reward is ==> -31355.903605350006\n",
            "Episode * 36 * Episodic Reward is ==> -31386.66338026302\n",
            "Episode * 37 * Episodic Reward is ==> -31365.396739705044\n",
            "Episode * 38 * Episodic Reward is ==> -31340.61956386666\n",
            "Episode * 39 * Episodic Reward is ==> -31374.79123026638\n",
            "Episode * 40 * Episodic Reward is ==> -31396.280425655677\n",
            "Episode * 41 * Episodic Reward is ==> -31400.13804555907\n",
            "Episode * 42 * Episodic Reward is ==> -31401.079480483342\n",
            "Episode * 43 * Episodic Reward is ==> -31397.96160799883\n",
            "Episode * 44 * Episodic Reward is ==> -31374.206576031538\n",
            "Episode * 45 * Episodic Reward is ==> -31398.342062802232\n",
            "Episode * 46 * Episodic Reward is ==> -31359.100180674643\n",
            "Episode * 47 * Episodic Reward is ==> -31395.890528699972\n",
            "Episode * 48 * Episodic Reward is ==> -31387.074844860934\n",
            "Episode * 49 * Episodic Reward is ==> -31373.89604899478\n",
            "Episode * 50 * Episodic Reward is ==> -31373.716559827637\n",
            "Episode * 51 * Episodic Reward is ==> -31374.926981563243\n",
            "Episode * 52 * Episodic Reward is ==> -31387.952355563033\n",
            "Episode * 53 * Episodic Reward is ==> -31382.75985352652\n",
            "Episode * 54 * Episodic Reward is ==> -31379.90575174666\n",
            "Episode * 55 * Episodic Reward is ==> -31373.509942260378\n",
            "Episode * 56 * Episodic Reward is ==> -31372.259953595116\n",
            "Episode * 57 * Episodic Reward is ==> -31371.03385412408\n",
            "Episode * 58 * Episodic Reward is ==> -31372.25450037625\n",
            "Episode * 59 * Episodic Reward is ==> -31366.642578336287\n",
            "Episode * 60 * Episodic Reward is ==> -31404.033368971614\n",
            "Episode * 61 * Episodic Reward is ==> -31354.874376089232\n",
            "Episode * 62 * Episodic Reward is ==> -31366.651105521327\n",
            "Episode * 63 * Episodic Reward is ==> -31362.861813876163\n",
            "Episode * 64 * Episodic Reward is ==> -31347.437080311338\n",
            "Episode * 65 * Episodic Reward is ==> -31393.264487226385\n",
            "Episode * 66 * Episodic Reward is ==> -31408.804532445436\n",
            "Episode * 67 * Episodic Reward is ==> -31428.311267784145\n",
            "Episode * 68 * Episodic Reward is ==> -31436.746917957913\n",
            "Episode * 69 * Episodic Reward is ==> -31415.598269427694\n",
            "Episode * 70 * Episodic Reward is ==> -31373.89886923708\n",
            "Episode * 71 * Episodic Reward is ==> -31405.209394405974\n",
            "Episode * 72 * Episodic Reward is ==> -31386.52360614101\n",
            "Episode * 73 * Episodic Reward is ==> -31352.254456342213\n",
            "Episode * 74 * Episodic Reward is ==> -31348.532724110886\n",
            "Episode * 75 * Episodic Reward is ==> -31416.249933867348\n",
            "Episode * 76 * Episodic Reward is ==> -31419.008325338273\n",
            "Episode * 77 * Episodic Reward is ==> -31429.491736611377\n",
            "Episode * 78 * Episodic Reward is ==> -31410.156040947917\n",
            "Episode * 79 * Episodic Reward is ==> -31406.509617467724\n",
            "Episode * 80 * Episodic Reward is ==> -31385.34419977881\n",
            "Episode * 81 * Episodic Reward is ==> -31338.102307356254\n",
            "Episode * 82 * Episodic Reward is ==> -31294.844446665207\n",
            "Episode * 83 * Episodic Reward is ==> -31314.46565666814\n",
            "Episode * 84 * Episodic Reward is ==> -31329.291728770735\n",
            "Episode * 85 * Episodic Reward is ==> -31336.291210205796\n",
            "Episode * 86 * Episodic Reward is ==> -31271.1830381036\n",
            "Episode * 87 * Episodic Reward is ==> -31264.76259841728\n",
            "Episode * 88 * Episodic Reward is ==> -31246.158952437752\n",
            "Episode * 89 * Episodic Reward is ==> -31299.926039634876\n",
            "Episode * 90 * Episodic Reward is ==> -31328.779595424316\n",
            "Episode * 91 * Episodic Reward is ==> -31327.23004343003\n",
            "Episode * 92 * Episodic Reward is ==> -31334.484730818396\n",
            "Episode * 93 * Episodic Reward is ==> -31263.110373938325\n",
            "Episode * 94 * Episodic Reward is ==> -31312.734672853956\n",
            "Episode * 95 * Episodic Reward is ==> -31311.63361758159\n",
            "Episode * 96 * Episodic Reward is ==> -31343.831181803198\n",
            "Episode * 97 * Episodic Reward is ==> -31343.109569402724\n",
            "Episode * 98 * Episodic Reward is ==> -31298.773886145726\n",
            "Episode * 99 * Episodic Reward is ==> -31332.642471366515\n",
            "Episode * 100 * Episodic Reward is ==> -31329.807731873556\n",
            "Episode * 101 * Episodic Reward is ==> -31320.391154058354\n",
            "Episode * 102 * Episodic Reward is ==> -31341.693751782914\n",
            "Episode * 103 * Episodic Reward is ==> -31339.03355853479\n",
            "Episode * 104 * Episodic Reward is ==> -31339.112379142687\n",
            "Episode * 105 * Episodic Reward is ==> -31325.473092990993\n",
            "Episode * 106 * Episodic Reward is ==> -31260.069852414133\n",
            "Episode * 107 * Episodic Reward is ==> -31287.915707068725\n",
            "Episode * 108 * Episodic Reward is ==> -31305.45275032958\n",
            "Episode * 109 * Episodic Reward is ==> -31299.460799432265\n",
            "Episode * 110 * Episodic Reward is ==> -31288.619233883623\n",
            "Episode * 111 * Episodic Reward is ==> -31292.907814956983\n",
            "Episode * 112 * Episodic Reward is ==> -31342.172996603313\n",
            "Episode * 113 * Episodic Reward is ==> -31344.829729588382\n",
            "Episode * 114 * Episodic Reward is ==> -31342.847134592714\n",
            "Episode * 115 * Episodic Reward is ==> -31338.42083461283\n",
            "Episode * 116 * Episodic Reward is ==> -31340.66600495239\n",
            "Episode * 117 * Episodic Reward is ==> -31343.251518723842\n",
            "Episode * 118 * Episodic Reward is ==> -31339.632241842286\n",
            "Episode * 119 * Episodic Reward is ==> -31342.458389614232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvzJ-KW2Cbo9"
      },
      "source": [
        "actor_model.save_weights(\"120actor.h5\")\n",
        "critic_model.save_weights(\"120critic.h5\")\n",
        "\n",
        "target_actor.save_weights(\"120target_actor.h5\")\n",
        "target_critic.save_weights(\"120target_critic.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "5gVkKsR6Ci5e",
        "outputId": "8f00bc45-dac2-4a4b-d6e8-3d68c5ef915a"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "from os import path\n",
        "import control\n",
        "from  control.matlab import *\n",
        "import tensorflow as tfl\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#from Buffer import Buffer\n",
        "#from OUActionNoise import OUActionNoise\n",
        "\n",
        "Us = [0]\n",
        "ts = np.array([0])\n",
        "yout = []\n",
        "\n",
        "class environment():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "            self.kp = 34.5047797848936\n",
        "            self.ki = 2.41863698260906\n",
        "            self.kd = 0.013145\n",
        "            self.j = 1 \n",
        "            self.max_input = 100\n",
        "            #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.I = 0\n",
        "            \n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "            self.Us = [0]\n",
        "            self.ts = np.array([0])\n",
        "            self.yout = []\n",
        "            #self.e00 = 0\n",
        "           #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.j = 1 \n",
        "            self.I = 0\n",
        "            u1 = 0\n",
        "            \n",
        "            #self.rl = random.randrange(-self.max_input, self.max_input)\n",
        "            return np.array([self.e2,u1])\n",
        "\n",
        "    def step(self ,a) :\n",
        "            \n",
        "\n",
        "            mot0 = tf([0.4602, 1.882, 2.038, 0.2338, 0.007103], [62.85, 383.5, 803.4, 624.1, 99.79, 5.916, 0.1204])\n",
        "            self.rl = np.clip(a, 0, self.max_input)[0]\n",
        "            \n",
        "            P = 34.5047797848936*self.e2\n",
        "            self.I = self.I + 2.41863698260906*(self.e2)*0.1\n",
        "            D = -0.013145*(self.e2-self.e1)/0.1\n",
        "            self.u = P + self.I + D\n",
        "            \n",
        "            uu = self.u + self.rl\n",
        "            \n",
        "            self.Us = np.append(self.Us,uu)\n",
        "            self.ts = np.append(self.ts,0.1*self.j)\n",
        "            y, T, xoutd = lsim(3.6*mot0, U=self.Us, T=self.ts)\n",
        "            self.yout.append(y[-1])\n",
        "            #self.ynow = y[-1]\n",
        "            self.e1 = self.e2\n",
        "            self.e2 = 30-y[-1]\n",
        "            self.j+=1\n",
        "            P1 = 34.5047797848936*self.e2\n",
        "            I1 = self.I\n",
        "            I1 = I1 + 2.41863698260906*(self.e2)*0.1\n",
        "            D1 = -0.013145*(self.e2-self.e1)/0.1\n",
        "            u1 = P1 + I1 + D1\n",
        "            \n",
        "            \n",
        "            #reward = 0.8*np.exp(-0.5*(self.e2)**2) + 0.2*np.exp(-np.absolute(self.rl))\n",
        "            reward = -(self.e2)**2\n",
        "\n",
        "            if self.j >= 500:\n",
        "              done = True\n",
        "            else :\n",
        "              done = False\n",
        "            self.state = np.array([self.e2,u1])\n",
        "            return self.state, reward, done, {}\n",
        "\n",
        "env = environment()\n",
        "num_states = 2\n",
        "num_actions = 1\n",
        "upper_bound = 100\n",
        "lower_bound = 0\n",
        "\n",
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tfl.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tfl.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tfl.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tfl.math.reduce_mean(tfl.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tfl.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tfl.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tfl.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tfl.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tfl.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tfl.cast(reward_batch, dtype=tfl.float32)\n",
        "        next_state_batch = tfl.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tfl.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tfl.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tfl.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tfl.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def policy(state, noise_object):\n",
        "    sampled_actions = tfl.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]\n",
        "\n",
        "std_dev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tfl.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tfl.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 1000\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(100000, 32)\n",
        "\n",
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tfl.expand_dims(tfl.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "        #print(j)\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    print(\"Episode * {} * Episodic Reward is ==> {}\".format(ep, episodic_reward))\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    #print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/control/timeresp.py:294: UserWarning: return_x specified for a transfer function system. Internal conversion to state space used; results may meaningless.\n",
            "  \"return_x specified for a transfer function system. Internal \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Episodic Reward is ==> -12430.936063193967\n",
            "Episode * 1 * Episodic Reward is ==> -12430.936063193967\n",
            "Episode * 2 * Episodic Reward is ==> -12430.936063193967\n",
            "Episode * 3 * Episodic Reward is ==> -12430.936063193967\n",
            "Episode * 4 * Episodic Reward is ==> -12430.936063193967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6e9b100f4a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_prev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mou_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Recieve state and reward from environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-6e9b100f4a7f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxoutd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmot0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m#self.ynow = y[-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/matlab/timeresp.py\u001b[0m in \u001b[0;36mlsim\u001b[0;34m(sys, U, T, X0)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;31m# Switch output argument order and transpose outputs (and always return x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforced_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/timeresp.py\u001b[0m in \u001b[0;36mforced_response\u001b[0;34m(sys, T, U, X0, transpose, interpolate, return_x, squeeze)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \"conversion to state space used; results may meaningless.\")\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0msys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_statespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/control/statesp.py\u001b[0m in \u001b[0;36m_convert_to_statespace\u001b[0;34m(sys, **kw)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                              \"convert to StateSpace system.\")\n\u001b[1;32m   1368\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mslycot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtd04ad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m                 raise TypeError(\"If sys is a TransferFunction, \"\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZO3T5Ow8TiS8",
        "outputId": "6a3d5c3f-98b8-4978-83a2-85e084809965"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "from os import path\n",
        "import control\n",
        "from  control.matlab import *\n",
        "import tensorflow as tfl\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#from Buffer import Buffer\n",
        "#from OUActionNoise import OUActionNoise\n",
        "\n",
        "Us = [0]\n",
        "ts = np.array([0])\n",
        "yout = []\n",
        "\n",
        "class environment():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "            self.kp = 29.5047797848936\n",
        "            self.ki = 2.41863698260906\n",
        "            self.kd = 0.013145\n",
        "            self.j = 1 \n",
        "            self.max_input = 100\n",
        "            #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.I = 0\n",
        "            \n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "            self.Us = [0]\n",
        "            self.ts = np.array([0])\n",
        "            self.yout = []\n",
        "            #self.e00 = 0\n",
        "           #self.e0 = 0\n",
        "            self.e1 = 0\n",
        "            self.e2 = 0\n",
        "            self.j = 1 \n",
        "            self.I = 0\n",
        "            u1 = 0\n",
        "            \n",
        "            #self.rl = random.randrange(-self.max_input, self.max_input)\n",
        "            return np.array([self.e2,u1])\n",
        "\n",
        "    def step(self ,a) :\n",
        "            \n",
        "\n",
        "            mot0 = tf([0.4602, 1.882, 2.038, 0.2338, 0.007103], [62.85, 383.5, 803.4, 624.1, 99.79, 5.916, 0.1204])\n",
        "            self.rl = np.clip(a, -self.max_input, self.max_input)[0]\n",
        "            \n",
        "            P = 29.5047797848936*self.e2\n",
        "            self.I = self.I + 2.41863698260906*(self.e2)*0.1\n",
        "            D = -0.013145*(self.e2-self.e1)/0.1\n",
        "            self.u = P + self.I + D\n",
        "            \n",
        "            uu = self.u + self.rl\n",
        "            \n",
        "            self.Us = np.append(self.Us,uu)\n",
        "            self.ts = np.append(self.ts,0.1*self.j)\n",
        "            y, T, xoutd = lsim(3.6*mot0, U=self.Us, T=self.ts)\n",
        "            self.yout.append(y[-1])\n",
        "            #self.ynow = y[-1]\n",
        "            self.e1 = self.e2\n",
        "            self.e2 = 30-y[-1]\n",
        "            self.j+=1\n",
        "            P1 = 29.5047797848936*self.e2\n",
        "            I1 = self.I\n",
        "            I1 = I1 + 2.41863698260906*(self.e2)*0.1\n",
        "            D1 = -0.013145*(self.e2-self.e1)/0.1\n",
        "            u1 = P1 + I1 + D1\n",
        "            \n",
        "            \n",
        "            #reward = 0.8*np.exp(-0.5*(self.e2)**2) + 0.2*np.exp(-np.absolute(self.rl))\n",
        "            reward = -(self.e2)**2\n",
        "\n",
        "            if self.j >= 500:\n",
        "              done = True\n",
        "            else :\n",
        "              done = False\n",
        "            self.state = np.array([self.e2,u1])\n",
        "            return self.state, reward, done, {}\n",
        "\n",
        "env = environment()\n",
        "num_states = 2\n",
        "num_actions = 1\n",
        "upper_bound = 100\n",
        "lower_bound = -100\n",
        "\n",
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tfl.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tfl.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tfl.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tfl.math.reduce_mean(tfl.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tfl.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tfl.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tfl.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tfl.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tfl.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tfl.cast(reward_batch, dtype=tfl.float32)\n",
        "        next_state_batch = tfl.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tfl.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tfl.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tfl.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tfl.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def policy(state, noise_object):\n",
        "    sampled_actions = tfl.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]\n",
        "\n",
        "std_dev = 10\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tfl.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tfl.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 500\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(100000, 64)\n",
        "\n",
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tfl.expand_dims(tfl.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "        #print(j)\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    print(\"Episode * {} * Episodic Reward is ==> {}\".format(ep, episodic_reward))\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    #print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(ep_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/control/timeresp.py:294: UserWarning: return_x specified for a transfer function system. Internal conversion to state space used; results may meaningless.\n",
            "  \"return_x specified for a transfer function system. Internal \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Episodic Reward is ==> -13215.179822516115\n",
            "Episode * 1 * Episodic Reward is ==> -13139.222396237554\n",
            "Episode * 2 * Episodic Reward is ==> -13231.645290591117\n",
            "Episode * 3 * Episodic Reward is ==> -13256.376155605321\n",
            "Episode * 4 * Episodic Reward is ==> -13177.149286341775\n",
            "Episode * 5 * Episodic Reward is ==> -13234.172935284188\n",
            "Episode * 6 * Episodic Reward is ==> -13288.203399847178\n",
            "Episode * 7 * Episodic Reward is ==> -13171.402752629561\n",
            "Episode * 8 * Episodic Reward is ==> -13372.60192617124\n",
            "Episode * 9 * Episodic Reward is ==> -13224.772791379062\n",
            "Episode * 10 * Episodic Reward is ==> -13379.601169985455\n",
            "Episode * 11 * Episodic Reward is ==> -13245.847742838952\n",
            "Episode * 12 * Episodic Reward is ==> -13200.963014646666\n",
            "Episode * 13 * Episodic Reward is ==> -13449.212366439557\n",
            "Episode * 14 * Episodic Reward is ==> -13217.66010954223\n",
            "Episode * 15 * Episodic Reward is ==> -13202.915642503176\n",
            "Episode * 16 * Episodic Reward is ==> -13207.347387097343\n",
            "Episode * 17 * Episodic Reward is ==> -13233.777564407419\n",
            "Episode * 18 * Episodic Reward is ==> -13192.111561367337\n",
            "Episode * 19 * Episodic Reward is ==> -13212.44357416959\n",
            "Episode * 20 * Episodic Reward is ==> -13139.64375470136\n",
            "Episode * 21 * Episodic Reward is ==> -13270.058892919082\n",
            "Episode * 22 * Episodic Reward is ==> -13281.817075399636\n",
            "Episode * 23 * Episodic Reward is ==> -13363.254888230373\n",
            "Episode * 24 * Episodic Reward is ==> -13197.221164228475\n",
            "Episode * 25 * Episodic Reward is ==> -13253.308111611595\n",
            "Episode * 26 * Episodic Reward is ==> -13179.279485017454\n",
            "Episode * 27 * Episodic Reward is ==> -13180.74735784883\n",
            "Episode * 28 * Episodic Reward is ==> -13175.350857777334\n",
            "Episode * 29 * Episodic Reward is ==> -13229.726681285196\n",
            "Episode * 30 * Episodic Reward is ==> -13342.507930435328\n",
            "Episode * 31 * Episodic Reward is ==> -13255.087658932352\n",
            "Episode * 32 * Episodic Reward is ==> -13208.301239650255\n",
            "Episode * 33 * Episodic Reward is ==> -13417.981779411268\n",
            "Episode * 34 * Episodic Reward is ==> -13352.680686492193\n",
            "Episode * 35 * Episodic Reward is ==> -13165.466332235334\n",
            "Episode * 36 * Episodic Reward is ==> -13299.895470220217\n",
            "Episode * 37 * Episodic Reward is ==> -13196.247413085937\n",
            "Episode * 38 * Episodic Reward is ==> -13273.949617473103\n",
            "Episode * 39 * Episodic Reward is ==> -13253.388896009681\n",
            "Episode * 40 * Episodic Reward is ==> -13234.974464323279\n",
            "Episode * 41 * Episodic Reward is ==> -13496.60963545644\n",
            "Episode * 42 * Episodic Reward is ==> -13219.57535753559\n",
            "Episode * 43 * Episodic Reward is ==> -13202.915642503176\n",
            "Episode * 44 * Episodic Reward is ==> -13244.64021375043\n",
            "Episode * 45 * Episodic Reward is ==> -13232.529489354545\n",
            "Episode * 46 * Episodic Reward is ==> -13347.191184264018\n",
            "Episode * 47 * Episodic Reward is ==> -13190.087191017199\n",
            "Episode * 48 * Episodic Reward is ==> -13413.10147823579\n",
            "Episode * 49 * Episodic Reward is ==> -13272.646375837738\n",
            "Episode * 50 * Episodic Reward is ==> -13257.129994056646\n",
            "Episode * 51 * Episodic Reward is ==> -13321.322274393417\n",
            "Episode * 52 * Episodic Reward is ==> -13297.710631506876\n",
            "Episode * 53 * Episodic Reward is ==> -13270.40835393797\n",
            "Episode * 54 * Episodic Reward is ==> -13355.279086700484\n",
            "Episode * 55 * Episodic Reward is ==> -13277.7447232319\n",
            "Episode * 56 * Episodic Reward is ==> -13291.924587634556\n",
            "Episode * 57 * Episodic Reward is ==> -13384.600352498594\n",
            "Episode * 58 * Episodic Reward is ==> -13382.298758227076\n",
            "Episode * 59 * Episodic Reward is ==> -13262.822597534101\n",
            "Episode * 60 * Episodic Reward is ==> -13242.903160368327\n",
            "Episode * 61 * Episodic Reward is ==> -13239.921166802786\n",
            "Episode * 62 * Episodic Reward is ==> -13456.717732446436\n",
            "Episode * 63 * Episodic Reward is ==> -13253.013648505934\n",
            "Episode * 64 * Episodic Reward is ==> -13344.721716748356\n",
            "Episode * 65 * Episodic Reward is ==> -13220.458382486402\n",
            "Episode * 66 * Episodic Reward is ==> -13364.46122737855\n",
            "Episode * 67 * Episodic Reward is ==> -13218.366354175023\n",
            "Episode * 68 * Episodic Reward is ==> -13224.734196115747\n",
            "Episode * 69 * Episodic Reward is ==> -13202.915642503176\n",
            "Episode * 70 * Episodic Reward is ==> -13199.99083895789\n",
            "Episode * 71 * Episodic Reward is ==> -13330.764513329148\n",
            "Episode * 72 * Episodic Reward is ==> -13244.39619720062\n",
            "Episode * 73 * Episodic Reward is ==> -13225.745724338887\n",
            "Episode * 74 * Episodic Reward is ==> -13248.17833102704\n",
            "Episode * 75 * Episodic Reward is ==> -13280.358159957921\n",
            "Episode * 76 * Episodic Reward is ==> -13219.543892492531\n",
            "Episode * 77 * Episodic Reward is ==> -13202.915642503176\n",
            "Episode * 78 * Episodic Reward is ==> -13202.857758337394\n",
            "Episode * 79 * Episodic Reward is ==> -13249.701319262365\n",
            "Episode * 80 * Episodic Reward is ==> -13204.633153715058\n",
            "Episode * 81 * Episodic Reward is ==> -13237.656713678638\n",
            "Episode * 82 * Episodic Reward is ==> -13226.959130360383\n",
            "Episode * 83 * Episodic Reward is ==> -13264.817273924904\n",
            "Episode * 84 * Episodic Reward is ==> -13223.869187641118\n",
            "Episode * 85 * Episodic Reward is ==> -13163.899358777848\n",
            "Episode * 86 * Episodic Reward is ==> -13088.66107888919\n",
            "Episode * 87 * Episodic Reward is ==> -13333.883996582945\n",
            "Episode * 88 * Episodic Reward is ==> -13188.076226466248\n",
            "Episode * 89 * Episodic Reward is ==> -13409.65926161846\n",
            "Episode * 90 * Episodic Reward is ==> -13295.825167581108\n",
            "Episode * 91 * Episodic Reward is ==> -13387.380388546551\n",
            "Episode * 92 * Episodic Reward is ==> -13196.730241958738\n",
            "Episode * 93 * Episodic Reward is ==> -13327.608253411392\n",
            "Episode * 94 * Episodic Reward is ==> -13550.14038798934\n",
            "Episode * 95 * Episodic Reward is ==> -13253.557208577557\n",
            "Episode * 96 * Episodic Reward is ==> -13394.72977786999\n",
            "Episode * 97 * Episodic Reward is ==> -13404.989977680652\n",
            "Episode * 98 * Episodic Reward is ==> -13249.266782458202\n",
            "Episode * 99 * Episodic Reward is ==> -13195.448441723269\n",
            "Episode * 100 * Episodic Reward is ==> -13360.430856135952\n",
            "Episode * 101 * Episodic Reward is ==> -13272.154517459541\n",
            "Episode * 102 * Episodic Reward is ==> -13262.615286869923\n",
            "Episode * 103 * Episodic Reward is ==> -13299.617091238077\n",
            "Episode * 104 * Episodic Reward is ==> -13354.88953419514\n",
            "Episode * 105 * Episodic Reward is ==> -13320.832755808246\n",
            "Episode * 106 * Episodic Reward is ==> -13334.520150850482\n",
            "Episode * 107 * Episodic Reward is ==> -13339.68774456354\n",
            "Episode * 108 * Episodic Reward is ==> -13306.83792949341\n",
            "Episode * 109 * Episodic Reward is ==> -13296.249502433075\n",
            "Episode * 110 * Episodic Reward is ==> -13320.489542476822\n",
            "Episode * 111 * Episodic Reward is ==> -13313.440906662343\n",
            "Episode * 112 * Episodic Reward is ==> -13227.090176506083\n",
            "Episode * 113 * Episodic Reward is ==> -13403.28221849541\n",
            "Episode * 114 * Episodic Reward is ==> -13321.447153536945\n",
            "Episode * 115 * Episodic Reward is ==> -13544.453221716523\n",
            "Episode * 116 * Episodic Reward is ==> -13324.22644237528\n",
            "Episode * 117 * Episodic Reward is ==> -13325.253640009481\n",
            "Episode * 118 * Episodic Reward is ==> -13307.991632555983\n",
            "Episode * 119 * Episodic Reward is ==> -13315.92670463948\n",
            "Episode * 120 * Episodic Reward is ==> -13309.489544178121\n",
            "Episode * 121 * Episodic Reward is ==> -13295.12188944014\n",
            "Episode * 122 * Episodic Reward is ==> -13286.350970045436\n",
            "Episode * 123 * Episodic Reward is ==> -13357.944315048586\n",
            "Episode * 124 * Episodic Reward is ==> -13345.78770270938\n",
            "Episode * 125 * Episodic Reward is ==> -13317.721712213757\n",
            "Episode * 126 * Episodic Reward is ==> -13253.65905447428\n",
            "Episode * 127 * Episodic Reward is ==> -13287.26833297567\n",
            "Episode * 128 * Episodic Reward is ==> -13280.526682404286\n",
            "Episode * 129 * Episodic Reward is ==> -13270.193438671142\n",
            "Episode * 130 * Episodic Reward is ==> -13266.576912992272\n",
            "Episode * 131 * Episodic Reward is ==> -13322.466966050908\n",
            "Episode * 132 * Episodic Reward is ==> -13410.385802907684\n",
            "Episode * 133 * Episodic Reward is ==> -13314.929427515066\n",
            "Episode * 134 * Episodic Reward is ==> -13288.069880264045\n",
            "Episode * 135 * Episodic Reward is ==> -13253.23252098019\n",
            "Episode * 136 * Episodic Reward is ==> -13208.842784443965\n",
            "Episode * 137 * Episodic Reward is ==> -13235.983573038677\n",
            "Episode * 138 * Episodic Reward is ==> -13493.612406368211\n",
            "Episode * 139 * Episodic Reward is ==> -13337.650407719128\n",
            "Episode * 140 * Episodic Reward is ==> -13353.74050676297\n",
            "Episode * 141 * Episodic Reward is ==> -13375.126880332304\n",
            "Episode * 142 * Episodic Reward is ==> -13287.272029033267\n",
            "Episode * 143 * Episodic Reward is ==> -13285.279388095893\n",
            "Episode * 144 * Episodic Reward is ==> -13287.011356498462\n",
            "Episode * 145 * Episodic Reward is ==> -13368.538181320851\n",
            "Episode * 146 * Episodic Reward is ==> -13279.187351348768\n",
            "Episode * 147 * Episodic Reward is ==> -13284.331167989621\n",
            "Episode * 148 * Episodic Reward is ==> -13278.970244312313\n",
            "Episode * 149 * Episodic Reward is ==> -13271.18359142549\n",
            "Episode * 150 * Episodic Reward is ==> -13324.935238692726\n",
            "Episode * 151 * Episodic Reward is ==> -13471.550747467443\n",
            "Episode * 152 * Episodic Reward is ==> -13255.881364973573\n",
            "Episode * 153 * Episodic Reward is ==> -13472.378805641682\n",
            "Episode * 154 * Episodic Reward is ==> -13355.919441373711\n",
            "Episode * 155 * Episodic Reward is ==> -13315.54764979404\n",
            "Episode * 156 * Episodic Reward is ==> -13346.626566290859\n",
            "Episode * 157 * Episodic Reward is ==> -13294.228731919753\n",
            "Episode * 158 * Episodic Reward is ==> -13272.367499720274\n",
            "Episode * 159 * Episodic Reward is ==> -13256.311576756594\n",
            "Episode * 160 * Episodic Reward is ==> -13375.722032511847\n",
            "Episode * 161 * Episodic Reward is ==> -13278.306376933617\n",
            "Episode * 162 * Episodic Reward is ==> -13292.7789744463\n",
            "Episode * 163 * Episodic Reward is ==> -13287.772306719338\n",
            "Episode * 164 * Episodic Reward is ==> -13267.382248365002\n",
            "Episode * 165 * Episodic Reward is ==> -13314.079220199179\n",
            "Episode * 166 * Episodic Reward is ==> -13306.702426043023\n",
            "Episode * 167 * Episodic Reward is ==> -13359.710822790234\n",
            "Episode * 168 * Episodic Reward is ==> -13295.550329756248\n",
            "Episode * 169 * Episodic Reward is ==> -13516.909905176935\n",
            "Episode * 170 * Episodic Reward is ==> -13335.474832182454\n",
            "Episode * 171 * Episodic Reward is ==> -13384.510072892976\n",
            "Episode * 172 * Episodic Reward is ==> -13536.454859697562\n",
            "Episode * 173 * Episodic Reward is ==> -13243.184874487153\n",
            "Episode * 174 * Episodic Reward is ==> -13425.482606820065\n",
            "Episode * 175 * Episodic Reward is ==> -13489.868700853709\n",
            "Episode * 176 * Episodic Reward is ==> -13392.984040342022\n",
            "Episode * 177 * Episodic Reward is ==> -13346.912320623422\n",
            "Episode * 178 * Episodic Reward is ==> -13427.360811195005\n",
            "Episode * 179 * Episodic Reward is ==> -13369.473777933003\n",
            "Episode * 180 * Episodic Reward is ==> -13361.690466135811\n",
            "Episode * 181 * Episodic Reward is ==> -13418.39716299624\n",
            "Episode * 182 * Episodic Reward is ==> -13330.4309736896\n",
            "Episode * 183 * Episodic Reward is ==> -13369.926629811443\n",
            "Episode * 184 * Episodic Reward is ==> -13332.38411155979\n",
            "Episode * 185 * Episodic Reward is ==> -13272.787234646417\n",
            "Episode * 186 * Episodic Reward is ==> -13313.363803468565\n",
            "Episode * 187 * Episodic Reward is ==> -13288.577256782542\n",
            "Episode * 188 * Episodic Reward is ==> -13269.304824787849\n",
            "Episode * 189 * Episodic Reward is ==> -13342.656490513462\n",
            "Episode * 190 * Episodic Reward is ==> -13323.980755845929\n",
            "Episode * 191 * Episodic Reward is ==> -13334.507810252455\n",
            "Episode * 192 * Episodic Reward is ==> -13555.790446286579\n",
            "Episode * 193 * Episodic Reward is ==> -13329.46745083176\n",
            "Episode * 194 * Episodic Reward is ==> -13432.207463403738\n",
            "Episode * 195 * Episodic Reward is ==> -13336.230815556322\n",
            "Episode * 196 * Episodic Reward is ==> -13329.199455228183\n",
            "Episode * 197 * Episodic Reward is ==> -13312.620872200167\n",
            "Episode * 198 * Episodic Reward is ==> -13534.838817301743\n",
            "Episode * 199 * Episodic Reward is ==> -13398.730740406018\n",
            "Episode * 200 * Episodic Reward is ==> -13397.991907877873\n",
            "Episode * 201 * Episodic Reward is ==> -13539.850920929455\n",
            "Episode * 202 * Episodic Reward is ==> -13296.97079450673\n",
            "Episode * 203 * Episodic Reward is ==> -13327.504505755118\n",
            "Episode * 204 * Episodic Reward is ==> -13283.071615236999\n",
            "Episode * 205 * Episodic Reward is ==> -13321.462240833946\n",
            "Episode * 206 * Episodic Reward is ==> -13433.387187642427\n",
            "Episode * 207 * Episodic Reward is ==> -13345.142418052968\n",
            "Episode * 208 * Episodic Reward is ==> -13579.91999744197\n",
            "Episode * 209 * Episodic Reward is ==> -13595.089274832679\n",
            "Episode * 210 * Episodic Reward is ==> -13429.295358769925\n",
            "Episode * 211 * Episodic Reward is ==> -13382.998889779634\n",
            "Episode * 212 * Episodic Reward is ==> -13479.775508687737\n",
            "Episode * 213 * Episodic Reward is ==> -13364.054441645494\n",
            "Episode * 214 * Episodic Reward is ==> -13318.257974740018\n",
            "Episode * 215 * Episodic Reward is ==> -13465.237921193851\n",
            "Episode * 216 * Episodic Reward is ==> -13359.227924685358\n",
            "Episode * 217 * Episodic Reward is ==> -13433.672221724006\n",
            "Episode * 218 * Episodic Reward is ==> -13414.404019096162\n",
            "Episode * 219 * Episodic Reward is ==> -13549.050325773303\n",
            "Episode * 220 * Episodic Reward is ==> -13357.278849594206\n",
            "Episode * 221 * Episodic Reward is ==> -13243.741473265618\n",
            "Episode * 222 * Episodic Reward is ==> -13330.462395779645\n",
            "Episode * 223 * Episodic Reward is ==> -13336.408434855563\n",
            "Episode * 224 * Episodic Reward is ==> -13341.669809084933\n",
            "Episode * 225 * Episodic Reward is ==> -13301.651695810895\n",
            "Episode * 226 * Episodic Reward is ==> -13340.239804421437\n",
            "Episode * 227 * Episodic Reward is ==> -13378.95132664268\n",
            "Episode * 228 * Episodic Reward is ==> -13260.47650955958\n",
            "Episode * 229 * Episodic Reward is ==> -13450.10512799089\n",
            "Episode * 230 * Episodic Reward is ==> -13511.255622739716\n",
            "Episode * 231 * Episodic Reward is ==> -13347.877658512945\n",
            "Episode * 232 * Episodic Reward is ==> -13444.103232968617\n",
            "Episode * 233 * Episodic Reward is ==> -13445.600859419517\n",
            "Episode * 234 * Episodic Reward is ==> -13435.548312601388\n",
            "Episode * 235 * Episodic Reward is ==> -13431.11906707475\n",
            "Episode * 236 * Episodic Reward is ==> -13202.730476136763\n",
            "Episode * 237 * Episodic Reward is ==> -13494.926927155168\n",
            "Episode * 238 * Episodic Reward is ==> -13263.208300765456\n",
            "Episode * 239 * Episodic Reward is ==> -13406.801893151334\n",
            "Episode * 240 * Episodic Reward is ==> -13257.83173696307\n",
            "Episode * 241 * Episodic Reward is ==> -13213.901161440963\n",
            "Episode * 242 * Episodic Reward is ==> -13217.921124074113\n",
            "Episode * 243 * Episodic Reward is ==> -13191.123929693058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9bf403c03f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mepisodic_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_actor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9bf403c03f7d>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mnext_state_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}